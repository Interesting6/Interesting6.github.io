---
title: my kmeans
tags:
  - machine_leanring
categories:
  - machine_leanring
  - code
mathjax: True
date: 2018-04-15 20:58:05
---

# k-means 聚类

## 1. 什么是聚类

聚类分析是一种**非监督学习**，在数据中发现数据对象之间的关系，将数据进行分组，组内的相似性越大，组间的差别越大，则聚类效果越好。

## 2.基本的聚类分析算法

>1. K均值：
基于原型的、划分的距离技术，它试图发现用户指定个数(K)的簇。

>2. 凝聚的层次距离：
思想是开始时，每个点都作为一个单点簇，然后，重复的合并两个最靠近的簇，直到尝试单个、包含所有点的簇。

>3. DBSCAN:
一种基于密度的划分距离的算法，簇的个数有算法自动的确定，低密度中的点被视为噪声而忽略，因此其不产生完全聚类。

## 3.距离度量
* 欧式距离：$d(x,y)=\sqrt{\sum_{i=1}^{n}(x_i-y_i)^2}, \forall x,y \in X$
* 曼哈顿距离：$d(x,y)=\sum_{i=1}^{n}\left| x_i-y_i \right|, \forall x,y \in X$
* 切比雪夫距离：$d(x,y)=\max_{1 \leq i \leq n}\left| x_i-y_i \right|, \forall x,y \in X$
* 余弦距离：$d(x,y)=\frac{xy}{\left|x\right| \left|y\right|} \forall x,y \in X$

## 4.初始质心的选取
选择适当的初始质心是基本kmeans算法的关键步骤。常见的方法是随机的选取初始中心，但是这样簇的质量常常很差。处理选取初始质心问题的一种常用技术是：多次运行，每次使用一组不同的随机初始质心，然后选取具有最小SSE(误差的平方和)的簇集。这种策略简单，但是效果可能不好，这取决于数据集和寻找的簇的个数。

第二种有效的方法是，取一个样本，并使用层次聚类技术对它聚类。从层次聚类中提取k个簇，并用这些簇的质心作为初始质心。该方法通常很有效，但仅对下列情况有效：(1)样本相对较小，例如数百到数千(层次聚类开销较大)；(2) k相对于样本大小较小。

第三种选择初始质心的方法，随机地选择第一个点，或取所有点的质心作为第一个点。然后，对于每个后继初始质心，选择离已经选取过的初始质心最远的点。使用这种方法，确保了选择的初始质心不仅是随机的，而且是散开的。但是，这种方法可能选中离群点。此外，求离当前初始质心集最远的点开销也非常大。为了克服这个问题，通常该方法用于点样本。由于离群点很少(多了就不是离群点了)，它们多半不会在随机样本中出现。计算量也大幅减少。

## 5. Kmeans算法流程

算法简要思想
``` text
选择K个点作为初始质心  
repeat  
    将每个点指派到最近的质心，形成K个簇  
    重新计算每个簇的质心  
until 簇不发生变化或达到最大迭代次数
```

输入：聚类个数k，数据集$X \in X^m, x \in R^n$。
输出：满足方差最小标准的k个聚类。

1. 选择k个初始中心点，例如c[0]=X[0] , … , c[k-1]=X[k-1]；

2. 对于X[0]….X[n]，分别与c[0]…c[k-1]比较，假定与c[i]差值最少，就标记为i；

3. 对于所有标记为i点，重新计算c[i]={ 所有标记为i的样本的每个特征的均值}；

4. 重复(2)(3)，直到所有c[i]值的变化小于给定阈值或者达到最大迭代次数。

Kmeans的时间复杂度：$O(tkmn)$，空间复杂度：$O((m+k)n)$。其中，t为迭代次数，k为簇的数目，m为样本数，n为特征数

![image](/images/Kmeans.png)

## 6. Kmeans算法优缺点
### 优点

* 算法原理简单。需要调节的超参数就是一个k。
* 由具有出色的速度和良好的可扩展性。

### 缺点

1. 在 Kmeans 算法中 k需要事先确定，这个 k值的选定有时候是比较难确定。

2. 在 Kmeans 算法中，首先需要初始k个聚类中心，然后以此来确定一个初始划分，然后对初始划分进行优化。这个初始聚类中心的选择对聚类结果有较大的影响，一旦初始值选择的不好，可能无法得到有效的聚类结果。多设置一些不同的初值，对比最后的运算结果，一直到结果趋于稳定结束。

3. 该算法需要不断地进行样本分类调整，不断地计算调整后的新的聚类中心，因此当数据量非常大时，算法的时间开销是非常大的。

4. 对离群点很敏感。


## 7.编程实现

见我GitHub.
