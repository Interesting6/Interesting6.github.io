---
title: Random Forest
tags:
  - machine_leanring

categories:
  - machine_leanring
  - theory
mathjax: false
date: 2018-05-18 23:22:33
---

之前我们讲到了决策树，现在我们来讲一下随机森林，把这段知识补上。

## 首先什么是随机森林？

随机森林就是用随机的方式建立一个森林，在森林里有很多决策树组成，并且每一棵决策树之间是没有关联的。当有一个新样本的时候，我们让森林的每一棵决策树分别进行判断，看看这个样本属于哪一类，然后用投票的方式，哪一类被选择的多，作为最终的分类结果。在回归问题中，随机森林输出所有决策树输出的平均值。

1. 随机森林既可以用于分类，也可以用于回归。

2. 它是一种降维手段，用于处理缺失值和异常值。

3. 它是集成学习的重要方法。

## 两个随机抽取

>* 样本有放回随机抽取固定数目

>* 构建决策树时，特征随机抽取

解释：两个随机性的引入对随机森林的分类性能至关重要。由于它们的引入，使得随机森林不容易陷入过拟合，并且具有很好得抗噪能力（比如：对缺省值不敏感）

## 随机森林算法是如何工作的？

在随机森林中，每一个决策树**“种植”**和**“生长”**的四个步骤：

* 假设我们设定训练集中的样本个数为N，然后通过有放回的有放回的随机选择N个样本(一次)，用这选择好了的N个样本用来训练一个决策树，作为决策树根节点处的样本；
* 当每个样本有M个属性时，在决策树的每个节点需要分裂时，随机从这M个属性中选取出m个属性，满足条件m << M。然后从这m个属性中采用某种策略（比如说信息增益）来选择1个属性作为该节点的分裂属性。
* 每棵决策树都最大可能地进行生长而不进行剪枝；
* 通过对所有的决策树进行加总来预测新的数据（在分类时采用多数投票，在回归时采用平均）。

## 随机森林的优缺点

**优点：**
* 由于每次不再考虑全部的属性，而是一个属性子集，所以相比于Bagging计算开销更小，训练效率更高
* 对高维数据的处理能力强，可以处理成千上万的输入变量，是一个非常不错的降维方法
* 能够输出特征的重要程度, 基于**oob误分类率**和基于**Gini系数的变化**
* 有效的处理缺省值

**缺点：**
* 在噪声较大的时候容易过拟合


## 重要参数

随机森林分类效果（错误率）与两个因素有关：

1. 森林中任意两棵树的**相关性**：相关性越大，错误率越大；

2. 森林中每棵树的**分类能力**：每棵树的分类能力越强，整个森林的错误率越低。

减小特征选择个数m，树的相关性和分类能力也会相应的降低；增大m，两者也会随之增大。所以关键问题是如何选择最优的m（或者是范围），这也是随机森林唯一的一个参数。在学习如何选择参数前首先介绍oob的概念。

## oob：袋外错误率

为了选择最优的m，这里需要利用的是`袋外错误率oob`（out-of-bag error）。我们知道，在构建每个决策树的时候，采用的是随机有放回的抽取，所以对于每棵树来说，都有一些样本(约占1/3)实际上没有参与树的生成，所以这些样本成为袋外样本，即oob。与交叉验证类似，我们可以将这个决策树的obb样本作为这棵树的验证集，对oob集合进行估计的步骤入下：

1. 对每个样本，计算它作为oob样本的树对它的分类情况
2. 多数投票作为该样本的分类结果
3. 用误分个数占样本总数的比率作为随机森林的oob误分率

oob误分率是随机森林泛化误差的一个**无偏估计**，它的结果近似于需要大量计算的k折交叉验证。所以没有必要对它进行交叉验证或者用一个独立的测试集来获得误差的一个无偏估计。它可以在内部进行评估，也就是说在生成的过程中就可以对误差建立一个无偏估计。

当我们知道了oob的计算方法，我们可以通过选取不同的m，计算oob_error，找出oob_error最小时对应的m的值。这和交叉验证的思想非常的相似。

## RF特征重要性的度量方法

* 对于每一棵决策树，计算其oob_error_0
* 选取一个特征，随机对特征加入噪声干扰，再次计算oob error_1
* 特征的重要性=∑(oob_error_1-oob_error_0)/随机森林中决策树的个数
* 对随机森林中的特征变量按照特征重要性降序排序。
* 然后重复以上步骤，直到选出m个特征。

解释：用这个公式来度量特征重要性，原因是：给某个特征随机的加入噪声后，如果oob error增大，说明这个特征对样本分类的结果影响比较大，说明重要程度比较高。

## RF特征选择

首先特征选择的目标有两个：

1：找到与分类结果高度相关的特征变量。

2：选择出数目较少的特征变量并且能够充分的预测应变量的结果。

特征选择的步骤：

（1）对于每一棵决策树，计算其oob_error

（2）随机的修改OOB中的每个特征xi的值，计算oob_error_2，再次计算重要性

（3）按照特征的重要性排序，然后剔除后面不重要的特征

（4）然后重复以上步骤，直到选出m个特征。

## 几个问题

###（1）为什么要随机抽取样本？

答：如果不进行随机抽样，对于每个树的训练集都是相同的，训练出来的结果也是一样的，所以此时进行投票决策没有意义。

###（2）为什么要有放回的去抽样呢?

答：如果不是有放回的抽样，那么每一棵树的训练样本是不同的，都是没有交集的，那么每棵树都是有偏的，都是片面的，树与树之间并不是完全公平的。我们需要的是，没颗决策树是公平的，然后让它们投票决策得出结果，并且这样可以防止过度拟合。

###（3）这里指的有放回的抽样，是每次抽一个放回，还是一次抽n个再放回？

答: 构造子数据集，子数据集的数据量是和原始数据集相同的。不同子数据集的元素可以重复，同一个子数据集中的元素也可以重复。

### (4)随机森林为什么可以用于处理缺失值和异常值的数据？

答：随机森林的容错率高，由于随机森林多棵树，如果一个数据缺失了的字段刚好有些树不需要这些特征，于是可以用该些树进行预测，从而得到结果。而决策树的话很有可能无法进行预测。

### (5)随机森林为什么不容易过拟合，为什么对噪声不敏感？

答：三个随机性的引入，即产生决策树的样本是随机生成，构建决策树的特征值是随机选取，树产生过程中裂变的时候是选择N个最佳方向中的随机一个裂变的。当随机森林产生的树的数目趋近无穷的时候，理论上根据大数定理可以证明训练误差与测试误差是收敛到一起的。

当然实际过程中，由于不可能产生无穷的决策树，模型参数的设置问题会影响在相同运行时间内拟合结果的过拟合程度的不同。但总而言之，调整参数后，随机森林可以有效的降低过拟合的程度。

 另外多颗决策树综合决策，以多数为输出代表，也能在一定程度上减少误差。
