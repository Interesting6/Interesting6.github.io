---
title: 梯度下降法
tags:
  - math
  - algorithm
categories:
  - math
  - algorithm
mathjax: True
date: 2018-04-2 14:12:51
---

## 梯度下降法
### 原理
&#8195&#8195梯度下降法是求解**无约束最优化**问题的一种最常用的**迭代法**。顾名思义，梯度下降法的计算过程就是沿梯度下降的方向求解极小值（也可以沿梯度上升方向求解极大值）。

&#8195&#8195假设$f(x)$是$R^n$上具有一阶连续偏导数的函数，要求解的无约束最优化问题是$$\min_{x\in R^n} f(x)$$
$x^*$表示目标函数的极小值点。

> 选取适当的初值$x^{(0)}$，不断迭代，更新$x$的值，进行目标函数的极小化，直到收敛。由于负梯度方向是使函数值下降最快的方向，在迭代的每一步，以负梯度方向更新$x$的值，从而达到减少函数值的目的。

&#8195&#8195由于$f(x)$具有一阶连续偏导数，若第k次迭代值为$x^{(k)}$，则可将$f(x)$在$x^{(k)}$附近进行一阶泰勒展开：$$f(x)=f(x^{(k)})+g_k^ \top \cdot(x-x^{(k)})$$
其中$g_k =g(x^{(k)})= -\triangledown f(x^{(k)}) $为$f(x)$在$x^{(k)}$的梯度(梯度上升为正号)。
&#8195&#8195求出第k+1次迭代值$x^{(k+1)}$: 
$$x^{(k+1)}\leftarrow x^{(k)}+\lambda_k P^{(k)} $$
![Gradient_descent](/images/Gradient_descent.png)

其中，$p^{(k)}$是搜索方向，取负梯度方向$P^{(k)}=-\triangledown f(x^{(k)}) $，$\lambda_k$是步长，由一维搜索确定，即$\lambda_k$使得$$f(x^{(k)}+\lambda_k P^{(k)})=\min_{\lambda \geqslant 0}f(x^{(k)}+\lambda \cdot p^{(k)})$$

> 一维搜索有个十分重要的性质：在搜索方向上所得最优点处目标函数的梯度和该搜索方向正交。
**定理:** 设目标函数$f(x)$具有一阶连续偏导数，$x^{(k+1)}$由如下规则产生
$$
    \left\{ 
      \begin{array}{c}
        \lambda_k: \min_\lambda f(x^{(k)}+\lambda P^{(k)}) \\ 
        x^{(k+1)} = x^{(k)}+\lambda_k P^{(k)} 
      \end{array}
    \right. 
$$
&#160;则有$$-\triangledown f(x^{(k+1)}) P^{(k)}=0  \tag{1}$$
**证明:** 构造函数$\varphi(\lambda)=f(x^{(k)}+\lambda P^{(k)})$，则得
$$
    \left\{ 
      \begin{array}{c}
        \varphi(\lambda_k)= \min_\lambda \varphi(\lambda) \\ 
        x^{(k+1)} = x^{(k)}+\lambda_k P^{(k)} 
      \end{array}
    \right. 
$$
即$\lambda_k$为$\varphi(\lambda)$的极小值点。此外$\varphi'(\lambda)=\triangledown f(x^{(k)}+\lambda P^{(k)})^\top P^{(k)}$。
由$\varphi'(\lambda)|_{\lambda=\lambda_k}=0$可得$$\triangledown f(x^{(k)}+\lambda_k P^{(k)})^\top P^{(k)}=\triangledown f(x^{(k+1)})^\top P^{(k)}=0$$定理得证。

#### 为什么$P^{(k)}=-\triangledown f(x^{(k)})$?
&#8195&#8195因为对于充分小的$\lambda$，只要$$f(x^{(k)})^\top P^{(k)}<0  \tag{2}$$就可以保证$$f(x^{(k)}+\lambda_k P^{(k)})<f(x^{(k)}) \tag{3}$$
&#8195&#8195现在考察不同的$P^{(k)}$。假定$P^{(k)}$的模一定(且不为零)，并设$\triangledown f(x^{(k)})$(否则，$x^{(k)}$是平稳点)，使得(2)式成立的$P^{(k)}$有无限多个，为了使目标函数数值能得到尽量大的改善，必须寻求使$f(x^{(k)})^\top P^{(k)} $取最小值的$P^{(k)}$，因为有
$$f(x^{(k)})^\top P^{(k)}=\|f(x^{(k)})\| \cdot \|P^{(k)}\| \cdot \cos\theta$$
式中$\theta$为$f(x^{(k)})$和$P^{(k)}$的夹角。当$P^{(k)}$与$f(x^{(k)})$反向时，$\theta=180°,\cos\theta=-1$。这时式(2)成立，且其左端取得最小值。

#### 一维搜索
&#8195&#8195为了得到下一个近似极小值点，在选定了搜索方向之后，还要确定步长$\lambda$。当采用可接受点算法时，就是取某一$\lambda$进行试算，看是否满足不等式(3)，若上述不等式成立，就可以迭代下去。否则缩小$\lambda$使得其满足不等式(3)。
&#8195&#8195另一种方法就是在负梯度方向的一维搜索，来确定使$f(x^{(k)})$最小的$\lambda_k$。最常用的有`试探法(斐波拉契，0.618法)`，`插值法(抛物线插值，三次插值)`，`微积分中的求根法(切线法、二分法等)`等。

&#8195&#8195若$f(x)$具有二阶连续偏导数，在$x^{(k)}$作$f(x^{(k)}-\lambda \triangledown f(x^{(k)}) )$的泰勒展开：
$$f(x^{(k)}-\lambda \triangledown f(x^{(k)})) \approx f(x^{(k)}) -\triangledown f(x^{(k)})^\top \lambda \triangledown f(x^{(k)})  + \frac{1}{2}\lambda \triangledown f(x^{(k)})^\top H(x^{(k)}) \lambda \triangledown f(x^{(k)})$$
对$\lambda$求导并且令其等于零，则得**近似最佳步长**
$$\lambda_k=\frac{ \triangledown f(x^{(k)})^\top \triangledown f(x^{(k)}) }{\triangledown f(x^{(k)})^\top H(x^{(k)}) \triangledown f(x^{(k)})}  \tag{4}$$
其中$$H(x^{(k)})=\begin{bmatrix} 
\frac{\partial^2 f(x^{(k)}))}{\partial x_1^2} & \frac{\partial^2 f(x^{(k)}))}{\partial x_1 \partial x_2} & ... &\frac{\partial^2 f(x^{(k)}))}{\partial x_1 \partial x_n} \\ 
\frac{\partial^2 f(x^{(k)}))}{\partial x_2 \partial x_1} & \frac{\partial^2 f(x^{(k)}))}{\partial x_2^2} & ...&\frac{\partial^2 f(x^{(k)}))}{\partial x_2 \partial x_n} \\ 
...&&& \\
\frac{\partial^2 f(x^{(k)}))}{\partial x_n \partial x_1} & \frac{\partial^2 f(x^{(k)}))}{\partial x_n \partial x_2} & ...&\frac{\partial^2 f(x^{(k)}))}{\partial x_n^2} \\ 
\end{bmatrix}$$
为$f(x)$在点$x^{(k)}$处的**海赛(Hesse)矩阵**。
可见近似最佳步长不只与梯度有关，还与海赛矩阵H也有关系，计算起来比较麻烦。
&#8195&#8195有时，将搜索方向$P^{(k)}$的模长规格化为1，在这种情况下
$$P^{(k)}=\frac{-\triangledown f(x^{(k)})}{ \|\triangledown f(x^{(k)})\|}$$
同时，式(4)变为$$\lambda_k=\frac{ \triangledown f(x^{(k)})^\top \triangledown f(x^{(k)}) \|\triangledown f(x^{(k)})\| }{\triangledown f(x^{(k)})^\top H(x^{(k)}) \triangledown f(x^{(k)})}  $$

### 固定步长
有一点需要注意的是步长a固定时的大小，如果a太小，则会迭代很多次才找到最优解，若a太大，可能跳过最优，从而找不到最优解。
![步长变化](/images/lambda.png)

### 算法
#### 梯度下降法算法如下：
* 输入：目标函数$f(x)$，梯度函数$g(x)=-\triangledown f(x)$，计算精度$\varepsilon  $;
* 输出：$f(x)$的极小值点$x^*$。
(1). 取初始值$x^{(0)}\in R^n$，置k=0;
(2). 计算$f(x^{(k)})$;
(3). 计算梯度$g_k =g(x^{(k)})$，当$\left \|  g_k\right \| < \varepsilon$时，停止迭代，令$x^*=x^{(k)}$；否则令$P^{(k)}=-g(x^{(k)})$，求$\lambda_k$，使得$$f(x^{(k)}+\lambda_k p_k)=\min_{\lambda \geqslant 0}f(x^{(k)}+\lambda \cdot P^{(k)})$$
(4). 置$x^{(k+1)}=x^{(k)}+\lambda_k P^{(k)} $，计算$f(x^{(k+1)})$，当$\left \|  f(x^{(k+1)})-f(x^{(k)}) \right \| < \varepsilon$或$\left \|  x^{(k+1)} - x^{(k)}\right \| < \varepsilon$时，停止迭代，令$x^*=x^{(k)}$；否则置$k=k+1$，转(3)。

### 随机梯度下降

名字中已经体现了核心思想，即**随机选取一个点**做梯度下降，而不是遍历所有样本后进行参数迭代。

因为梯度下降法的代价函数计算需要遍历所有样本，而且是每次迭代都要遍历，直至达到局部最优解，在样本量庞大时就显得收敛速度比较慢了，计算量非常庞大。

随机梯度下降仅以当前样本点进行最小值求解，通常无法达到真正局部最优解，但可以比较接近。属于大样本兼顾计算成本的折中方案。

![两者比较](/images/RGD.jpg)
## 最后
&#8195&#8195当目标函数是凸函数时，梯度下降法的解是全局最优解。一般情况下，其解不保证是全局最优解。梯度下降法的收敛速度也未必是很快的。

#### 参考资料
> 《统计学习方法》李航 著  清华大学出版社
> 《运筹学》第四版 清华大学出版社
