---
title: 高斯混合模型
tags:
  - machine_leanring
  - python
categories:
  - machine_leanring
  - code
mathjax: true
date: 2018-05-17 23:30:24
---

## 混合高斯模型要解决的问题与一些假设

观察到从K个类中产生的样本$X=\{x_1^T,x_2^T,...,x_M^T\}^T\in R^{M\times N}$，M个样本，每个样本N维，但是每一个样本没有对应的标签标记该样本是属于哪一个类，如何确定这些样本最有可能的分布方式？这就是一个无监督学习问题。使用混合高斯模型解决该聚类问题时，假设每一个类内部的样本服从高斯分布，（并且类与类之间的互相独立？）。

## 基于以上假设如何对观察到的样本进行建模？

信息不完全，需要引入**隐变量**来表示每个样本具体属于哪一个类。引入变量$z_{mk}$表示**第m个样本**是否属于**第k类**，若第m个样本属于第k类则$z_{mk}=1$，否则$z_{mk}=0$。显然，对于一个样本，其对应的k个$z$中只有一个为1，其他都为0，因为一个样本只能属于一个类。所以有$\sum_{k=1}^{K}z_{mk}=1$。

对于某一个样本$x$，其被观察到的概率为$p(x)$，则其可由$x$与$z$的联合分布中消去$z$得到，即
$p(x)=\sum_{z}{p(x,z)} =\sum_{k=1}^{K}{p(x,z_{k}=1)}$。

由混合高斯模型的假设可知$p(x|z_{k}=1)=\mathcal{N}(x|\mu_k,\Sigma_k) $，即每个类的内部的样本服从高斯分布。

如果再假设先验：对于任意一个样本$x$，其属于第k类的概率为$p(z_k=1)=\pi_k$。可以理解为任意取一个样本$x$，忽略其本身的特征，其属于第k类的概率为$\pi_k$。也可看做第k类样本在总体样本中所占的比例，所以有$\sum_{k=1}^K\pi_k=1$。

则根据以上的假设可以得到$x$与$z$的联合分布为

$p(x,z_k=1)=p(x|z_k=1)p(z_k=1)=\pi_k \mathcal{N}(x|\mu_k,\Sigma_k)$。

所以$p(x)=\sum_{z}{p(x,z)} =\sum_{k=1}^{K}{p(x,z_{k}=1)}=\sum_{k=1}^K\pi_k \mathcal{N}(x|\mu_k,\Sigma_k)$。

至此，我们已经对样本建模完成，若果我们知道隐变量$z$的值的话，这个问题就直接是一个分类问题，以上就是一个高斯判别式模型GDA，直接求出$\mu_k$与$\Sigma_k$就可以得到我们需要的总体样本的分布$p(x)$。

对于一个新样本需要预测其所属的类别，可以通过贝叶斯公式得到：观察到一个样本$x$，其属于第k类的概率为
$$p(z_k=1|x)=\frac{p(x,z_k=1)}{p(x)} =\frac{\pi_k\mathcal{N}(x|\mu_k,\Sigma_k)}{\sum_{k=1}^K\pi_k\mathcal{N}(x|\mu_k,\Sigma_k)}$$
为方便将$p(z_k=1|x)$记做$\gamma(z_k)$。

但是我们不知道z的值，无法将每一个样本$x_m$对应到相应的$z_k$上去（即不知道哪个$z_{mk}=1$），所以无法计算出$\pi_k$，也无法根据似然函数去直接计算$\mu_k$与$\Sigma_k$。

##  直观理解
如图1，图中的点在我们看来明显分成两个聚类。这两个聚类中的点分别通过两个不同的正态分布随机生成而来。但是如果没有GMM，那么只能用一个的二维高斯分布来描述图1中的数据。图1中的椭圆即为二倍标准差的正态分布椭圆。这显然不太合理，毕竟肉眼一看就觉得应该把它们分成两类。
![图一](/images/GMM_2.png)
这时候就可以使用GMM了！如图2，数据在平面上的空间分布和图1一样，这时使用两个二维高斯分布来描述图2中的数据，分别记为$\mathcal{N}(\mu_1,\Sigma_1)$和$\mathcal{N}(\mu_2,\Sigma_2)$. 图中的两个椭圆分别是这两个高斯分布的二倍标准差椭圆。可以看到使用两个二维高斯分布来描述图中的数据显然更合理。实际上图中的两个聚类的中的点是通过两个不同的正态分布随机生成而来。如果将两个二维高斯分布$\mathcal{N}(\mu_1,\Sigma_1)$和$\mathcal{N}(\mu_2,\Sigma_2)$合成一个二维的分布，那么就可以用合成后的分布来描述图2中的所有点。最直观的方法就是对这两个二维高斯分布做线性组合，用线性组合后的分布来描述整个集合中的数据。这就是高斯混合模型（GMM）。
![image](/images/GMM_3.png)

## 为什么无法根据似然函数计算$\mu_k$与$\Sigma_k$？

写出似然函数：
$$p(X|\pi,\mu,\Sigma)=\prod_{m=1}^{M}p(x_m)= \prod_{m=1}^{M}(\sum_{k=1}^{K}\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k))$$

取对数：
$$\theta(\pi,\mu,\Sigma;X)=ln\ p(X|\pi,\mu,\Sigma)=ln\prod_{m=1}^{M}p(x_m)= ln\prod_{m=1}^{M}(\sum_{k=1}^{K}\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k))=\sum_{m=1}^Mln(\sum_{k=1}^{K}\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k))$$

缺少隐变量$z$的信息会导致：
1. $\pi_k$无法计算;
2. $ln$中的求和符号无法消除（在GDA的似然中样本$x_m$不属于的类根本就不会出现在$ln$中）。

对$\mu_k$求偏导
$$
\frac{\partial ln\ p(X|\pi,\mu,\Sigma)}{\partial \mu_k}=\frac{\partial \sum_{m=1}^Mln(\sum_{k=1}^{K}\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k))}{\partial \mu_k}$$$$=\sum_{m=1}^M\frac{\partial ln(\sum_{k=1}^{K}\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k))}{\partial \mu_k}=\sum_{m=1}^M\frac{\pi_k}{\sum_{k=1}^{K}\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k)}\frac{\partial \mathcal{N}(x_m|\mu_k,\Sigma_k)}{\partial \mu_k}
$$$$=\sum_{m=1}^M\frac{\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k)}{\sum_{k=1}^{K}\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k)}\frac{\partial (-\frac{1}{2}(x_m-\mu_k)^T\Sigma_k^{-1}(x_m-\mu_k))}{\partial \mu_k}$$$$=\sum_{m=1}^M\frac{\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k)}{\sum_{k=1}^{K}\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k)}(-\frac{1}{2})\Sigma_k(x_m-\mu_k) $$

令$\frac{\partial ln\ p(X|\pi,\mu,\Sigma)}{\partial \mu_k}=0$得
$$-\sum_{m=1}^M\frac{\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k)}{\sum_{k=1}^{K}\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k)}\Sigma_k(x_m-\mu_k)=0$$

假设$\Sigma_k$可逆，则两边同乘以$\Sigma_k^{-1}$可消去$\Sigma_k$，则可得
$$\mu_k\sum_{m=1}^M\frac{\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k)}{\sum_{k=1}^{K}\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k)}=\sum_{m=1}^M\frac{\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k)}{\sum_{k=1}^{K}\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k)}x_m$$

所以$$\mu_k=\frac{\sum_{m=1}^M\gamma(z_{mk})x_m}{\sum_{m=1}^M\gamma(z_{mk})}$$

观察该公式，每个类的类中心相当于所有样本的加权平均数，对于第k类，每个样本的权重$w_{km}=\frac{\gamma(z_{mk})}{\sum_{m=1}^M\gamma(z_{mk})}$。

对$\Sigma_k$求偏导
$$\frac{\partial ln\ p(X|\pi,\mu,\Sigma)}{\partial \Sigma_k}=\frac{\partial \sum_{m=1}^Mln(\sum_{k=1}^{K}\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k))}{\partial \Sigma_k}
$$$$=\sum_{m=1}^M\frac{\partial ln(\sum_{k=1}^{K}\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k))}{\partial \Sigma_k}=\sum_{m=1}^M\frac{\pi_k}{\sum_{k=1}^{K}\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k)}\frac{\partial \mathcal{N}(x_m|\mu_k,\Sigma_k)}{\partial \Sigma_k}
$$$$=\sum_{m=1}^M\frac{\pi_k}{\sum_{k=1}^{K}\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k)}\frac{1}{(2\pi)^{N/2}}\frac{\partial\frac{1}{|\Sigma_k|^{1/2}}exp\{-\frac{1}{2}(x_m-\mu_k)^T\Sigma_k^{-1}(x_m-\mu_k)\}}{\partial \Sigma_k}
$$$$=..........$$

最后令$\frac{\partial ln\ p(X|\pi,\mu,\Sigma)}{\partial \Sigma_k}=0$得到
$$\Sigma_k\sum_{m=1}^M\frac{\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k)}{\sum_{k=1}^{K}\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k)}=\sum_{m=1}^M\frac{\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k)}{\sum_{k=1}^{K}\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k)}(x_m-\mu_k)(x_m-\mu_k)^T$$

所以$$\Sigma_k=\sum_{m=1}^M\frac{\gamma(z_{mk})(x_m-\mu_k)(x_m-\mu_k)^T}{\sum_{m=1}^M\gamma(z_{mk})}$$

以上求导可以看出$\mu_k$与$\Sigma_k$**互相嵌套**，无法得到一个**闭式解(解析解)**。

在似然函数中$\pi_k$也属于未知量，也需要对其求导进行优化

在优化$\pi_k$使似然函数最小化的同时，$\pi_k$还需要满足条件$\sum_{k=1}^K\pi_k=1$。所以可以通过拉格朗日乘子将该约束加入似然函数得
$$\theta(\pi,\mu,\Sigma;X)=ln\ p(X|\pi,\mu,\Sigma)+\lambda(\sum_{k=1}^K\pi_k-1)=\sum_{m=1}^Mln(\sum_{k=1}^{K}\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k))+\lambda(\sum_{k=1}^K\pi_k-1)$$

对\pi_k求偏导
$$\frac{\partial \theta(\pi,\mu,\Sigma;X)}{\partial \pi_k}=\sum_{m=1}^M\frac{\partial ln(\sum_{k=1}^{K}\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k))}{\partial \pi_k}+\lambda\frac{\sum_{k=1}^K\pi_k-1}{\partial \pi_k}=\sum_{m=1}^M\frac{\mathcal{N}(x_m|\mu_k,\Sigma_k)}{\sum_{k=1}^{K}\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k)}+\lambda$$

令$\frac{\partial \theta(\pi,\mu,\Sigma;X)}{\partial \pi_k}=0$得
$$\sum_{m=1}^M\frac{\mathcal{N}(x_m|\mu_k,\Sigma_k)}{\sum_{k=1}^{K}\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k)}+\lambda=0\Rightarrow \sum_{m=1}^M\frac{\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k)}{\sum_{k=1}^{K}\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k)}+\lambda\pi_k=0$$

所以$\sum_{m=1}^M\gamma(z_{mk})+\lambda\pi_k=0$

所以
$$\sum_{k=1}^K(\sum_{m=1}^M\gamma(z_{mk})+\lambda\pi_k)=0\Rightarrow \sum_{k=1}^K\sum_{m=1}^M\gamma(z_{mk})+\lambda\sum_{k=1}^K\pi_k=0\Rightarrow \lambda=-\sum_{k=1}^K\sum_{m=1}^M\gamma(z_{mk})$$

带入原式得
$$\pi_k=\frac{\sum_{m=1}^M\gamma(z_{mk})}{\sum_{k=1}^K\sum_{m=1}^M\gamma(z_{mk})}$$

## EM方法

根据最初推导的模型，如果我们知道\pi_k，\mu_k与\Sigma_k，那么我们就相当于解决了问题，可以根据这三个参数得到所需的概率密度，从而可以对原样本进行划分，对新样本进行预测。然而经过最大似然求解，我们发现无法得到闭式解。

观察$\pi_k$，$\mu_k$与$\Sigma_k$的表达式，发现我们之所以我们计算出$\pi_k$，$\mu_k$与$\Sigma_k$是因为我们无法得到$\gamma(z_k)$，而无法计算$\gamma(z_k)$又是因为$\gamma(z_k)$依赖于$\pi_k$，$\mu_k$与$\Sigma_k$。

其依赖关系如下图
![图三](/images/GMM_1.jpg)

则从上图很容易想到一个迭代过程：
>1. 初始化$\pi_k$，$\mu_k$与$\Sigma_k$。
>2. 计算$\gamma(z_k)$，即预测对于一个样本x他属于哪个类$p(z_k=1|x)$。
>3. 对所有样本预测完成后根据$\gamma(z_k)$重新计算$\pi_k$，$\mu_k$与$\Sigma_k$。
>4. 若没有达到退出条件，从1.步继续迭代。

初始化$\pi_k$，$\mu_k$与$\Sigma_k$步骤，可以选择向样本空间随机布置几个$\Sigma=I$的高斯分布得到，也可以使用K-means算法进行初始聚类后，对每个类生成一个初始高斯分布得到。

** 完整混合高斯模型的EM算法如下：**

>1. 初始化$\pi_k$，$\mu_k$与$\Sigma_k$，可以选择向样本空间随机布置几个$\Sigma=I$的高斯分布得到，也可以使用K-means算法进行初始聚类后，对每个类生成一个初始高斯分布得到。

>2. （E step）计算$\gamma(z_k)$，即预测对于一个样本$x$他属于哪个类
$$p(z_k=1|x)=\frac{\pi_k\mathcal{N}(x|\mu_k,\Sigma_k)}{\sum_{k=1}^K\pi_k\mathcal{N}(x|\mu_k,\Sigma_k)}$$

>3. （M step）对所有样本预测完成后根据$\gamma(z_k)$重新计算$\pi_k$，$\mu_k$与$\Sigma_k$。
$$\mu_k^{new}=\frac{\sum_{m=1}^M\gamma(z_{mk})x_m}{\sum_{m=1}^M\gamma(z_{mk})},
\Sigma_k^{new}=\sum_{m=1}^M\frac{\gamma(z_{mk})(x_m-\mu_k)(x_m-\mu_k)^T}{\sum_{m=1}^M\gamma(z_{mk})}，
\pi_k^{new}=\frac{\sum_{m=1}^M\gamma(z_{mk})}{\sum_{k=1}^K\sum_{m=1}^M\gamma(z_{mk})}$$

>4. 评估似然函数
$$\theta(\pi^{new},\mu^{new},\Sigma^{new};X)=\sum_{m=1}^Mln(\sum_{k=1}^{K}\pi_k^{new}\mathcal{N}(x_m|\mu_k^{new},\Sigma_k^{new}))$$
若似然函数的变化没有低于某个阈值，继续迭代：
$$\pi_k=\pi_k^{new}，\mu_k=\mu_k^{new}，\Sigma_k=\Sigma_k^{new}。$$

其中第2步被称为**E step**，即期望（分类）(expectation)步骤，其根据目前已知的参数$\pi_k$，$\mu_k$与$\Sigma_k$对样本的类型进行预测。
第3步被称为**M step**，即最大化（似然函数）（maximization）步骤，根据2步得到的新的分类情况最大化似然函数$\theta(\pi^{new},\mu^{new},\Sigma^{new};X)$，即使用第3步中的三个公式计算$\pi_k$，$\mu_k$与$\Sigma_k$就相当于最大化了似然函数。

可以受到启示，要优化参数具有前面所展示的图中依赖关系的问题，均可以套用EM方法的迭代模式，对目标函数进行优化。

## 下面给出一个sklearn的例子 
``` python
import numpy as np
from sklearn import datasets
from sklearn.cluster import KMeans
from sklearn.mixture import GaussianMixture

x = np.array([[1.2,2.6],[2.8,3.9],[3.2,4],[3,3],[1,1],[-1,0.2],[-2,-1],[2,0]])

#K-Means
kmeans=KMeans(n_clusters=2)
kmeans.fit(x)
print( 'K-Means均值 = \n', kmeans.cluster_centers_ )
print(kmeans.predict(x))

gmm=GaussianMixture(n_components=2,covariance_type='full')
gmm.fit(x)
print('GMM均值 = \n', gmm.means_  )
print(gmm.fit(x))
```

