<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Treamy&#39;s website</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-04-24T07:09:59.976Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Treamy</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>logistic regression</title>
    <link href="http://yoursite.com/2018/04/23/logistic-regression/"/>
    <id>http://yoursite.com/2018/04/23/logistic-regression/</id>
    <published>2018-04-23T14:11:30.000Z</published>
    <updated>2018-04-24T07:09:59.976Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-逻辑斯蒂分布-logistic-distribution"><a href="#1-逻辑斯蒂分布-logistic-distribution" class="headerlink" title="1 逻辑斯蒂分布(logistic distribution)"></a>1 逻辑斯蒂分布(logistic distribution)</h1><p><strong>定义</strong>：设X是连续随机变量，X服从<code>logistic分布</code>是指X具有下列分布函数和密度函数：</p><p>$$ F = P(X \leq x)=\frac{1}{1+exp(-\frac{(x-\mu)}{\gamma})} $$<br>$$ f= F’(x) = \frac{exp(-\frac{(x-\mu)}{\gamma})}{\gamma[1+exp(-\frac{(x-\mu)}{\gamma})]^2} $$<br>式中，$\mu$为位置参数，$\gamma &gt; 0$为形状参数。</p><p>其分布图形如下：</p><p><img src="https://note.youdao.com/favicon.ico" alt="image"></p><p>F曲线在中心附近增长速度较快，在两端增长速度较慢。形状参数$\gamma$值越小，曲线在中心附近增长得越快。</p><h1 id="2-二项逻辑斯蒂回归模型"><a href="#2-二项逻辑斯蒂回归模型" class="headerlink" title="2 二项逻辑斯蒂回归模型"></a>2 二项逻辑斯蒂回归模型</h1><p><strong>定义</strong>：二项逻辑斯蒂回归模型是如下的条件概率分布<br>$$P(y=1\mid x) = \frac{\exp(\omega^\top x+b)}{1 + \exp(\omega^\top x+b)} \tag{1}$$<br>$$P(y=0\mid x) = \frac{1}{1 + \exp(\omega^\top x+b)}  \tag{2}$$<br>这里，$x\in R^n$是输入，$y\in {0,1 }$是输出，$\omega \in R^n$和$b\in R$是参数，$\omega$称为权值向量，$b$称为偏置，$\omega^\top x$为$w$和$x$的内积。</p><blockquote><p>对于给定的输入实例$x$，按照(1)式和(2)式可以分别求得$P(y=1\mid x)$和$P(y=0\mid x)$。逻辑斯蒂回归比较这两个条件概率值的大小，将实例$x$分到概率值较大的那一类。</p></blockquote><p><strong>定义</strong>：一个事件的几率(odds)是指该事件发生的概率与该事件不发生的概率的比值。如果事件发生的概率为$p$，那么该事件的几率是$\frac{p}{1-p}$，该事件的对数几率(log odds)或者logit函数是$$logit(p)=log\frac{p}{1-p}$$<br>对于逻辑斯蒂回归而言，由(1),(2)式得$$log\frac{P(y=1\mid x)}{1-P(y=1\mid x)}=\omega^\top x+b$$<br>也就是说，在逻辑斯蒂回归模型中，输出$y=1$的对数几率是由输入$x$的线性函数表示的模型。</p><h1 id="3-模型参数估计"><a href="#3-模型参数估计" class="headerlink" title="3 模型参数估计"></a>3 模型参数估计</h1><p>对于给定的训练数据集$T={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}$，其中$x_i\in R^n$，$y_i\in {0,1 }$，可以应用<strong>极大似然估计法</strong>估计模型参数，从而得到逻辑斯蒂回归模型。</p><p>设：$P(y=1\mid x)=\pi(x)$，$P(y=0\mid x)=1-\pi(x)$则似然函数为：<br>$$\prod_{i=0}^N [\pi(x_i)]^{y_i}[1-\pi(x_i)]^{1-y_i}$$<br>对数似然函数为：<br>$$<br>\begin{aligned}<br>L(\omega,b)&amp;=\sum_{i=0}^N [y_i\ln(\pi(x_i))+(1-y_i)\ln(1-\pi(x_i))] \<br>&amp;= \sum_{i=0}^N [y_i\ln\frac{\pi(x_i)}{1-\pi(x_i)}+\ln(1-\pi(x_i))]  \<br>&amp;=\sum_{i=0}^N \left [ y_i(\omega^\top x+b) -\ln \left ( 1+ exp(\omega^\top x+b) \right )\right ]<br>\end{aligned}$$<br>这样，问题就变成了以对数似然函数为目标函数的最优化问题，逻辑斯蒂回归学习中通常采用的方法是<strong>梯度下降法</strong>及<strong>牛顿法</strong>。</p><h1 id="4-梯度下降法求解"><a href="#4-梯度下降法求解" class="headerlink" title="4 梯度下降法求解"></a>4 梯度下降法求解</h1><p>利用梯度下降(上升)求解对数似然函数：$L(\omega,b)$<br>因为要使得似然函数最大，我们使用<strong>梯度上升法</strong>。<br>为了计算方便，我们将权值向量和输入向量加以<strong>扩充</strong>，仍记作$\omega,x$，即$\omega=(\omega^{(1)},\omega^{(2)},…,\omega^{(n)},b),\;x=(x^{(1)},x^{(2)},…,x^{(n)},1)$，这时$\omega_{new}^\top x_{new}=\omega_{old}^\top x_{old}+b_{old}$；<br>于是有$$l(\omega)=\sum_{i=0}^N \left [ y_i(\omega^\top x) -\ln \left ( 1+ exp(\omega^\top x) \right )\right ]$$<br>先求各个偏导数：<br>$\ln \left ( 1+ exp(\omega^\top x) \right )$<br>$$<br>\begin{aligned}<br>\frac{\partial l(\omega)}{\partial \omega_j}&amp;=\frac{\partial }{\partial \omega_j}\left (<br> \sum_{i=0}^N \left [ y_i(\omega^\top x) -\ln \left ( 1+ exp(\omega^\top x) \right )\right ]\right ) \<br> &amp;= y_i x_i^{(j)} - \frac{exp(w^\top x_i)}{1+exp(w^\top x_i)} x_i^{(j)}  \<br> &amp;=\left (  y_i +  \frac{exp(w^\top x_i)}{1+exp(w^\top x_i)} \right ) x_i^{(j)}  \<br> &amp;= ( y_i +  \pi(x_i)  ) x_i^{(j)}<br>\end{aligned}$$</p><p>得到参数的迭代公式：<br>$$\omega_j = \omega +\lambda \cdot ( y_i +  \pi(x_i)  ) x_i^{(j)} $$</p><h1 id="5-模型的优缺点"><a href="#5-模型的优缺点" class="headerlink" title="5 模型的优缺点"></a>5 模型的优缺点</h1><p>缺点：</p><ul><li>逻辑回归需要大样本量，因为最大似然估计在低样本量的情况下不如最小二乘法有效。</li><li>为防止过拟合和欠拟合，应该让模型构建的变量是显著的。</li><li>对模型中自变量多重共线性较为敏感，需要对自变量进行相关性分析，剔除线性相关的变量。</li></ul><p>优点：<br>模型更简单，好理解，实现起来，特别是大规模线性分类时比较方便</p><h1 id="6-模型实现"><a href="#6-模型实现" class="headerlink" title="6 模型实现"></a>6 模型实现</h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-逻辑斯蒂分布-logistic-distribution&quot;&gt;&lt;a href=&quot;#1-逻辑斯蒂分布-logistic-distribution&quot; class=&quot;headerlink&quot; title=&quot;1 逻辑斯蒂分布(logistic distribution)&quot;
      
    
    </summary>
    
      <category term="machine_leanring" scheme="http://yoursite.com/categories/machine-leanring/"/>
    
      <category term="code" scheme="http://yoursite.com/categories/machine-leanring/code/"/>
    
    
      <category term="machine_leanring" scheme="http://yoursite.com/tags/machine-leanring/"/>
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>my kmeans</title>
    <link href="http://yoursite.com/2018/04/15/my-kmeans/"/>
    <id>http://yoursite.com/2018/04/15/my-kmeans/</id>
    <published>2018-04-15T12:58:05.000Z</published>
    <updated>2018-04-15T12:58:05.168Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="machine_leanring" scheme="http://yoursite.com/categories/machine-leanring/"/>
    
      <category term="code" scheme="http://yoursite.com/categories/machine-leanring/code/"/>
    
    
      <category term="machine_leanring" scheme="http://yoursite.com/tags/machine-leanring/"/>
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>梯度下降法</title>
    <link href="http://yoursite.com/2018/04/02/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/"/>
    <id>http://yoursite.com/2018/04/02/梯度下降法/</id>
    <published>2018-04-02T06:12:51.000Z</published>
    <updated>2018-04-24T06:54:31.879Z</updated>
    
    <content type="html"><![CDATA[<h1 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h1><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>梯度下降法是求解<strong>无约束最优化</strong>问题的一种最常用的<strong>迭代法</strong>。顾名思义，梯度下降法的计算过程就是沿梯度下降的方向求解极小值（也可以沿梯度上升方向求解极大值）。</p><p>假设$f(x)$是$R^n$上具有一阶连续偏导数的函数，要求解的无约束最优化问题是$$\min_{x\in R^n} f(x)$$<br>$x^*$表示目标函数的极小值点。</p><p>选取适当的初值$x^{(0)}$，不断迭代，更新$x$的值，进行目标函数的极小化，直到收敛。由于负梯度方向是使函数值下降最快的方向，在迭代的每一步，以负梯度方向更新$x$的值，从而达到减少函数值的目的。</p><p>由于$f(x)$具有一阶连续偏导数，若第k次迭代值为$x^{(k)}$，则可将$f(x)$在$x^{(k)}$附近进行一阶泰勒展开：$$f(x)=f(x^{(k)})+g_k^ \top \cdot(x-x^{(k)})$$<br>其中$g_k =g(x^{(k)})= -\triangledown f(x^{(k)}) $为$f(x)$在$x^{(k)}$的梯度(梯度上升为正号)。<br>求出第k+1次迭代值$x^{(k+1)}$: $$x^{(k+1)}\leftarrow x^{(k)}+\lambda_k p_k $$<br>其中，$p_k$是搜索方向，取负梯度方向$p_k=-\triangledown f(x^{(k)}) $，$\lambda_k$是步长，由一维搜索确定，即$\lambda_k$使得$$f(x^{(k)}+\lambda_k p_k)=\min_{\lambda \geqslant 0}f(x^{(k)}+\lambda \cdot p_k)$$</p><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p>梯度下降法算法如下：</p><ul><li>输入：目标函数$f(x)$，梯度函数$g(x)=-\triangledown f(x)$，计算精度$\varepsilon  $;</li><li>输出：$f(x)$的极小值点$x^<em>$。<br>(1). 取初始值$x^{(0)}\in R^n$，置k=0;<br>(2). 计算$f(x^{(k)})$;<br>(3). 计算梯度$g_k =g(x^{(k)})$，当$\left |  g_k\right | &lt; \varepsilon$时，停止迭代，令$x^</em>=x^{(k)}$；否则令$p_k=-g(x^{(k)})$，求$\lambda_k$，使得$$f(x^{(k)}+\lambda_k p_k)=\min_{\lambda \geqslant 0}f(x^{(k)}+\lambda \cdot p_k)$$<br>(4). 置$x^{(k+1)}=x^{(k)}+\lambda_k p_k $，计算$f(x^{(k+1)})$，当$\left |  f(x^{(k+1)})-f(x^{(k)}) \right | &lt; \varepsilon$或$\left |  x^{(k+1)} - x^{(k)}\right | &lt; \varepsilon$时，停止迭代，令$x^*=x^{(k)}$；否则置$k=k+1$，转(3)。</li></ul><p>当目标函数是凸函数时，梯度下降法的解是全局最优解。一般情况下，其解不保证是全局最优解。梯度下降法的收敛速度也未必是很快的。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;梯度下降法&quot;&gt;&lt;a href=&quot;#梯度下降法&quot; class=&quot;headerlink&quot; title=&quot;梯度下降法&quot;&gt;&lt;/a&gt;梯度下降法&lt;/h1&gt;&lt;h2 id=&quot;原理&quot;&gt;&lt;a href=&quot;#原理&quot; class=&quot;headerlink&quot; title=&quot;原理&quot;&gt;&lt;/a&gt;原
      
    
    </summary>
    
      <category term="math" scheme="http://yoursite.com/categories/math/"/>
    
      <category term="algorithm" scheme="http://yoursite.com/categories/math/algorithm/"/>
    
    
      <category term="math" scheme="http://yoursite.com/tags/math/"/>
    
      <category term="algorithm" scheme="http://yoursite.com/tags/algorithm/"/>
    
  </entry>
  
  <entry>
    <title>my decision tree id3</title>
    <link href="http://yoursite.com/2018/03/27/my-decision-tree-id3/"/>
    <id>http://yoursite.com/2018/03/27/my-decision-tree-id3/</id>
    <published>2018-03-27T02:41:18.000Z</published>
    <updated>2018-03-27T06:56:09.575Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>  本文主要讲述一下决策树的基本算法–ID3生成决策树算法。一开始看例子的时候，我觉得决策树好简单呀，应该实现起来用<code>pandas</code>也能像实现朴素贝叶斯一样容易实现，可是到实践的时候才发现，这个实现起来也好难啊orz。刚开始尝试直接通过算gini指数用<code>CART</code>算法生成树，但发现当两个的gini指数相同时我的程序就没法择优选择了。。。这个还有待改进。。最后参考了一下机器学习实战这本书把id3生成和可视化决策树实现了（不得不说这本书可视化部分写得我都看不懂了。。。）。</p><h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点和有向边组成。结点有两种类型：内部节点和叶节点，内部节点表示一个特征或属性，叶节点表示一个类。 </p><h3 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h3><p>分类决策树的核心思想就是在一个数据集中找到一个最优特征，然后从这个特征的选值中找一个最优候选值(这段话稍后解释)，根据这个最优候选值将数据集分为两个子数据集，然后递归上述操作，直到满足指定条件为止。</p><h3 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h3><h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><blockquote><p>1：理解和解释起来简单，且决策树模型可以想象<br>2：需要准备的数据量不大，而其他的技术往往需要很大的数据集，需要创建虚拟变量，去除不完整的数据，但是该算法对于丢失的数据不能进行准确的预测<br>3：决策树算法的时间复杂度(即预测数据)是用于训练决策树的数据点的对数<br>4：能够处理数字和数据的类别（需要做相应的转变），而其他算法分析的数据集往往是只有一种类型的变量<br>5：能够处理多输出的问题<br>6：使用白盒模型，如果给定的情况是在一个模型中观察到的，该条件的解释很容易解释的布尔逻辑，相比之下，在一个黑盒子模型（例如人工神经网络），结果可能更难以解释<br>7：可能使用统计检验来验证模型，这是为了验证模型的可靠性<br>8：从数据结果来看，它执行的效果很好，虽然它的假设有点违反真实模型</p></blockquote><h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><blockquote><p>1：决策树算法学习者可以创建复杂的树，但是没有推广依据，这就是所谓的过拟合，为了避免这种问题，出现了剪枝的概念，即设置一个叶子结点所需要的最小数目或者设置树的最大深度<br>2：决策树的结果可能是不稳定的，因为在数据中一个很小的变化可能导致生成一个完全不同的树，这个问题可以通过使用集成决策树来解决<br>3：众所周知，学习一恶搞最优决策树的问题是NP——得到几方面完全的优越性，甚至是一些简单的概念。因此，实际决策树学习算法是基于启发式算法，如贪婪算法，寻求在每个节点上的局部最优决策。这样的算法不能保证返回全局最优决策树。这可以减轻训练多棵树的合奏学习者，在那里的功能和样本随机抽样更换。<br>4：这里有一些概念是很难的理解的，因为决策树本身并不难很轻易的表达它们，比如说异或校验或复用的问题。<br>5：决策树学习者很可能在某些类占主导地位时创建有有偏异的树，因此建议用平衡的数据训练决策树<br>–当然最重要的还是容易<strong>过拟合</strong>！，所以迫切需要剪纸或者集成学习。</p></blockquote><h3 id="举个栗子"><a href="#举个栗子" class="headerlink" title="举个栗子"></a>举个栗子</h3><p>各位立志于脱单的单身男女在找对象的时候就已经完完全全使用了决策树的思想。假设一位母亲在给女儿介绍对象时，有这么一段对话：</p><blockquote><p>母亲：给你介绍个对象。<br>女儿：年纪多大了？<br>母亲：26。<br>女儿：长的帅不帅？<br>母亲：挺帅的。<br>女儿：收入高不？<br>母亲：不算很高，中等情况。<br>女儿：是公务员不？<br>母亲：是，在税务局上班呢。<br>女儿：那好，我去见见。</p></blockquote><p>这个女生的决策过程就是典型的分类决策树。相当于对年龄、外貌、收入和是否公务员等特征将男人分为两个类别：见或者不见。假设这个女生的决策逻辑如下：<br><img src="/images/tree1.png" alt="image"><br>上图完整表达了这个女孩决定是否见一个约会对象的策略，其中绿色结点（内部结点）表示判断条件，橙色结点（叶结点）表示决策结果，箭头表示在一个判断条件在不同情况下的决策路径，图中红色箭头表示了上面例子中女孩的决策过程。</p><p>这幅图基本可以算是一棵决策树，说它“基本可以算”是因为图中的判定条件没有量化，如收入高中低等等，还不能算是严格意义上的决策树，如果将所有条件量化，则就变成真正的决策树了。（以上的决策树模型纯属瞎编乱造，旨在直观理解决策树，不代表任何女生的择偶观，各位女同志无须在此挑刺。。。）</p><h3 id="决策树的学习"><a href="#决策树的学习" class="headerlink" title="决策树的学习"></a>决策树的学习</h3><p>决策树学习算法包含特征选择、决策树的生成与剪枝过程。决策树的学习算法通常是递归地选择最优特征，并用最优特征对数据集进行分割。开始时，构建根结点，选择最优特征，该特征有几种值就分割为几个子集，每个子集分别递归调用此方法，返回结点，返回的结点就是上一层的子结点。直到所有特征都已经用完，或者数据集只有一维特征为止。（这里就不介绍关于决策树的剪枝过程，日后再介绍）</p><h4 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h4><p>特征选择问题希望选取对训练数据具有良好分类能力的特征，这样可以提高决策树学习的效率。如果利用一个特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征是没有分类能力的（对象是否喜欢打游戏应该不会成为关键特征吧，也许也会……）。为了解决特征选择问题，找出最优特征，先要介绍一些信息论里面的概念。 </p><ol><li><p>熵（entropy）<br>熵是表示随机变量不确定性的度量。设$X$是一个取有限个值的离散随机变量，其概率分布为$$P(X=x_i)=p_i, i=1,2,…,n$$<br>则随机变量的熵定义为$$entropy(X) = -\sum_{i=1}^n P_ilog_2 P_i$$<br>另外，$0log0=0$，当对数的底为2时，熵的单位为bit；为e时，单位为nat。<br><strong>熵越大</strong>，随机变量的<strong>不确定性就越大</strong>。<br>从定义可验证$0&lt;=H(p)&lt;=logn$.<br>python实现计算如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">calc_entropy = <span class="keyword">lambda</span> P_: sum(map(<span class="keyword">lambda</span> p: -p * np.log2(p), P_))</span><br><span class="line"><span class="comment"># 其中P_为X的概率分布，p为X取某个随机变量的概率</span></span><br></pre></td></tr></table></figure></li><li><p>条件熵（conditional entropy）<br>设有随机变量$(X,Y)$，其联合概率分布为$$P(X=x_i,Y=y_i)=p_{ij}, i=1,2,…,n;j=1,2,…,m$$条件熵$H(Y|X)$表示在<strong>已知随机变量X的条件下</strong>随机变量Y的不确定性。随机变量X给定的条件下随机变量Y的条件熵$H(Y|X)$，定义为X给定条件下Y的条件概率分布的熵对X的数学期望$$entropy(Y|X) = \sum_{i=1}^k p_i H(Y|X=x_i)$$这里$p_i=P(X=x_i), i=1,2,…,n$。<br>用python实现求条件熵时，只需要把P_更换为feat_value_cate_P再调用calc_entropy，然后把所有分组的概率与对应的熵相乘再相加：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># group为Y对于这个特征X取不同的值的分组</span></span><br><span class="line"><span class="keyword">for</span> feat_value, group <span class="keyword">in</span> groups:</span><br><span class="line">    feat_value_P = len(group) / len(df)  <span class="comment"># 特征X取某值的概率</span></span><br><span class="line">    feat_value_cate_P = group[cate].value_counts() / group[cate].count()  <span class="comment"># 特征X取某值对应不同的类别的概率</span></span><br><span class="line">    feat_value_entropy += feat_value_P * calc_entropy(feat_value_cate_P)</span><br></pre></td></tr></table></figure></li><li><p>信息增益（information gain）<br>信息增益表示<strong>得知特征X的信息而使得类Y的信息的不确定性减少的程度</strong>。特征A对训练数据集D的信息增益$g(D,A)$，定义为集合D的<code>经验熵H(D)</code>与特征A给定条件下D的<code>经验条件熵H(D|A)</code>之<strong>差</strong>，即$$g(D,A)=H(D)−H(D|A)$$<br>这个差又称为互信息，表示由于特征A而使得对数据集D的分类不确定性减少的程度。信息增益大的特征具有更强的分类能力。<br>设训练数据为$D$，$|D|$表示其样本容量，即样本个数。设$D$有$K$个类$C_k$，$k=1,2,…,K$，$|C_k|$为属于类$C_k$的样本个数，$\sum_{k=1}^K |C_k|=|D|$. 设特征A有n个不同的取值${a_1,a_2,…a_n}$, 根据特征A 的取值将D划分为n个子集$D_1,D_2,…,D_n$, $|D_i|$为的样本$D_i$个数，$\sum_{i=1}^n |D_i|=|D|$。记子集$D_i$中属于类$C_k$的样本的集合为$D_{ik}$，即$D_{ik}=D_i\bigcap C_k$，$|D_{ik}|$为$D_{ik}$的样本个数。</p><blockquote><p>计算信息增益的算法如下： </p></blockquote></li></ol><ul><li>输入：训练数据集$D$和特征$A$；</li><li>输出：特征A对训练数据集$D$的信息增益$g(D,A)$.</li><li>计算数据集D的经验熵H(D)<br>$$H(D)=-\sum_{i=1}^k \frac{|C_k|}{|D|} log_2 \frac {|C_k|}{|D|}$$</li><li>计算特征A对数据集D的经验条件熵$H(D|A)$<br>$$H(D|A)=\sum_{i=1}^n \frac{|D_i|}{|D|}H(D_i)=\sum_{i=1}^n  \frac{|D_i|}{|D|}  \sum_{i=1}^K \frac{|D_{ik}|}{|D|} log_2 \frac{|D_{ik}|}{|D|}$$</li><li>计算信息增益$$g(D,A)=H(D)−H(D|A)$$</li></ul><h4 id="决策树的生成"><a href="#决策树的生成" class="headerlink" title="决策树的生成"></a>决策树的生成</h4><p>本次我们只介绍ID3算法，ID3算法由Ross Quinlan发明，建立在“奥卡姆剃刀”的基础上：越是小型的决策树越优于大的决策树（be simple简单理论）。ID3算法中根据信息增益评估和选择特征，每次选择信息增益最大的特征作为判断模块建立子结点。ID3算法可用于划分标称型数据集，没有剪枝的过程，为了去除过度数据匹配的问题，可通过裁剪合并相邻的无法产生大量信息增益的叶子节点（例如设置信息增益阀值）。使用信息增益的话其实是有一个缺点，那就是它偏向于具有大量值的属性。就是说在训练集中，某个属性所取的不同值的个数越多，那么越有可能拿它来作为分裂属性，而这样做有时候是没有意义的，另外ID3不能处理连续分布的数据特征，于是就有了C4.5算法。CART算法也支持连续分布的数据特征。<br><img src="/images/treeid3.jpg" alt="image"><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select</span><span class="params">(df, features, cate , H_D)</span>:</span></span><br><span class="line">    gain_dict = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> feat <span class="keyword">in</span> features:</span><br><span class="line">        groups = df.groupby(feat)</span><br><span class="line">        feat_value_entropy = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> feat_value, group <span class="keyword">in</span> groups:</span><br><span class="line">            feat_value_P = len(group) / len(df)  <span class="comment"># 特征取某值的概率</span></span><br><span class="line">            feat_value_cate_P = group[cate].value_counts() / group[cate].count()  <span class="comment"># 特征取某值对应不同的类别的概率</span></span><br><span class="line">            feat_value_entropy += feat_value_P * calc_entropy(feat_value_cate_P)</span><br><span class="line">        imfor_gain = H_D - feat_value_entropy</span><br><span class="line">        gain_dict[feat] = imfor_gain</span><br><span class="line">    <span class="keyword">return</span> max(gain_dict, key=<span class="keyword">lambda</span> x:gain_dict[x])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_tree</span><span class="params">(df, cate, H_D )</span>:</span></span><br><span class="line">    feat = df.columns[:<span class="number">-1</span>].tolist()</span><br><span class="line">    cate_values = df[cate].unique()</span><br><span class="line">    <span class="keyword">if</span> len(cate_values)==<span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> cate_values[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">if</span> len(feat) == <span class="number">0</span>: <span class="comment"># 用完所有特征后</span></span><br><span class="line">        temp = df[cate].value_counts().to_dict()</span><br><span class="line">        <span class="keyword">return</span> max(temp, key=<span class="keyword">lambda</span> x:temp[x]) <span class="comment"># 取最多的类别作为返回值</span></span><br><span class="line">    best_feat = select(df, feat, cate, H_D)</span><br><span class="line">    my_tree = &#123; best_feat:&#123;&#125; &#125;</span><br><span class="line">    unique_feat_values = df[best_feat].unique()</span><br><span class="line">    <span class="keyword">for</span> feat_value <span class="keyword">in</span> unique_feat_values:</span><br><span class="line">        df_ = df[df[best_feat] == feat_value].copy()</span><br><span class="line">        df_ =  df_.drop(best_feat, axis=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 这里一定不能是df，必须是一个新的df_，才能使递归*中的feat*越来越小</span></span><br><span class="line">        my_tree[best_feat][feat_value] = create_tree(df_, cate, H_D )</span><br><span class="line">    <span class="keyword">return</span> my_tree</span><br></pre></td></tr></table></figure></p><p>我们这里用Python语言的字典套字典类型存储树的信息，简单方便。当然也可以定义一个新的数据结构存储树。<br>来生成一个树：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">df = pd.read_excel(<span class="string">"TreeData.xlsx"</span>, index_col=<span class="string">"id"</span>)</span><br><span class="line">cate = df.columns[<span class="number">-1</span>]</span><br><span class="line">P_cate = df[cate].value_counts() / df[cate].count()</span><br><span class="line">H_D = calc_entropy(P_cate)</span><br><span class="line">tree = create_tree(df,cate, H_D)</span><br><span class="line">print(tree)</span><br><span class="line"><span class="comment"># &#123;'有自己的房子': &#123;'否': &#123;'有工作': &#123;'否': '否', '是': '是'&#125;&#125;, '是': '是'&#125;&#125;</span></span><br></pre></td></tr></table></figure></p><h4 id="决策树的可视化"><a href="#决策树的可视化" class="headerlink" title="决策树的可视化"></a>决策树的可视化</h4><p>我们主要用python的matplotlib来处理图像，它的annotate很方便用于注释。（以下代码来源：机器学习实战，我对其简单的更改了一下）<br>先获得叶子节点个数和树的深度：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_leafs_num</span><span class="params">(tree_)</span>:</span></span><br><span class="line">    num_leafs = <span class="number">0</span></span><br><span class="line">    first_key = list(tree_.keys())[<span class="number">0</span>]</span><br><span class="line">    second_dict = tree_[first_key]</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> second_dict.keys():</span><br><span class="line">        <span class="keyword">if</span> type(second_dict[key]).__name__ == <span class="string">"dict"</span>:</span><br><span class="line">            num_leafs += get_leafs_num(second_dict[key])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            num_leafs += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> num_leafs</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_tree_depth</span><span class="params">(tree_)</span>:</span></span><br><span class="line">    max_depth = <span class="number">0</span></span><br><span class="line">    first_key = list(tree_.keys())[<span class="number">0</span>]</span><br><span class="line">    second_dict = tree_[first_key]</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> second_dict.keys():</span><br><span class="line">        <span class="keyword">if</span> type(second_dict[key]).__name__ == <span class="string">"dict"</span>: <span class="comment"># 如果还是字典，继续深入</span></span><br><span class="line">            this_depth = <span class="number">1</span> + get_tree_depth(second_dict[key])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            this_depth = <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> this_depth &gt; max_depth:</span><br><span class="line">            max_depth = this_depth</span><br><span class="line">    <span class="keyword">return</span> max_depth</span><br></pre></td></tr></table></figure></p><p>然后再画图：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.rcParams[<span class="string">'font.sans-serif'</span>]=[<span class="string">'SimHei'</span>] </span><br><span class="line">plt.rcParams[<span class="string">'axes.unicode_minus'</span>]=<span class="keyword">False</span> </span><br><span class="line"></span><br><span class="line">decision_node=&#123;<span class="string">"boxstyle"</span>: <span class="string">"sawtooth"</span>, <span class="string">"fc"</span>: <span class="string">"0.8"</span>, &#125;</span><br><span class="line">leaf_node=&#123;<span class="string">"boxstyle"</span>: <span class="string">"round4"</span>, <span class="string">"fc"</span>: <span class="string">"0.8"</span>&#125;</span><br><span class="line">arrow_args=&#123;<span class="string">"arrowstyle"</span>: <span class="string">"&lt;-"</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_node</span><span class="params">(node_txt, centerPt, parentPt, node_type)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> ax1</span><br><span class="line">    ax1.annotate(node_txt, xy=parentPt, xycoords=<span class="string">'axes fraction'</span>,xytext=centerPt,</span><br><span class="line">        textcoords=<span class="string">'axes fraction'</span>,va=<span class="string">"center"</span>, ha=<span class="string">"center"</span>, bbox=node_type, arrowprops=arrow_args)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_mid_text</span><span class="params">(cntrPt, parentPt, txt_string)</span>:</span>  <span class="comment"># 在两个节点之间的线上写上字</span></span><br><span class="line">    <span class="keyword">global</span> ax1</span><br><span class="line">    xMid = (parentPt[<span class="number">0</span>] - cntrPt[<span class="number">0</span>]) / <span class="number">2.0</span> + cntrPt[<span class="number">0</span>]</span><br><span class="line">    yMid = (parentPt[<span class="number">1</span>] - cntrPt[<span class="number">1</span>]) / <span class="number">2.0</span> + cntrPt[<span class="number">1</span>]</span><br><span class="line">    ax1.text(xMid, yMid, txt_string)  <span class="comment"># text() 的使用</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_tree</span><span class="params">( tree_, parent_point, node_txt)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> ax1,xOff,yOff,totalD,totalW</span><br><span class="line">    num_leafs = get_leafs_num(tree_)</span><br><span class="line">    depth = get_tree_depth(tree_)</span><br><span class="line">    first_key = list(tree_.keys())[<span class="number">0</span>]</span><br><span class="line">    center_point = (xOff + (<span class="number">1.0</span> + float(num_leafs)) / <span class="number">2.0</span> / totalW, yOff)</span><br><span class="line">    plot_mid_text( center_point, parent_point, node_txt)  <span class="comment"># 在父子节点间填充文本信息</span></span><br><span class="line">    plot_node(first_key, center_point, parent_point, decision_node)  <span class="comment"># 绘制带箭头的注解</span></span><br><span class="line">    second_dict = tree_[first_key]</span><br><span class="line">    yOff = yOff - <span class="number">1.0</span> / totalD</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> second_dict.keys():</span><br><span class="line">        <span class="keyword">if</span> type(second_dict[key]).__name__ == <span class="string">'dict'</span>:  <span class="comment"># 判断是不是字典，</span></span><br><span class="line">            plot_tree(second_dict[key], center_point, str(key))  <span class="comment"># 递归绘制树形图</span></span><br><span class="line">        <span class="keyword">else</span>:  <span class="comment"># 如果是叶节点</span></span><br><span class="line">            xOff = xOff + <span class="number">1.0</span> / totalW</span><br><span class="line">            plot_node(second_dict[key], (xOff, yOff), center_point, leaf_node)</span><br><span class="line">            plot_mid_text((xOff, yOff), center_point, str(key))</span><br><span class="line">    yOff = yOff + <span class="number">1.0</span> / totalD</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_tree</span><span class="params">(tree_)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> ax1,xOff,yOff,totalD,totalW</span><br><span class="line">    fig = plt.figure(<span class="number">1</span>, facecolor=<span class="string">'white'</span>)</span><br><span class="line">    fig.clf()  <span class="comment"># 清空绘图区</span></span><br><span class="line">    axprops = dict(xticks=[], yticks=[])</span><br><span class="line">    ax1 = plt.subplot(<span class="number">111</span>, frameon=<span class="keyword">False</span>, **axprops)</span><br><span class="line">    totalW = float(get_leafs_num(tree_))</span><br><span class="line">    totalD = float(get_tree_depth(tree_))</span><br><span class="line">    xOff = <span class="number">-0.5</span> / totalW  <span class="comment"># 追踪已经绘制的节点位置 初始值为 将总宽度平分 在取第一个的一半</span></span><br><span class="line">    yOff = <span class="number">1.0</span></span><br><span class="line">    plot_tree(tree_, (<span class="number">0.5</span>, <span class="number">1.0</span>), <span class="string">''</span>)  <span class="comment"># 调用函数，并指出根节点源坐标</span></span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure></p><p>下面用一个实例来可视化一下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tree = &#123;<span class="string">'有自己的房子'</span>: &#123;<span class="string">'否'</span>: &#123;<span class="string">'有工作'</span>: &#123;<span class="string">'否'</span>: <span class="string">'否'</span>, <span class="string">'是'</span>: <span class="string">'是'</span>&#125;&#125;, <span class="string">'是'</span>: <span class="string">'是'</span>&#125;&#125;</span><br><span class="line"></span><br><span class="line">print(tree)</span><br><span class="line">show_tree(tree)</span><br></pre></td></tr></table></figure></p><p>可视化结果如下：<br><img src="/images/tree2.png" alt="image"></p><blockquote><ul><li>由于篇幅过长，完整代码（结构化封装）就不在这里给出，详情参见我的<br><a href="https://github.com/Interesting6/my_machine_learning/blob/master/my_decision_tree_id3.py" target="_blank" rel="noopener">GitHub</a>。</li></ul></blockquote><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>这里我们采用uci的lense<a href="http://archive.ics.uci.edu/ml/datasets/Lenses" target="_blank" rel="noopener">隐形眼镜测试集</a>,总样本为24个，四个特征，三个类别。我们通过网络爬虫，直接从该网址抓取数据，并转换为dataframe类型。</p><blockquote><p>– 3 Classes:<br>     1 : the patient should be fitted with hard contact lenses,<br>     2 : the patient should be fitted with soft contact lenses,<br>     3 : the patient should not be fitted with contact lenses.</p></blockquote><blockquote><p>– 4 Features:</p><pre><code>1. age of the patient: (1) young, (2) pre-presbyopic, (3) presbyopic2. spectacle prescription:  (1) myope, (2) hypermetrope3. astigmatic:     (1) no, (2) yes4. tear production rate:  (1) reduced, (2) normal</code></pre></blockquote><p>下面我们给出代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ID3 <span class="keyword">import</span> ID3_tree</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data</span><span class="params">()</span>:</span></span><br><span class="line">    url = <span class="string">"http://archive.ics.uci.edu/ml/machine-learning-databases/lenses/lenses.data"</span></span><br><span class="line">    data = requests.get(url).text</span><br><span class="line">    verctors = data.split(<span class="string">'\n'</span>)</span><br><span class="line">    verctors = [ver.split() <span class="keyword">for</span> ver <span class="keyword">in</span> verctors]</span><br><span class="line">    features = [<span class="string">"id"</span>,<span class="string">"age"</span>,<span class="string">"prescript"</span>,<span class="string">"astigmatic"</span>,<span class="string">"tearRate"</span>,<span class="string">"category"</span>]</span><br><span class="line">    df = pd.DataFrame(verctors,columns=features, dtype=int)</span><br><span class="line">    df = df.set_index(<span class="string">"id"</span>).dropna()</span><br><span class="line">    key_list = [&#123;<span class="string">"1"</span>:<span class="string">"young"</span>, <span class="string">"2"</span>:<span class="string">"pre-presbyopic"</span>, <span class="string">"3"</span>:<span class="string">"presbyopic"</span>&#125;, &#123;<span class="string">"1"</span>: <span class="string">"myope"</span>, <span class="string">"2"</span>: <span class="string">"hypermetrope"</span>&#125;</span><br><span class="line">    , &#123;<span class="string">"1"</span>: <span class="string">"no"</span>, <span class="string">"2"</span>: <span class="string">"yes"</span>&#125;, &#123;<span class="string">"1"</span>: <span class="string">"reduced"</span>, <span class="string">"2"</span>:<span class="string">"normal"</span>&#125;,&#123;<span class="string">"1"</span>:<span class="string">"hard"</span>,<span class="string">"2"</span>:<span class="string">"soft"</span>,<span class="string">"3"</span>:<span class="string">"no lenses"</span>&#125;]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        df.iloc[:,i] = df.iloc[:,i].apply(<span class="keyword">lambda</span> x:key_list[i][x])</span><br><span class="line">    <span class="keyword">return</span> df</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    df = get_data()</span><br><span class="line">    <span class="comment"># print(df)</span></span><br><span class="line">    id3_tree = ID3_tree(df)</span><br><span class="line">    id3_tree = id3_tree.train(df)</span><br><span class="line">    my_tree = id3_tree.my_tree</span><br><span class="line">    print(my_tree)</span><br><span class="line">    id3_tree.show_tree(my_tree)</span><br></pre></td></tr></table></figure></p><p>得出的决策树如下：<br><img src="/images/tree3.png" alt="image"></p><h2 id="后续"><a href="#后续" class="headerlink" title="后续"></a>后续</h2><p>由于本文只给出了ID3算法生成决策树和决策树的可视化，日后我将继续给出C4.5的生成决策树算法、决策树的减枝问题与及CART分类和回归树的构造。然后我们还可以把它拓宽，引入集成学习的随机森林。</p><p>作者时间精力有限，我就先写到这里啦。如有疑问，记得联系我哦。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><blockquote><p>《统计学习方法》李航 著  清华大学出版社<br>《机器学习实战》Peter Harrington 著 人民邮电出版社</p></blockquote><h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>如果你觉得本文对你有帮助的话，不如给作者一点打赏吧~<br>| <img src="/images/alipay.jpg" alt="image"> | <img src="/images/wechatpay.png" alt="image"> |<br>谢谢！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;  本文主要讲述一下决策树的基本算法–ID3生成决策树算法。一开始看例子的时候，我觉得决策树好简单呀，应该实现起来用&lt;code&gt;pandas
      
    
    </summary>
    
      <category term="machine_leanring" scheme="http://yoursite.com/categories/machine-leanring/"/>
    
      <category term="code" scheme="http://yoursite.com/categories/machine-leanring/code/"/>
    
    
      <category term="machine_leanring" scheme="http://yoursite.com/tags/machine-leanring/"/>
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>Radial basis function kernel</title>
    <link href="http://yoursite.com/2018/03/18/Radial-basis-function-kernel/"/>
    <id>http://yoursite.com/2018/03/18/Radial-basis-function-kernel/</id>
    <published>2018-03-18T10:54:27.000Z</published>
    <updated>2018-03-23T08:15:06.901Z</updated>
    
    <content type="html"><![CDATA[<h2 id="高斯径向基函数"><a href="#高斯径向基函数" class="headerlink" title="高斯径向基函数"></a>高斯径向基函数</h2><h4 id="简介："><a href="#简介：" class="headerlink" title="简介："></a>简介：</h4><p>在机器学习中，（高斯）径向基函数核（英语：Radial basis function kernel），或称为RBF核，是一种常用的核函数。它是支持向量机分类中最为常用的核函数。—— <a href="https://zh.wikipedia.org/wiki/%E5%BE%84%E5%90%91%E5%9F%BA%E5%87%BD%E6%95%B0%E6%A0%B8" target="_blank" rel="noopener">维基百科</a></p><h2 id="高斯径向基函数-1"><a href="#高斯径向基函数-1" class="headerlink" title="高斯径向基函数"></a>高斯径向基函数</h2><h4 id="高斯径向基函数公式如下："><a href="#高斯径向基函数公式如下：" class="headerlink" title="高斯径向基函数公式如下："></a>高斯径向基函数公式如下：</h4><p>$$K(x_1,x_2)=\exp{(-\frac{\parallel x_1-x_2 \parallel^2 }{2\sigma^2})}, \sigma&gt;0$$</p><h4 id="那么它有什么几何意义呢"><a href="#那么它有什么几何意义呢" class="headerlink" title="那么它有什么几何意义呢?"></a>那么它有什么几何意义呢?</h4><p>先看看x经过映射以后，在高维空间里这个点到原点的距离公式：<br> $$ \parallel x_i-0\parallel^2 = \parallel x_i \parallel^2=\left \langle  \Phi (x_i), \Phi (x_i)\right \rangle=K(x_i,x_i)=1$$<br>这表明样本x映射到高维空间后，在高维空间中的点$\Phi (x_i)$到高维空间中原点的距离为1，也即$\Phi (x_i)$存在于一个<strong>超球面</strong>上。</p><h4 id="为什么核函数能映射到高维空间呢？"><a href="#为什么核函数能映射到高维空间呢？" class="headerlink" title="为什么核函数能映射到高维空间呢？"></a>为什么核函数能映射到高维空间呢？</h4><p>先考虑普通的多项式核函数：$K(x,y)=(x^T\cdot y+m)^p$,其中$x,y\in \mathbb{R}^n$，多项式参数$p,m\in \mathbb{R}$<br>考虑$x=(x_1,x_2),y=(y_1,y_2),m=0,p=2$，即$K(x,y)=(x^T\cdot y)^2=x_1^2y_1^2+2x_1x_2y_1y_2+x_2^2y_2^2$<br>现在回到之前的映射$k(x,y)=\left \langle \Phi(x),\Phi(y)\right \rangle$，并取$\Phi(x)=(x_1^2,\sqrt{2}x_1x_2,x_2^2)$<br>则有$k(x,y)=\left \langle \Phi(x),\Phi(y)\right \rangle=x_1^2y_1^2+2x_1x_2y_1y_2+x_2^2y_2^2=(x^T\cdot y)^2=K(x,y)$<br>这就是前面的$K(x,y)$，因此，该核函数就将2维映射到了3维空间。</p><h4 id="径向基核又为什么能够映射到无限维空间呢？"><a href="#径向基核又为什么能够映射到无限维空间呢？" class="headerlink" title="径向基核又为什么能够映射到无限维空间呢？"></a>径向基核又为什么能够映射到无限维空间呢？</h4><p>看完了普通多项式核函数由2维向3维的映射，再来看看高斯径向基函数会把2维平面上一点映射到多少维。<br>$$\begin{eqnarray}<br>K(x,y) &amp; = &amp; \exp(| x_1-x_2 |^2 ) \<br>&amp; = &amp; \exp(-(x_1-y_1)^2-(x_2-y_2)^2) \<br>&amp; = &amp; \exp(-x_1^2+2x_1y_1-y_1^2-x_2^2+2x_2y_2-y_2^2) \<br>&amp; = &amp; \exp(-|x|^2)\exp(-|y|^2)\exp(2x^Ty)\<br>\end{eqnarray}$$<br>将最后一项泰勒展开你就会恍然大悟：<br>$$K(x,y)=\exp(-|x|^2)\exp(-|y|^2)\sum_{n=0}^{\infty}\frac{(2x^Ty)^n}{n!}$$</p><p>再具体一点：<br>高斯核是这样定义的：$K(x_1,x_2)=\exp{(-\frac{\parallel x_1-x_2 \parallel^2 }{2\sigma^2})}, \sigma&gt;0$<br>尽管我不会解释它（但相信我）高斯核可以简单修正为这个样子：$K(x_1,x_2)=\exp(-\frac{x_1\cdot x_2}{\sigma^2}),\sigma&gt;0$，这里$x_1\cdot x_2$可解释为内积。<br>再用泰勒展开就会得到$K(x_1,x_2)=\sum_{n=0}^{\infty}\frac{(x_1\cdot x_2)^n}{\sigma^nn!}$<br>求和号里面的元素是不是看起来很熟悉呢？<br>没错，这就是一个n次多项式核。因为每一个多项式核都将一个向量投影到更高维的空间中，因此高斯核是那些$degree\geq0$的<strong>多项式核</strong>的<strong>组合</strong>，所以我们说高斯核是投影到无穷维空间中。</p><ul><li>参考<br>Quora上的问题：<a href="https://www.quora.com/Machine-Learning/Why-does-the-RBF-radial-basis-function-kernel-map-into-infinite-dimensional-space-mentioned-many-times-in-machine-learning-lectures" target="_blank" rel="noopener">https://www.quora.com/Machine-Learning/Why-does-the-RBF-radial-basis-function-kernel-map-into-infinite-dimensional-space-mentioned-many-times-in-machine-learning-lectures</a></li></ul><h4 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h4><p>初学者使用markdown与LaTeX的语法真不适应啊，就写到这里吧，写篇博客太累了orz饭都没吃，关于RBF神经网络的话，以后用到再来讲吧，累死我惹。<br>如有疑问，欢迎咨询，联系方式见“关于”页面。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;高斯径向基函数&quot;&gt;&lt;a href=&quot;#高斯径向基函数&quot; class=&quot;headerlink&quot; title=&quot;高斯径向基函数&quot;&gt;&lt;/a&gt;高斯径向基函数&lt;/h2&gt;&lt;h4 id=&quot;简介：&quot;&gt;&lt;a href=&quot;#简介：&quot; class=&quot;headerlink&quot; title=
      
    
    </summary>
    
      <category term="math" scheme="http://yoursite.com/categories/math/"/>
    
      <category term="theory" scheme="http://yoursite.com/categories/math/theory/"/>
    
    
      <category term="machine_leanring" scheme="http://yoursite.com/tags/machine-leanring/"/>
    
      <category term="math" scheme="http://yoursite.com/tags/math/"/>
    
  </entry>
  
  <entry>
    <title>python 的闭包、装饰器</title>
    <link href="http://yoursite.com/2018/03/17/python-%E7%9A%84%E9%97%AD%E5%8C%85%E3%80%81%E8%A3%85%E9%A5%B0%E5%99%A8/"/>
    <id>http://yoursite.com/2018/03/17/python-的闭包、装饰器/</id>
    <published>2018-03-17T09:33:14.000Z</published>
    <updated>2018-03-17T10:10:55.183Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、闭包（Closure）"><a href="#一、闭包（Closure）" class="headerlink" title="一、闭包（Closure）"></a>一、闭包（Closure）</h1><h2 id="什么是闭包？"><a href="#什么是闭包？" class="headerlink" title="什么是闭包？"></a>什么是闭包？</h2><blockquote><p>在计算机科学中，闭包（英语：Closure），又称词法闭包（Lexical Closure）或函数闭包（function closures），是引用了自由变量的函数。这个被引用的自由变量将和这个函数一同存在，即使已经离开了创造它的环境也不例外。所以，有另一种说法认为闭包是由函数和与其相关的引用环境组合而成的实体。闭包在运行时可以有多个实例，不同的引用环境和相同的函数组合可以产生不同的实例。—— <a href="https://zh.wikipedia.org/wiki/闭包_(计算机科学" target="_blank" rel="noopener">维基百科</a>)</p></blockquote><p>这里我给个简单的解释：一个<strong>闭包</strong>就是你调用了一个<em>函数A</em>，这个<em>函数A</em>返回了一个<em>函数B</em>给你。这个<strong>返回的函数B</strong>就叫做闭包。你在<strong>调用函数A</strong>的时候<strong>传递的参数</strong>就是<strong>自由变量</strong>。</p><h2 id="举个例子："><a href="#举个例子：" class="headerlink" title="举个例子："></a>举个例子：</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">(name)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">inner_func</span><span class="params">(age)</span>:</span></span><br><span class="line">        <span class="keyword">print</span> <span class="string">'name:'</span>, name, <span class="string">'age:'</span>, age</span><br><span class="line">    <span class="keyword">return</span> inner_func</span><br><span class="line"></span><br><span class="line">bb = func(<span class="string">'matrix'</span>)</span><br><span class="line">bb(<span class="number">26</span>)  <span class="comment"># &gt;&gt;&gt; name: matrix age: 26</span></span><br></pre></td></tr></table></figure><p>这里面调用func的时候就产生了一个闭包– <em>inner_func</em>,并且该闭包持有自由变量– <em>name</em>，因此这也意味着，当函数func的生命周期结束之后，<strong>name这个变量依然存在</strong>，因为它被闭包引用了，所以不会被回收。</p><ul><li>也有人说这种内部函数inner_func可以使用外部函数的变量name的行为就叫闭包。</li></ul><h1 id="二、装饰器（Decorator）"><a href="#二、装饰器（Decorator）" class="headerlink" title="二、装饰器（Decorator）"></a>二、装饰器（Decorator）</h1><h2 id="什么是装饰器？"><a href="#什么是装饰器？" class="headerlink" title="什么是装饰器？"></a>什么是装饰器？</h2><blockquote><p>“装饰器的功能是将被装饰的函数当作参数传递给与装饰器对应的函数（名称相同的函数），并返回包装后的被装饰的函数”</p></blockquote><p>听起来有点绕，没关系，直接看示意图,其中a为 与<em>装饰器@a</em>对应的函数，<em>b</em>为装饰器修饰的函数，<em>装饰器@a</em>的作用是：<br><img src="/images/decorator.png" alt="image"><br><strong>简而言之：@a 就是将 b 传递给 a()，并返回新的 b = a(b)</strong></p><h2 id="举个例子"><a href="#举个例子" class="headerlink" title="举个例子"></a>举个例子</h2><ol><li><p>先导入包：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> reduce</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line">logging.basicConfig(level=logging.INFO)</span><br></pre></td></tr></table></figure></li><li><p>定义一个检查参数的装饰器：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">checkParams</span><span class="params">(fn)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapper</span><span class="params">(*numbers)</span>:</span></span><br><span class="line">        temp = map(<span class="keyword">lambda</span> x:isinstance(x,(int,)),numbers) <span class="comment"># 检查参数是否都为整型</span></span><br><span class="line">        <span class="keyword">if</span> reduce(<span class="keyword">lambda</span> x,y: x <span class="keyword">and</span> y, temp): <span class="comment"># 若都为整型</span></span><br><span class="line">            <span class="keyword">return</span> fn(*numbers)             <span class="comment"># 则调用fn(*numbers)返回计算结果</span></span><br><span class="line">        <span class="comment">#否则通过logging记录错误信息，并友好退出</span></span><br><span class="line">        logging.warning(<span class="string">"variable numbers cannot be added"</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="keyword">return</span> wrapper     <span class="comment">#fn引用gcd，被封存在闭包的执行环境中返回</span></span><br></pre></td></tr></table></figure></li><li><p>然后定义求最大公约数的函数（能求多个的最大公约数）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gcd</span><span class="params">(*numbers)</span>:</span></span><br><span class="line">    <span class="string">"""return the greatest common divisor of the given integers."""</span></span><br><span class="line">    <span class="keyword">return</span> reduce(math.gcd, numbers)</span><br></pre></td></tr></table></figure></li><li><p>调用</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;gcd = checkParams(gcd)</span><br><span class="line">&gt;&gt;&gt;gcd(3, <span class="string">'hello'</span>)</span><br><span class="line"><span class="comment"># 输出 WARNING:root: variable numbers cannot be added</span></span><br></pre></td></tr></table></figure></li></ol><p>注意checkParams函数：</p><blockquote><ul><li>首先看参数fn，当我们调用checkParams(gcd)的时候，它将成为函数对象gcd的一个本地(Local)引用；</li><li>在checkParams内部，我们定义了一个wrapper函数，添加了参数类型检查的功能，然后调用了fn(*numbers)，根据LEGB法则，解释器将搜索几个作用域，并最终在(Enclosing层) checkParams函数的本地作用域中找到fn；</li><li>注意最后的return wrapper，这将创建一个闭包，fn变量(gcd函数对象的一个引用)将会封存在闭包的执行环境中，不会随着checkParams的返回而被回收；</li></ul></blockquote><p>当调用gcd = checkParams(gcd)时，gcd指向了新的wrapper对象，它添加了参数检查和记录日志的功能，同时又能够通过封存的fn，继续调用原始的gcd进行最大公约数运算。</p><p>因此调用gcd(3, ‘hello’)将不会返回计算结果，而是打印出日志：root: variable numbers cannot be added。</p><p>有人觉得add = checkParams(add)这样的写法未免太过麻烦，于是python提供了一种更优雅的写法，被称为<strong>语法糖</strong>：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@checkParams</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lcm</span><span class="params">(*numbers)</span>:</span></span><br><span class="line">    <span class="string">"""return lowest common multiple."""</span></span><br><span class="line">    f = <span class="keyword">lambda</span> a,b:int((a*b)/gcd(a,b))</span><br><span class="line">    <span class="keyword">return</span> reduce(f, numbers)</span><br></pre></td></tr></table></figure></p><p>其实这只是一种写法上的优化，解释器仍然会将它转化为gcd = checkParams(gcd)来执行。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;一、闭包（Closure）&quot;&gt;&lt;a href=&quot;#一、闭包（Closure）&quot; class=&quot;headerlink&quot; title=&quot;一、闭包（Closure）&quot;&gt;&lt;/a&gt;一、闭包（Closure）&lt;/h1&gt;&lt;h2 id=&quot;什么是闭包？&quot;&gt;&lt;a href=&quot;#什么是
      
    
    </summary>
    
      <category term="math" scheme="http://yoursite.com/categories/math/"/>
    
      <category term="python" scheme="http://yoursite.com/categories/math/python/"/>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="fun math" scheme="http://yoursite.com/tags/fun-math/"/>
    
  </entry>
  
  <entry>
    <title>hexo常用命令笔记</title>
    <link href="http://yoursite.com/2018/03/13/hexo%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2018/03/13/hexo常用命令笔记/</id>
    <published>2018-03-13T14:59:57.000Z</published>
    <updated>2018-03-13T15:10:19.665Z</updated>
    
    <content type="html"><![CDATA[<h1 id="hexo"><a href="#hexo" class="headerlink" title="hexo"></a>hexo</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo -g <span class="comment">#安装</span></span><br><span class="line">npm update hexo -g <span class="comment">#升级</span></span><br><span class="line">hexo init <span class="comment">#初始化</span></span><br></pre></td></tr></table></figure><h1 id="简写"><a href="#简写" class="headerlink" title="简写"></a>简写</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hexo n <span class="string">"我的博客"</span> == hexo new <span class="string">"我的博客"</span> <span class="comment">#新建文章</span></span><br><span class="line">hexo p == hexo publish</span><br><span class="line">hexo g == hexo generate<span class="comment">#生成</span></span><br><span class="line">hexo s == hexo server <span class="comment">#启动服务预览</span></span><br><span class="line">hexo d == hexo deploy<span class="comment">#部署</span></span><br></pre></td></tr></table></figure><h1 id="服务器"><a href="#服务器" class="headerlink" title="服务器"></a>服务器</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hexo server <span class="comment">#Hexo 会监视文件变动并自动更新，您无须重启服务器。</span></span><br><span class="line">hexo server -s <span class="comment">#静态模式</span></span><br><span class="line">hexo server -p 5000 <span class="comment">#更改端口</span></span><br><span class="line">hexo server -i 192.168.1.1 <span class="comment">#自定义 IP</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo clean #清除缓存 网页正常情况下可以忽略此条命令</span><br><span class="line">hexo g #生成静态网页</span><br><span class="line">hexo d #开始部署</span><br></pre></td></tr></table></figure><h1 id="监视文件变动"><a href="#监视文件变动" class="headerlink" title="监视文件变动"></a>监视文件变动</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hexo generate #使用 Hexo 生成静态文件快速而且简单</span><br><span class="line">hexo generate --watch #监视文件变动</span><br></pre></td></tr></table></figure><h1 id="完成后部署"><a href="#完成后部署" class="headerlink" title="完成后部署"></a>完成后部署</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 以下两个命令的作用是相同的</span><br><span class="line">hexo generate --deploy</span><br><span class="line">hexo deploy --generate</span><br></pre></td></tr></table></figure><h2 id="简写-1"><a href="#简写-1" class="headerlink" title="简写"></a>简写</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hexo deploy -g</span><br><span class="line">hexo server -g</span><br></pre></td></tr></table></figure><h1 id="草稿"><a href="#草稿" class="headerlink" title="草稿"></a>草稿</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo publish [layout] &lt;title&gt;</span><br></pre></td></tr></table></figure><h1 id="模版"><a href="#模版" class="headerlink" title="模版"></a>模版</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hexo new &quot;postName&quot; #新建文章</span><br><span class="line">hexo new page &quot;pageName&quot; #新建页面</span><br><span class="line">hexo generate #生成静态页面至public目录</span><br><span class="line">hexo server #开启预览访问端口（默认端口4000，&apos;ctrl + c&apos;关闭server）</span><br><span class="line">hexo deploy #将.deploy目录部署到GitHub</span><br></pre></td></tr></table></figure><p>如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo new [layout] &lt;title&gt;</span><br><span class="line">hexo new photo &quot;My Gallery&quot;</span><br><span class="line">hexo new &quot;Hello World&quot; --lang tw</span><br></pre></td></tr></table></figure></p><table><thead><tr><th>变量</th><th style="text-align:center">描述</th></tr></thead><tbody><tr><td>layout</td><td style="text-align:center">布局</td></tr><tr><td>title</td><td style="text-align:center">标题</td></tr><tr><td>date</td><td style="text-align:center">文件建立日期</td></tr></tbody></table><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">title: 使用Hexo搭建个人博客</span><br><span class="line">layout: post</span><br><span class="line">date: 2014-03-03 19:07:43</span><br><span class="line">comments: true</span><br><span class="line">categories: Blog</span><br><span class="line">tags: [Hexo]</span><br><span class="line">keywords: Hexo, Blog</span><br><span class="line">description: 生命在于折腾，又把博客折腾到Hexo了。给Hexo点赞。</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;hexo&quot;&gt;&lt;a href=&quot;#hexo&quot; class=&quot;headerlink&quot; title=&quot;hexo&quot;&gt;&lt;/a&gt;hexo&lt;/h1&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre
      
    
    </summary>
    
      <category term="hexo" scheme="http://yoursite.com/categories/hexo/"/>
    
      <category term="Instructions" scheme="http://yoursite.com/categories/hexo/Instructions/"/>
    
    
      <category term="hexo" scheme="http://yoursite.com/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>my_knn</title>
    <link href="http://yoursite.com/2018/03/13/my-knn/"/>
    <id>http://yoursite.com/2018/03/13/my-knn/</id>
    <published>2018-03-13T13:58:30.000Z</published>
    <updated>2018-03-23T08:14:41.549Z</updated>
    
    <content type="html"><![CDATA[<h1 id="关于"><a href="#关于" class="headerlink" title="关于"></a>关于</h1><p>在一个月前，需要用<code>1nn</code>做二分类的测试的时候，开始因为用<code>sklearn</code>训练数据时用错了数据集，百思不得其解，于是自己写了个<code>knn</code>来训练，当时写好后，才真正把原理给弄懂了orz，原来是数据集训练时用错了。。。改正后对比了一下自己的<code>knn</code>和<code>sklearn</code>的<code>knn</code>的准确率都差不多（也就是说测试通过啦），就上传到了我的GitHub。</p><p>当时我虽然有个用腾讯云搭建的博客，但基本上都没在上面写过了orz，本博客当时还没有问世，正好基于GitHub的服务器最近搭了这个博客，空空的也不好，最近老师第一讲就讲knn，那就把之前的代码贴上吧。</p><h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><p>对于一个输入的测试数据，计算该样本点到训练数据各样本点的距离，然后对所有距离由小到大排列，取前k个数据；统计该k个数据中对应的标签出现次数最多的标签，则该测试样本就被标记为该标签。</p><h1 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h1><ul><li>输入: 训练数据集：$T={(X_1,y_1),(X_2,y_2),…,(X_N,y_N)}$, 其中$X_i={x_i^1,x_i^2,…,x_i^n}$,有n个特征，N个样本点;</li><li>输入：最近邻个数k，及要预测的样本点$X_0={x_0^1,,x_0^2,…,x_0^n}$;</li><li>计算：样本点X_0到训练数据集T中各样本点的距离（一般为欧氏距离）;</li><li>排序：将以上算出的距离由小到大排序，并选出前k个距离数据;</li><li>统计：统计前k个距离数据中各个标签对应的个数，选出个数最多的那个标签，即为该样本点预测的结果。</li></ul><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">my_knn</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""docstring for my_knn"""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, k)</span>:</span></span><br><span class="line">        super(my_knn, self).__init__()</span><br><span class="line">        self.k = k</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, X_train, y_train)</span>:</span></span><br><span class="line">        self.X_train, self.y_train = np.array(X_train), np.array(y_train)</span><br><span class="line">        <span class="keyword">if</span> len(self.X_train) != len(self.y_train):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"X_test,y_test or y_train was not equail!"</span></span><br><span class="line">                             <span class="string">"The length of X_test,y_test is %s"</span></span><br><span class="line">                             <span class="string">"But the length of y_train is %s"</span> % (len(self.X_train), len(self.y_train)))</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict_one</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        dist2xtrain = np.sum((X - self.X_train)**<span class="number">2</span>, axis=<span class="number">1</span>)**<span class="number">0.5</span></span><br><span class="line">        index = dist2xtrain.argsort() <span class="comment"># 从小到大（近到远）</span></span><br><span class="line">        label_count = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.k):</span><br><span class="line">            label = self.y_train[index[i]]</span><br><span class="line">            label_count[label] = label_count.get(label, <span class="number">0</span>) + <span class="number">1</span></span><br><span class="line">        <span class="comment"># 将label_count的值从大到小排列label_count的键</span></span><br><span class="line">        y_predict = sorted(label_count, key=<span class="keyword">lambda</span> x: label_count[x], reverse=<span class="keyword">True</span>)[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> y_predict</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict_all</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> np.array(list(map(self.predict_one, X)))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calc_accuracy</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">        predict = self.predict_all(X)</span><br><span class="line">        total = X.shape[<span class="number">0</span>]</span><br><span class="line">        right = sum(predict == y)</span><br><span class="line">        accuracy = right/total</span><br><span class="line">        <span class="keyword">return</span> accuracy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    data_set = load_iris()</span><br><span class="line">    datas = data_set[<span class="string">"data"</span>]</span><br><span class="line">    labels = data_set[<span class="string">'target'</span>]</span><br><span class="line">    X_train, X_test, y_train, y_test = train_test_split(datas, labels, test_size=<span class="number">0.4</span>, random_state=<span class="number">0</span>)</span><br><span class="line">    knn = my_knn(<span class="number">1</span>)</span><br><span class="line">    knn = knn.train(X_train,y_train)</span><br><span class="line">    accuracy = knn.calc_accuracy(X_test,y_test)</span><br><span class="line">    print(<span class="string">"%.3f%%"</span> % (accuracy * <span class="number">100</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line">    neigh = KNeighborsClassifier(n_neighbors=<span class="number">1</span>)</span><br><span class="line">    neigh.fit(X_train, y_train)</span><br><span class="line">    print(neigh.score(X_train,y_train))</span><br><span class="line">    print(neigh.score(X_test, y_test))</span><br></pre></td></tr></table></figure><h1 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h1><p>关于对knn的kd树加速这部分还需要日后的后续学习，这里就先不说啦（其实是我也不会23333）。<br>由于我对markdown语法不太熟悉，写起文章来的有点别扭还望理解（逃。</p><h1 id="写给自己"><a href="#写给自己" class="headerlink" title="写给自己"></a>写给自己</h1><p>  还是要多花点时间学习啊！一个多月没学习就忘得差不多了orz,还好看一下就能回想起来。多练习吧！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;关于&quot;&gt;&lt;a href=&quot;#关于&quot; class=&quot;headerlink&quot; title=&quot;关于&quot;&gt;&lt;/a&gt;关于&lt;/h1&gt;&lt;p&gt;在一个月前，需要用&lt;code&gt;1nn&lt;/code&gt;做二分类的测试的时候，开始因为用&lt;code&gt;sklearn&lt;/code&gt;训练数据时用错了数
      
    
    </summary>
    
      <category term="machine_leanring" scheme="http://yoursite.com/categories/machine-leanring/"/>
    
      <category term="code" scheme="http://yoursite.com/categories/machine-leanring/code/"/>
    
    
      <category term="machine_leanring" scheme="http://yoursite.com/tags/machine-leanring/"/>
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>my_bayes</title>
    <link href="http://yoursite.com/2018/03/13/my-bayes/"/>
    <id>http://yoursite.com/2018/03/13/my-bayes/</id>
    <published>2018-03-13T13:09:38.000Z</published>
    <updated>2018-03-23T08:14:51.626Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本周一晚上老师讲到了<code>naive bayes（朴素贝叶斯分类器）</code>，于是自己用python来实现了一下。现在这个脚本对于比较大的数据可能会计算的比较慢，还需要以后慢慢再研究一下里面的加速。</p><p>本程序主要利用了pandas里dataframe的groupby分组函数，大大的方便了对数据的统计。对于条件概率，有不同的标签，不同的特征和特征里的不同数据，我们采用了<code>dict</code>数据结构，第一层key为标签，value是一个新的dict；第二层（前面那个新的dict）的key为特征，value是一个Series或者字典；第三层的key/index为特征的取值，value为频数/概率。、、（虽然看起来比较拗口，但我感觉这样能够比较清晰的分清了各个条件概率了，如果你有更好的方法，欢迎留言给我，谢谢。）</p><h1 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h1><blockquote><ul><li>输入：训练数据集及其标签集，要预测的数据集</li><li>统计各标签出现的频数，并拉普拉斯平滑，计算先验概率</li><li>统计在各标签下各个特征的频数，并拉普拉斯平滑，计算条件概率</li><li>查找要预测数据集各特征在不同标签下的条件概率和先验概率相乘得到（半）后验概率</li><li>对半后验概率进行从大到小排序，选出最大值对应的标签，即为预测结果</li><li>实例化测试<br>ps：这里半后验概率为我自己的定义：$P(Y_j) *\prod_{i=1}^N P(A_i|Y_j) ; i:1\to n_{feature}; j:1\to n_{label}$</li></ul></blockquote><h1 id="解释"><a href="#解释" class="headerlink" title="解释"></a>解释</h1><p>本程序主要分为一下部分：</p><blockquote><ul><li>定义一个bayes分类器（类）</li><li>计算先验概率</li><li>计算所有条件概率</li><li>进行调用训练</li><li>对测试数据进行预测</li><li>实例化测试</li></ul></blockquote><p>以上各对应之下的各个函数：（废话不多说，直接上代码）</p><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">my_naive_bayes</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, df)</span>:</span></span><br><span class="line">        super(my_naive_bayes, self).__init__()</span><br><span class="line">        self.df = df</span><br><span class="line">        self.X_train = df.iloc[:,:<span class="number">-1</span>]</span><br><span class="line">        self.y_train = df.iloc[:,<span class="number">-1</span>]</span><br><span class="line">        self.label_set = set(self.y_train)</span><br><span class="line">        self.features = df.columns[:<span class="number">-1</span>]</span><br><span class="line">        self.label_name = df.columns[<span class="number">-1</span>]</span><br><span class="line">        self.feature_dict = &#123;&#125;</span><br><span class="line">        self.n_sample = len(df)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_prior_p</span><span class="params">(self, g)</span>:</span></span><br><span class="line">        n = len(g)</span><br><span class="line">        prior_p = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> label <span class="keyword">in</span> self.label_set:</span><br><span class="line">            prior_p[label] = g.size()[label] / self.n_sample</span><br><span class="line">        <span class="keyword">return</span> prior_p</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_cond_p</span><span class="params">(self, g)</span>:</span></span><br><span class="line">        cond_p = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> label, group <span class="keyword">in</span> g:</span><br><span class="line">            cond_p[label] = &#123;&#125;</span><br><span class="line">            <span class="keyword">for</span> feature <span class="keyword">in</span> self.features:</span><br><span class="line">                counts = group[feature].value_counts()</span><br><span class="line">                cond_p[label][feature] = counts / sum(counts)</span><br><span class="line">        <span class="keyword">return</span> cond_p</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, )</span>:</span></span><br><span class="line">        <span class="keyword">for</span> feature <span class="keyword">in</span> self.features:</span><br><span class="line">            self.feature_dict[feature] = set(self.df[feature])</span><br><span class="line">        g = self.df.groupby(self.label_name)</span><br><span class="line"></span><br><span class="line">        self.prior_p = self.get_prior_p(g)</span><br><span class="line">        self.cond_p = self.get_cond_p(g)</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict_one</span><span class="params">(self, test_X)</span>:</span></span><br><span class="line">        semi_post_p = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> label <span class="keyword">in</span> self.label_set:</span><br><span class="line">            temp = <span class="number">1</span></span><br><span class="line">            <span class="keyword">for</span> feature <span class="keyword">in</span> self.features:</span><br><span class="line">                temp = temp * self.cond_p[label][feature][test_X[feature]]</span><br><span class="line">            semi_post_p[label] = self.prior_p[label] * temp</span><br><span class="line">        <span class="keyword">return</span> max(semi_post_p, key=semi_post_p.get)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">df = pd.read_excel(<span class="string">"bayes_data.xlsx"</span>,index_col=<span class="string">"index"</span>)</span><br><span class="line"><span class="comment"># n = len(df)</span></span><br><span class="line"><span class="comment"># train_n = int(n*0.6)</span></span><br><span class="line"><span class="comment"># train_df = df[:train_n]</span></span><br><span class="line"><span class="comment"># test_df = df[train_n:]</span></span><br><span class="line">bayes = my_naive_bayes(df)</span><br><span class="line">bayes = bayes.train()</span><br><span class="line">test_x = df.loc[<span class="number">6</span>]</span><br><span class="line">label = bayes.predict_one(test_x)</span><br><span class="line">print(label)</span><br></pre></td></tr></table></figure><h1 id="最后，好好学习，天天向上！"><a href="#最后，好好学习，天天向上！" class="headerlink" title="最后，好好学习，天天向上！"></a>最后，好好学习，天天向上！</h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;本周一晚上老师讲到了&lt;code&gt;naive bayes（朴素贝叶斯分类器）&lt;/code&gt;，于是自己用python来实现了一下。现在这个脚本对
      
    
    </summary>
    
      <category term="machine_leanring" scheme="http://yoursite.com/categories/machine-leanring/"/>
    
      <category term="code" scheme="http://yoursite.com/categories/machine-leanring/code/"/>
    
    
      <category term="machine_leanring" scheme="http://yoursite.com/tags/machine-leanring/"/>
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>18_03_07</title>
    <link href="http://yoursite.com/2018/03/07/18-03-07/"/>
    <id>http://yoursite.com/2018/03/07/18-03-07/</id>
    <published>2018-03-07T08:27:50.000Z</published>
    <updated>2018-03-07T08:43:35.364Z</updated>
    
    <content type="html"><![CDATA[<p>今天是开学第三天，各种事情莫名烦躁。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;今天是开学第三天，各种事情莫名烦躁。&lt;/p&gt;

      
    
    </summary>
    
      <category term="diary" scheme="http://yoursite.com/categories/diary/"/>
    
    
      <category term="diary" scheme="http://yoursite.com/tags/diary/"/>
    
      <category term="life-style" scheme="http://yoursite.com/tags/life-style/"/>
    
  </entry>
  
  <entry>
    <title>SVM explained-well</title>
    <link href="http://yoursite.com/2018/03/02/SVM-explained-well/"/>
    <id>http://yoursite.com/2018/03/02/SVM-explained-well/</id>
    <published>2018-03-02T09:40:40.000Z</published>
    <updated>2018-03-02T09:52:39.291Z</updated>
    
    <content type="html"><![CDATA[<p>Support vector machines (SVM)</p><p>User <a href="http://www.reddit.com/user/copperking" target="_blank" rel="noopener">copperking</a> stepped up to the plate:</p><p>“We have 2 colors of balls on the table that we want to separate.</p><p><img src="/images/SVM/svm1-300x225.png" alt="image"></p><p>We get a stick and put it on the table, this works pretty well right?</p><p><img src="/images/SVM/svm2-300x225.png" alt="image"></p><p>Some villain comes and places more balls on the table, it kind of works but one of the balls is on the wrong side and there is probably a better place to put the stick now.</p><p><img src="/images/SVM/svm3-300x225.png" alt="image"></p><p>SVMs try to put the stick in the best possible place by having as big a gap on either side of the stick as possible.</p><p><img src="/images/SVM/svm4-300x225.png" alt="image"></p><p>Now when the villain returns the stick is still in a pretty good spot.</p><p><img src="/images/SVM/svm5-300x225.png" alt="image"></p><p>There is another trick in the SVM toolbox that is even more important. Say the villain has seen how good you are with a stick so he gives you a new challenge.</p><p><img src="/images/SVM/svm6-300x225.png" alt="image">svm7-300x167.png</p><p>There’s no stick in the world that will let you split those balls well, so what do you do? You flip the table of course! Throwing the balls into the air. Then, with your pro ninja skills, you grab a sheet of paper and slip it between the balls.</p><p><img src="/images/SVM/svm7-300x167.png" alt="image"></p><p>Now, looking at the balls from where the villain is standing, they balls will look split by some curvy line.</p><p><img src="/images/SVM/svm8-300x225.png" alt="image"></p><p>Boring adults the call balls data, the stick a classifier, the biggest gap trick optimization, call flipping the table kernelling and the piece of paper a hyperplane.”</p><p>I think the last step is the most beautiful no mater in mathematic or machine learning! Hope it will help you.</p><p>source: <a href="http://bytesizebio.net/2014/02/05/support-vector-machines-explained-well/" target="_blank" rel="noopener">http://bytesizebio.net/2014/02/05/support-vector-machines-explained-well/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Support vector machines (SVM)&lt;/p&gt;
&lt;p&gt;User &lt;a href=&quot;http://www.reddit.com/user/copperking&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;copperking&lt;/a&gt; s
      
    
    </summary>
    
      <category term="theory" scheme="http://yoursite.com/categories/theory/"/>
    
    
      <category term="machine_leanring" scheme="http://yoursite.com/tags/machine-leanring/"/>
    
      <category term="fun math" scheme="http://yoursite.com/tags/fun-math/"/>
    
  </entry>
  
  <entry>
    <title>new-post</title>
    <link href="http://yoursite.com/2018/02/27/new-post/"/>
    <id>http://yoursite.com/2018/02/27/new-post/</id>
    <published>2018-02-27T06:29:07.000Z</published>
    <updated>2018-03-13T14:58:29.904Z</updated>
    
    <content type="html"><![CDATA[<h1 id="this-is-a-test-blog"><a href="#this-is-a-test-blog" class="headerlink" title="this is a test blog."></a>this is a test blog.</h1><p>welcome to treamy’s world.</p><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><h3 id="add-some-code"><a href="#add-some-code" class="headerlink" title="add some code"></a>add some code</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(<span class="string">"My New Post"</span>)</span><br></pre></td></tr></table></figure><h3 id="标签页面"><a href="#标签页面" class="headerlink" title="标签页面"></a>标签页面</h3><p>1&gt;运行以下命令<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new page <span class="string">"tags"</span></span><br></pre></td></tr></table></figure></p><p>同时，在/source目录下会生成一个tags文件夹，里面包含一个index.md文件</p><h3 id="推送到服务器上"><a href="#推送到服务器上" class="headerlink" title="推送到服务器上"></a>推送到服务器上</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo g -d</span><br></pre></td></tr></table></figure><p>先generate一下生成静态页面，再deploy部署到服务器。<br>好像<code>hexo d -g</code>也可以。。一次记错了写成这个也行。。我也不知道为啥在一个网页上看到说他们左右是相同的，还是用前面那个较好解读的吧。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;this-is-a-test-blog&quot;&gt;&lt;a href=&quot;#this-is-a-test-blog&quot; class=&quot;headerlink&quot; title=&quot;this is a test blog.&quot;&gt;&lt;/a&gt;this is a test blog.&lt;/h1&gt;&lt;p&gt;
      
    
    </summary>
    
    
      <category term="test" scheme="http://yoursite.com/tags/test/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://yoursite.com/2018/02/27/hello-world/"/>
    <id>http://yoursite.com/2018/02/27/hello-world/</id>
    <published>2018-02-27T06:23:46.596Z</published>
    <updated>2018-02-27T06:23:46.596Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
  </entry>
  
</feed>
