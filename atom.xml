<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Treamy&#39;s website</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-05-13T15:25:53.463Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Treamy</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>从形式文法到数学的极限(1)</title>
    <link href="http://yoursite.com/2018/05/12/%E4%BB%8E%E5%BD%A2%E5%BC%8F%E6%96%87%E6%B3%95%E5%88%B0%E6%95%B0%E5%AD%A6%E7%9A%84%E6%9E%81%E9%99%90-1/"/>
    <id>http://yoursite.com/2018/05/12/从形式文法到数学的极限-1/</id>
    <published>2018-05-12T14:38:27.000Z</published>
    <updated>2018-05-13T15:25:53.463Z</updated>
    
    <content type="html"><![CDATA[<h1 id="从形式文法到数学的极限-1-乔姆斯基和他的形式文法"><a href="#从形式文法到数学的极限-1-乔姆斯基和他的形式文法" class="headerlink" title="从形式文法到数学的极限 (1) - 乔姆斯基和他的形式文法"></a>从形式文法到数学的极限 (1) - 乔姆斯基和他的形式文法</h1><p><em>诺姆·乔姆斯基（Noam Chomsky）</em>，美国语言学家，为科学作出的最大贡献便是他提出的形式文法。这个概念一经提出，直接促成了不知道多少重要技术，其中收益最多的领域可能就是信息行业了。</p><h2 id="语法分析器"><a href="#语法分析器" class="headerlink" title="语法分析器"></a>语法分析器</h2><p>在计算机中，要想使写好的代码运行起来，必须要有一个叫做“编译”的中间过程，将代码转换成可直接由机器执行的文件（这里只涉及编译型语言）。为了让计算机在编译时读懂你写的代码，一个适当的语法分析器是必不可少的。</p><p>每种语言都有自己不同的语法，语法分析器按照各种语言定义的语法，逐字符地将源代码解析成一棵语法树。<br>而用来描述这种语法的工具，就是乔姆斯基的形式文法。</p><h2 id="巴克斯-瑙尔范式"><a href="#巴克斯-瑙尔范式" class="headerlink" title="巴克斯-瑙尔范式"></a>巴克斯-瑙尔范式</h2><p>要定义一套语法很简单，下面是一组（简陋的）示例：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&lt;sentence&gt; := [ &lt;subject&gt; ] &lt;predicate&gt; &#123; &lt;object&gt; &#125;</span><br><span class="line">&lt;subject&gt; := &lt;nominal-phrase&gt;</span><br><span class="line">&lt;object&gt; := &lt;nominal-phrase&gt;</span><br><span class="line">&lt;nominal-phrase&gt; := &lt;quantifier&gt; &lt;noun&gt;</span><br><span class="line">&lt;quantifier&gt; := &lt;article&gt; | &lt;number&gt;</span><br><span class="line">&lt;article&gt; := &quot;a&quot; | &quot;an&quot; | &quot;the&quot;</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p><p>如你所见，所有的字面量都很直观、浅显，只是有几个符号需要解释一下：</p><pre><code>每条语法的最左项是它的名字，用尖括号括起来，跟随一个冒号和一个等号，表示定义；在语法定义里引用其他语法，也用尖括号括起要引用的名字即可；若要表示出现与否均可的选项，用方括号括起，比如 &lt;sentence&gt; 里的 &lt;subject&gt;；若要表示出现任意次数（包括 0 次），用大括号括起，比如 &lt;sentence&gt; 里的 &lt;object&gt;；若要表示多个模式里任选，用竖杠分隔开；若要表示欲匹配的字符串，用双引号括起。</code></pre><p>这种书写记号是荷兰学者<em>瑙尔（Naur）</em>最先提出的，后经美国学者<em>巴克斯（Bakus）</em>完善，形成了现在的<strong>巴克斯-瑙尔范式</strong>。乔姆斯基在他自己的形式文法里，也采用了类似<strong>B-N 范式</strong>的书写方式。</p><p>他把所有出现在尖括号里的那些名称称作“非终结符”，意为可以继续向下分解成更小的单位；将出现在引号里的字符串称作“终结符”，意为<strong>最小单位</strong>，不可继续分解。在语言学中，通常用大写字母表示非终结符，而用小写字母表示终结符。</p><h2 id="语法层级"><a href="#语法层级" class="headerlink" title="语法层级"></a>语法层级</h2><p>乔姆斯基给他的文法根据不同的规则划分了不同的等级，以表示它们所描述语言的<code>“自由”</code>程度。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">0 型语言：任意语法所描述的语言都是 0 型的</span><br><span class="line">上下文有关（1 型）语言：对于所有语法都有 aAb := cBd 形式的语言，称之为上下文有关语言</span><br><span class="line">上下文无关（2 型）语言：对于所有语法都有 A := aBb 形式的语言，称之为上下文无关语言</span><br><span class="line">正则（3 型）语言：对于所有语法都有 A := aB 或 A := Ba 形式的语言，称之为正则语言</span><br></pre></td></tr></table></figure></p><p>可以看出，从上到下对语法的限制愈发增多，语言的多样性也随之减少。对于 1 和 2 型语言的命名中的“有关”、“无关”，是指非终结符的解析是否受其邻接的终结符影响。</p><p>大部分自然语言都是上下文有关语言。比如英语中<code>&quot;I broke a window&quot;</code>和<code>&quot;The window broke&quot;</code>中的<code>&quot;broke&quot;</code>便是受到宾语有无的影响而分别成为了及物和不及物动词。</p><p>上下文无关语言主要被用于设计程序，这是因为解析 2 型语言时不需考虑语法结构所处的上下文，其语法解析器相对容易实现。</p><p>可以看出，正则语言的表达能力极为受限，因为其解析过程只能在每一步中增添一前缀或后缀。对于总是增加前缀的正则语言，我们称为左线性语言，反之称为右线性语言。事实上，对这种高度受限的语言，人们已经设计出来一些强大的用来解析它们的工具了，比如正则表达式。</p><h2 id="语法推导"><a href="#语法推导" class="headerlink" title="语法推导"></a>语法推导</h2><p>对于任意一组语法和一个指定的非终结符，总是可以根据其中的规则将这个非终结符扩展成一个终结符串（可能是无限的）。<br>比如对于这条（正则）文法：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;A&gt; := &quot;a&quot; &lt;A&gt; | &quot;&quot;</span><br></pre></td></tr></table></figure></p><p>不断地将 <code>&lt;A&gt;</code>替换为<code>&quot;a&quot;</code>，便可以推出下列字符串：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a, aa, aaa, aaaa, ...</span><br></pre></td></tr></table></figure></p><p>如果一个字符串（包括终结符和非终结符）能由某组语法推导过来，就说这个字符串规约到这组语法。</p><h2 id="数学推理"><a href="#数学推理" class="headerlink" title="数学推理"></a>数学推理</h2><p>在数学上，我们把最开始给定的那组语法叫做“公理”，把由所有终结符可能构成的字符串称作“命题”。所有的公理都是公认为真的命题，比如欧式几何里的“过两点必有一条直线”。</p><p>对于那些可以规约到公理的命题，我们说它们是也是真的，并称它们为“定理”。而公理到定理的推导过程，自然就叫“证明”。</p><p>所有的公理都可以在自己前面加上一个终结符“非”，来变成自己的反命题。对于所有能规约到公理的反命题的命题，我们说它们是假的。这样，所有命题就有了自己的真假，或者更专业一点地，叫它们“真值”。</p><p>对于任意的数学命题，只需要对它们进行规约，看看是否能回到最初的公理即可知道它们正确与否了。</p><p>这似乎看上去太美好了，你怎么能保证所有命题都一定能规约到公理或它们的反命题上去呢？有没有可能存在一个命题，它根本就不可能通过有限次规约，得到最初的公理呢？</p><p>还真的有一个神人挖到了这么一个大坑，直接否定了数学的完备性（即所有命题均能证明或证伪）。至于这人是谁，以及这坑怎么挖的，请看下回分解。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;从形式文法到数学的极限-1-乔姆斯基和他的形式文法&quot;&gt;&lt;a href=&quot;#从形式文法到数学的极限-1-乔姆斯基和他的形式文法&quot; class=&quot;headerlink&quot; title=&quot;从形式文法到数学的极限 (1) - 乔姆斯基和他的形式文法&quot;&gt;&lt;/a&gt;从形式文法到数
      
    
    </summary>
    
      <category term="math" scheme="http://yoursite.com/categories/math/"/>
    
    
      <category term="math" scheme="http://yoursite.com/tags/math/"/>
    
  </entry>
  
  <entry>
    <title>logistic regression</title>
    <link href="http://yoursite.com/2018/04/23/logistic-regression/"/>
    <id>http://yoursite.com/2018/04/23/logistic-regression/</id>
    <published>2018-04-23T14:11:30.000Z</published>
    <updated>2018-04-25T10:09:24.533Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-逻辑斯蒂分布-logistic-distribution"><a href="#1-逻辑斯蒂分布-logistic-distribution" class="headerlink" title="1 逻辑斯蒂分布(logistic distribution)"></a>1 逻辑斯蒂分布(logistic distribution)</h1><p><strong>定义</strong>：设X是连续随机变量，X服从<code>logistic分布</code>是指X具有下列分布函数和密度函数：</p><p>$$ F = P(X \leq x)=\frac{1}{1+exp(-\frac{(x-\mu)}{\gamma})} $$<br>$$ f= F’(x) = \frac{exp(-\frac{(x-\mu)}{\gamma})}{\gamma[1+exp(-\frac{(x-\mu)}{\gamma})]^2} $$<br>式中，$\mu$为位置参数，$\gamma &gt; 0$为形状参数。</p><p>其分布图形如下：</p><p><img src="/images/lg_d.jpg" alt="image"></p><p>F曲线在中心附近增长速度较快，在两端增长速度较慢。形状参数$\gamma$值越小，曲线在中心附近增长得越快。</p><h1 id="2-二项逻辑斯蒂回归模型"><a href="#2-二项逻辑斯蒂回归模型" class="headerlink" title="2 二项逻辑斯蒂回归模型"></a>2 二项逻辑斯蒂回归模型</h1><p><strong>定义</strong>：二项逻辑斯蒂回归模型是如下的条件概率分布<br>$$P(y=1\mid x) = \frac{\exp(\omega^\top x+b)}{1 + \exp(\omega^\top x+b)} \tag{1}$$<br>$$P(y=0\mid x) = \frac{1}{1 + \exp(\omega^\top x+b)}  \tag{2}$$<br>这里，$x\in R^n$是输入，$y\in {0,1 }$是输出，$\omega \in R^n$和$b\in R$是参数，$\omega$称为权值向量，$b$称为偏置，$\omega^\top x$为$w$和$x$的内积。</p><blockquote><p>对于给定的输入实例$x$，按照(1)式和(2)式可以分别求得$P(y=1\mid x)$和$P(y=0\mid x)$。逻辑斯蒂回归比较这两个条件概率值的大小，将实例$x$分到概率值较大的那一类。</p></blockquote><p><strong>定义</strong>：一个事件的<code>几率(odds)</code>是指该事件发生的概率与该事件不发生的概率的比值。如果事件发生的概率为$p$，那么该事件的几率是$\frac{p}{1-p}$，该事件的<code>对数几率</code>(log odds)或者logit函数是$$logit(p)=log\frac{p}{1-p}$$<br>对于逻辑斯蒂回归而言，由(1),(2)式得$$log\frac{P(y=1\mid x)}{1-P(y=1\mid x)}=\omega^\top x+b$$<br>也就是说，在逻辑斯蒂回归模型中，输出$y=1$的对数几率是由输入$x$的线性函数表示的模型。</p><h1 id="3-模型参数估计"><a href="#3-模型参数估计" class="headerlink" title="3 模型参数估计"></a>3 模型参数估计</h1><p>对于给定的训练数据集$T={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}$，其中$x_i\in R^n$，$y_i\in {0,1 }$，可以应用<code>极大似然估计法</code>估计模型参数，从而得到逻辑斯蒂回归模型。</p><p>设：$P(y=1\mid x)=\pi(x)$，$P(y=0\mid x)=1-\pi(x)$则似然函数为：<br>$$\prod_{i=1}^N [\pi(x_i)]^{y_i}[1-\pi(x_i)]^{1-y_i}$$<br>对数似然函数为：<br>$$<br>\begin{aligned}<br>L(\omega,b)&amp;=\sum_{i=1}^N [y_i\ln(\pi(x_i))+(1-y_i)\ln(1-\pi(x_i))] \<br>&amp;= \sum_{i=1}^N [y_i\ln\frac{\pi(x_i)}{1-\pi(x_i)}+\ln(1-\pi(x_i))]  \<br>&amp;=\sum_{i=1}^N \left [ y_i(\omega^\top x+b) -\ln \left ( 1+ exp(\omega^\top x+b) \right )\right ]<br>\end{aligned}$$<br>这样，问题就变成了以对数似然函数为目标函数的最优化问题，逻辑斯蒂回归学习中通常采用的方法是<strong>梯度下降法</strong>及<strong>牛顿法</strong>。</p><h1 id="4-梯度下降-上升-法求解"><a href="#4-梯度下降-上升-法求解" class="headerlink" title="4 梯度下降(上升)法求解"></a>4 梯度下降(上升)法求解</h1><p>利用梯度下降(上升)求解对数似然函数：$L(\omega,b)$<br>因为要使得似然函数最大，我们使用<strong>梯度上升法</strong>。<br>为了计算方便，我们将权值向量和输入向量加以<strong>扩充</strong>，仍记作$\omega,x$，即$$\omega=(\omega^{(1)},\omega^{(2)},…,\omega^{(n)},b),\;x=(x^{(1)},x^{(2)},…,x^{(n)},1)$$</p><h4 id="梯度上升求解"><a href="#梯度上升求解" class="headerlink" title="梯度上升求解:"></a>梯度上升求解:</h4><p>这时$$\omega_{new}^\top x_{new}=\omega_{old}^\top x_{old}+b_{old}$$<br>我们令：<br>$$z=\omega^\top x; z_i=\omega^\top x_i;z_i^{(k)}=\omega_k^\top x_i$$<br>$$\pi(z) =  \frac{exp(z)}{1+exp(z)}= \frac{1}{1+exp(-z)} $$</p><p>于是有$$l(\omega)=\sum_{i=1}^N \left [ y_i(\omega^\top x) -\ln \left ( 1+ exp(\omega^\top x) \right )\right ]$$<br>先求各个偏导数：<br>$$\begin{aligned}<br>\frac{\partial l(\omega)}{\partial \omega^{(j)}}&amp;=\frac{\partial }{\partial \omega^{(j)}}\left (<br> \sum_{i=1}^N \left [ y_i(\omega^\top x) -\ln \left ( 1+ exp(\omega^\top x) \right )\right ]\right ) \<br> &amp;= \sum_{i=1}^N \left [ y_i x_i^{(j)} - \frac{exp(w^\top x_i)}{1+exp(w^\top x_i)} x_i^{(j)}\right ]  \<br> &amp;= \sum_{i=1}^N  \left (  y_i -  \frac{exp(w^\top x_i)}{1+exp(w^\top x_i)} \right ) x_i^{(j)}  \<br> &amp;= \sum_{i=1}^N ( y_i -  \pi(z_i)  ) x_i^{(j)}<br>\end{aligned}$$</p><p>得到参数的迭代公式：<br>$$\omega_{k+1}^{(j)} = \omega_{k}^{(j)} +\lambda_k \cdot (-\sum_{i=1}^N ( y_i -  \pi(z_i^{(k)}) ) ) x_i^{(j)} $$<br>令$$s^{(k)}=(s_1^{(k)},s_2^{(k)},…,s_N^{(k)}),s_i^{(k)}= y_i -  \pi_k(z_i^{(k)}) $$<br>则<br>$$\begin{aligned}<br>\triangledown l(\omega_{k}) &amp;= ( \frac{\partial l(\omega_{k})}{\partial \omega_{k}^{(0)}}, \frac{\partial l(\omega_{k})}{\partial \omega_{k}^{(1)}},…, \frac{\partial l(\omega_{k})}{\partial \omega_{k}^{(n)}} ) \<br> &amp;= [\sum_{i=1}^N ( y_i -  \pi(z_i^{(k)})  ) x_i^{(j)}],j=0,1,…,n \<br>&amp;=[\sum_{i=1}^N s_i^{(k)} x_i^{(j)}] \<br>&amp;=s^{(k)}\cdot x\<br>\end{aligned}$$</p><p><strong>注意梯度上升为正梯度方向</strong>,即 $ P^{(k)} =  \triangledown l(\omega_{k})$<br>即有：</p><blockquote><p>$$\omega_{k+1} = \omega_{k} +\lambda_k P^{(k)} = \omega_{k} +\lambda_k \cdot (s^{(k)}\cdot x) $$</p></blockquote><h4 id="求解一维搜索"><a href="#求解一维搜索" class="headerlink" title="求解一维搜索"></a>求解一维搜索</h4><p>$$l(\omega_{k}+\lambda_k P^{(k)})=\max_{\lambda \geqslant 0}l(\omega_{k}+\lambda \cdot P^{(k)})$$</p><p><strong>得</strong></p><blockquote><p>$$\lambda_k=\frac{ - \triangledown l(\omega_{k})^\top \triangledown l(\omega_{k}) }{\triangledown l(\omega_{k})^\top H(\omega_{k}) \triangledown l(\omega_{k})} $$</p></blockquote><p>其中</p><p>$$H(\omega_{k})=\begin{bmatrix}<br>\frac{\partial^2 l(\omega_{k})}{\partial \omega_{k}^{(p)}\partial \omega_{k}^{(q)}}<br>\end{bmatrix} ;p,q \in {0,1,2,..,n}$$</p><p>$$\frac{\partial^2 l(\omega_{k})}{\partial \omega_{k}^{(p)}\partial \omega_{k}^{(q)}} = \sum_{i=1}^N \pi’(\omega_k x_i)  (  x_i^{(p)} x_i^{(q)})  $$</p><p>$$\pi’(z) =  \frac{exp(-z)}{(1+exp(-z))^2}=\pi(z)(1-\pi(z))$$</p><h1 id="5-模型的优缺点"><a href="#5-模型的优缺点" class="headerlink" title="5 模型的优缺点"></a>5 模型的优缺点</h1><p>缺点：</p><ul><li>逻辑回归需要大样本量，因为最大似然估计在低样本量的情况下不如最小二乘法有效。</li><li>为防止过拟合和欠拟合，应该让模型构建的变量是显著的。</li><li>对模型中自变量多重共线性较为敏感，需要对自变量进行相关性分析，剔除线性相关的变量。</li></ul><p>优点：<br>模型更简单，好理解，实现起来，特别是大规模线性分类时比较方便</p><h1 id="6-模型实现"><a href="#6-模型实现" class="headerlink" title="6 模型实现"></a>6 模型实现</h1><p>见我GitHub。</p><p><img src="/images/LR.png" alt="分类图"></p><h1 id="7-最后"><a href="#7-最后" class="headerlink" title="7 最后"></a>7 最后</h1><p>在写的过程中才发现，没写一次都要花挺长的时间去理解以及使用markdown码上数学公式，但是这都很大的促进了我对原理的理解！</p><h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><blockquote><p>《统计学习方法》李航 著  清华大学出版社<br>《机器学习实战》Peter Harrington 著 人民邮电出版社<br>《运筹学》第四版 清华大学出版社</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-逻辑斯蒂分布-logistic-distribution&quot;&gt;&lt;a href=&quot;#1-逻辑斯蒂分布-logistic-distribution&quot; class=&quot;headerlink&quot; title=&quot;1 逻辑斯蒂分布(logistic distribution)&quot;
      
    
    </summary>
    
      <category term="machine_leanring" scheme="http://yoursite.com/categories/machine-leanring/"/>
    
      <category term="code" scheme="http://yoursite.com/categories/machine-leanring/code/"/>
    
    
      <category term="machine_leanring" scheme="http://yoursite.com/tags/machine-leanring/"/>
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>my kmeans</title>
    <link href="http://yoursite.com/2018/04/15/my-kmeans/"/>
    <id>http://yoursite.com/2018/04/15/my-kmeans/</id>
    <published>2018-04-15T12:58:05.000Z</published>
    <updated>2018-04-15T12:58:05.168Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="machine_leanring" scheme="http://yoursite.com/categories/machine-leanring/"/>
    
      <category term="code" scheme="http://yoursite.com/categories/machine-leanring/code/"/>
    
    
      <category term="machine_leanring" scheme="http://yoursite.com/tags/machine-leanring/"/>
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>梯度下降法</title>
    <link href="http://yoursite.com/2018/04/02/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/"/>
    <id>http://yoursite.com/2018/04/02/梯度下降法/</id>
    <published>2018-04-02T06:12:51.000Z</published>
    <updated>2018-04-25T11:03:16.842Z</updated>
    
    <content type="html"><![CDATA[<h2 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h2><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>&amp;#8195&amp;#8195梯度下降法是求解<strong>无约束最优化</strong>问题的一种最常用的<strong>迭代法</strong>。顾名思义，梯度下降法的计算过程就是沿梯度下降的方向求解极小值（也可以沿梯度上升方向求解极大值）。</p><p>&amp;#8195&amp;#8195假设$f(x)$是$R^n$上具有一阶连续偏导数的函数，要求解的无约束最优化问题是$$\min_{x\in R^n} f(x)$$<br>$x^*$表示目标函数的极小值点。</p><blockquote><p>选取适当的初值$x^{(0)}$，不断迭代，更新$x$的值，进行目标函数的极小化，直到收敛。由于负梯度方向是使函数值下降最快的方向，在迭代的每一步，以负梯度方向更新$x$的值，从而达到减少函数值的目的。</p></blockquote><p>&amp;#8195&amp;#8195由于$f(x)$具有一阶连续偏导数，若第k次迭代值为$x^{(k)}$，则可将$f(x)$在$x^{(k)}$附近进行一阶泰勒展开：$$f(x)=f(x^{(k)})+g_k^ \top \cdot(x-x^{(k)})$$<br>其中$g_k =g(x^{(k)})= -\triangledown f(x^{(k)}) $为$f(x)$在$x^{(k)}$的梯度(梯度上升为正号)。<br>&amp;#8195&amp;#8195求出第k+1次迭代值$x^{(k+1)}$:<br>$$x^{(k+1)}\leftarrow x^{(k)}+\lambda_k P^{(k)} $$<br><img src="/images/Gradient_descent.png" alt="Gradient_descent"></p><p>其中，$p^{(k)}$是搜索方向，取负梯度方向$P^{(k)}=-\triangledown f(x^{(k)}) $，$\lambda_k$是步长，由一维搜索确定，即$\lambda_k$使得$$f(x^{(k)}+\lambda_k P^{(k)})=\min_{\lambda \geqslant 0}f(x^{(k)}+\lambda \cdot p^{(k)})$$</p><blockquote><p>一维搜索有个十分重要的性质：在搜索方向上所得最优点处目标函数的梯度和该搜索方向正交。<br><strong>定理:</strong> 设目标函数$f(x)$具有一阶连续偏导数，$x^{(k+1)}$由如下规则产生<br>$$<br>    \left{<br>      \begin{array}{c}<br>        \lambda_k: \min_\lambda f(x^{(k)}+\lambda P^{(k)}) \<br>        x^{(k+1)} = x^{(k)}+\lambda_k P^{(k)}<br>      \end{array}<br>    \right.<br>$$<br>&#160;则有$$-\triangledown f(x^{(k+1)}) P^{(k)}=0  \tag{1}$$<br><strong>证明:</strong> 构造函数$\varphi(\lambda)=f(x^{(k)}+\lambda P^{(k)})$，则得<br>$$<br>    \left{<br>      \begin{array}{c}<br>        \varphi(\lambda_k)= \min_\lambda \varphi(\lambda) \<br>        x^{(k+1)} = x^{(k)}+\lambda_k P^{(k)}<br>      \end{array}<br>    \right.<br>$$<br>即$\lambda_k$为$\varphi(\lambda)$的极小值点。此外$\varphi’(\lambda)=\triangledown f(x^{(k)}+\lambda P^{(k)})^\top P^{(k)}$。<br>由$\varphi’(\lambda)|_{\lambda=\lambda_k}=0$可得$$\triangledown f(x^{(k)}+\lambda_k P^{(k)})^\top P^{(k)}=\triangledown f(x^{(k+1)})^\top P^{(k)}=0$$定理得证。</p></blockquote><h4 id="为什么-P-k-triangledown-f-x-k"><a href="#为什么-P-k-triangledown-f-x-k" class="headerlink" title="为什么$P^{(k)}=-\triangledown f(x^{(k)})$?"></a>为什么$P^{(k)}=-\triangledown f(x^{(k)})$?</h4><p>&amp;#8195&amp;#8195因为对于充分小的$\lambda$，只要$$f(x^{(k)})^\top P^{(k)}&lt;0  \tag{2}$$就可以保证$$f(x^{(k)}+\lambda_k P^{(k)})&lt;f(x^{(k)}) \tag{3}$$<br>&amp;#8195&amp;#8195现在考察不同的$P^{(k)}$。假定$P^{(k)}$的模一定(且不为零)，并设$\triangledown f(x^{(k)})$(否则，$x^{(k)}$是平稳点)，使得(2)式成立的$P^{(k)}$有无限多个，为了使目标函数数值能得到尽量大的改善，必须寻求使$f(x^{(k)})^\top P^{(k)} $取最小值的$P^{(k)}$，因为有<br>$$f(x^{(k)})^\top P^{(k)}=|f(x^{(k)})| \cdot |P^{(k)}| \cdot \cos\theta$$<br>式中$\theta$为$f(x^{(k)})$和$P^{(k)}$的夹角。当$P^{(k)}$与$f(x^{(k)})$反向时，$\theta=180°,\cos\theta=-1$。这时式(2)成立，且其左端取得最小值。</p><h4 id="一维搜索"><a href="#一维搜索" class="headerlink" title="一维搜索"></a>一维搜索</h4><p>&amp;#8195&amp;#8195为了得到下一个近似极小值点，在选定了搜索方向之后，还要确定步长$\lambda$。当采用可接受点算法时，就是取某一$\lambda$进行试算，看是否满足不等式(3)，若上述不等式成立，就可以迭代下去。否则缩小$\lambda$使得其满足不等式(3)。<br>&amp;#8195&amp;#8195另一种方法就是在负梯度方向的一维搜索，来确定使$f(x^{(k)})$最小的$\lambda_k$。最常用的有<code>试探法(斐波拉契，0.618法)</code>，<code>插值法(抛物线插值，三次插值)</code>，<code>微积分中的求根法(切线法、二分法等)</code>等。</p><p>&amp;#8195&amp;#8195若$f(x)$具有二阶连续偏导数，在$x^{(k)}$作$f(x^{(k)}-\lambda \triangledown f(x^{(k)}) )$的泰勒展开：<br>$$f(x^{(k)}-\lambda \triangledown f(x^{(k)})) \approx f(x^{(k)}) -\triangledown f(x^{(k)})^\top \lambda \triangledown f(x^{(k)})  + \frac{1}{2}\lambda \triangledown f(x^{(k)})^\top H(x^{(k)}) \lambda \triangledown f(x^{(k)})$$<br>对$\lambda$求导并且令其等于零，则得<strong>近似最佳步长</strong><br>$$\lambda_k=\frac{ \triangledown f(x^{(k)})^\top \triangledown f(x^{(k)}) }{\triangledown f(x^{(k)})^\top H(x^{(k)}) \triangledown f(x^{(k)})}  \tag{4}$$<br>其中$$H(x^{(k)})=\begin{bmatrix}<br>\frac{\partial^2 f(x^{(k)}))}{\partial x_1^2} &amp; \frac{\partial^2 f(x^{(k)}))}{\partial x_1 \partial x_2} &amp; … &amp;\frac{\partial^2 f(x^{(k)}))}{\partial x_1 \partial x_n} \<br>\frac{\partial^2 f(x^{(k)}))}{\partial x_2 \partial x_1} &amp; \frac{\partial^2 f(x^{(k)}))}{\partial x_2^2} &amp; …&amp;\frac{\partial^2 f(x^{(k)}))}{\partial x_2 \partial x_n} \<br>…&amp;&amp;&amp; \<br>\frac{\partial^2 f(x^{(k)}))}{\partial x_n \partial x_1} &amp; \frac{\partial^2 f(x^{(k)}))}{\partial x_n \partial x_2} &amp; …&amp;\frac{\partial^2 f(x^{(k)}))}{\partial x_n^2} \<br>\end{bmatrix}$$<br>为$f(x)$在点$x^{(k)}$处的<strong>海赛(Hesse)矩阵</strong>。<br>可见近似最佳步长不只与梯度有关，还与海赛矩阵H也有关系，计算起来比较麻烦。<br>&amp;#8195&amp;#8195有时，将搜索方向$P^{(k)}$的模长规格化为1，在这种情况下<br>$$P^{(k)}=\frac{-\triangledown f(x^{(k)})}{ |\triangledown f(x^{(k)})|}$$<br>同时，式(4)变为$$\lambda_k=\frac{ \triangledown f(x^{(k)})^\top \triangledown f(x^{(k)}) |\triangledown f(x^{(k)})| }{\triangledown f(x^{(k)})^\top H(x^{(k)}) \triangledown f(x^{(k)})}  $$</p><h3 id="固定步长"><a href="#固定步长" class="headerlink" title="固定步长"></a>固定步长</h3><p>有一点需要注意的是步长a固定时的大小，如果a太小，则会迭代很多次才找到最优解，若a太大，可能跳过最优，从而找不到最优解。<br><img src="/images/lambda.png" alt="步长变化"></p><h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><h4 id="梯度下降法算法如下："><a href="#梯度下降法算法如下：" class="headerlink" title="梯度下降法算法如下："></a>梯度下降法算法如下：</h4><ul><li>输入：目标函数$f(x)$，梯度函数$g(x)=-\triangledown f(x)$，计算精度$\varepsilon  $;</li><li>输出：$f(x)$的极小值点$x^<em>$。<br>(1). 取初始值$x^{(0)}\in R^n$，置k=0;<br>(2). 计算$f(x^{(k)})$;<br>(3). 计算梯度$g_k =g(x^{(k)})$，当$\left |  g_k\right | &lt; \varepsilon$时，停止迭代，令$x^</em>=x^{(k)}$；否则令$P^{(k)}=-g(x^{(k)})$，求$\lambda_k$，使得$$f(x^{(k)}+\lambda_k p_k)=\min_{\lambda \geqslant 0}f(x^{(k)}+\lambda \cdot P^{(k)})$$<br>(4). 置$x^{(k+1)}=x^{(k)}+\lambda_k P^{(k)} $，计算$f(x^{(k+1)})$，当$\left |  f(x^{(k+1)})-f(x^{(k)}) \right | &lt; \varepsilon$或$\left |  x^{(k+1)} - x^{(k)}\right | &lt; \varepsilon$时，停止迭代，令$x^*=x^{(k)}$；否则置$k=k+1$，转(3)。</li></ul><h3 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h3><p>名字中已经体现了核心思想，即<strong>随机选取一个点</strong>做梯度下降，而不是遍历所有样本后进行参数迭代。</p><p>因为梯度下降法的代价函数计算需要遍历所有样本，而且是每次迭代都要遍历，直至达到局部最优解，在样本量庞大时就显得收敛速度比较慢了，计算量非常庞大。</p><p>随机梯度下降仅以当前样本点进行最小值求解，通常无法达到真正局部最优解，但可以比较接近。属于大样本兼顾计算成本的折中方案。</p><p><img src="/images/RGD.jpg" alt="两者比较"></p><h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>&amp;#8195&amp;#8195当目标函数是凸函数时，梯度下降法的解是全局最优解。一般情况下，其解不保证是全局最优解。梯度下降法的收敛速度也未必是很快的。</p><h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><blockquote><p>《统计学习方法》李航 著  清华大学出版社<br>《运筹学》第四版 清华大学出版社</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;梯度下降法&quot;&gt;&lt;a href=&quot;#梯度下降法&quot; class=&quot;headerlink&quot; title=&quot;梯度下降法&quot;&gt;&lt;/a&gt;梯度下降法&lt;/h2&gt;&lt;h3 id=&quot;原理&quot;&gt;&lt;a href=&quot;#原理&quot; class=&quot;headerlink&quot; title=&quot;原理&quot;&gt;&lt;/a&gt;原
      
    
    </summary>
    
      <category term="math" scheme="http://yoursite.com/categories/math/"/>
    
      <category term="algorithm" scheme="http://yoursite.com/categories/math/algorithm/"/>
    
    
      <category term="math" scheme="http://yoursite.com/tags/math/"/>
    
      <category term="algorithm" scheme="http://yoursite.com/tags/algorithm/"/>
    
  </entry>
  
  <entry>
    <title>my decision tree id3</title>
    <link href="http://yoursite.com/2018/03/27/my-decision-tree-id3/"/>
    <id>http://yoursite.com/2018/03/27/my-decision-tree-id3/</id>
    <published>2018-03-27T02:41:18.000Z</published>
    <updated>2018-04-25T11:20:45.726Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>  本文主要讲述一下决策树的基本算法–ID3生成决策树算法。一开始看例子的时候，我觉得决策树好简单呀，应该实现起来用<code>pandas</code>也能像实现朴素贝叶斯一样容易实现，可是到实践的时候才发现，这个实现起来也好难啊orz。刚开始尝试直接通过算gini指数用<code>CART</code>算法生成树，但发现当两个的gini指数相同时我的程序就没法择优选择了。。。这个还有待改进。。最后参考了一下机器学习实战这本书把id3生成和可视化决策树实现了（不得不说这本书可视化部分写得我都看不懂了。。。）。</p><h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点和有向边组成。结点有两种类型：内部节点和叶节点，内部节点表示一个特征或属性，叶节点表示一个类。 </p><h3 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h3><p>分类决策树的核心思想就是在一个数据集中找到一个最优特征，然后从这个特征的选值中找一个最优候选值(这段话稍后解释)，根据这个最优候选值将数据集分为两个子数据集，然后递归上述操作，直到满足指定条件为止。</p><h3 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h3><h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><blockquote><p>1：理解和解释起来简单，且决策树模型可以想象<br>2：需要准备的数据量不大，而其他的技术往往需要很大的数据集，需要创建虚拟变量，去除不完整的数据，但是该算法对于丢失的数据不能进行准确的预测<br>3：决策树算法的时间复杂度(即预测数据)是用于训练决策树的数据点的对数<br>4：能够处理数字和数据的类别（需要做相应的转变），而其他算法分析的数据集往往是只有一种类型的变量<br>5：能够处理多输出的问题<br>6：使用白盒模型，如果给定的情况是在一个模型中观察到的，该条件的解释很容易解释的布尔逻辑，相比之下，在一个黑盒子模型（例如人工神经网络），结果可能更难以解释<br>7：可能使用统计检验来验证模型，这是为了验证模型的可靠性<br>8：从数据结果来看，它执行的效果很好，虽然它的假设有点违反真实模型</p></blockquote><h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><blockquote><p>1：决策树算法学习者可以创建复杂的树，但是没有推广依据，这就是所谓的过拟合，为了避免这种问题，出现了剪枝的概念，即设置一个叶子结点所需要的最小数目或者设置树的最大深度<br>2：决策树的结果可能是不稳定的，因为在数据中一个很小的变化可能导致生成一个完全不同的树，这个问题可以通过使用集成决策树来解决<br>3：众所周知，学习一恶搞最优决策树的问题是NP——得到几方面完全的优越性，甚至是一些简单的概念。因此，实际决策树学习算法是基于启发式算法，如贪婪算法，寻求在每个节点上的局部最优决策。这样的算法不能保证返回全局最优决策树。这可以减轻训练多棵树的合奏学习者，在那里的功能和样本随机抽样更换。<br>4：这里有一些概念是很难的理解的，因为决策树本身并不难很轻易的表达它们，比如说异或校验或复用的问题。<br>5：决策树学习者很可能在某些类占主导地位时创建有有偏异的树，因此建议用平衡的数据训练决策树<br>–当然最重要的还是容易<strong>过拟合</strong>！，所以迫切需要剪纸或者集成学习。</p></blockquote><h3 id="举个栗子"><a href="#举个栗子" class="headerlink" title="举个栗子"></a>举个栗子</h3><p>各位立志于脱单的单身男女在找对象的时候就已经完完全全使用了决策树的思想。假设一位母亲在给女儿介绍对象时，有这么一段对话：</p><blockquote><p>母亲：给你介绍个对象。<br>女儿：年纪多大了？<br>母亲：26。<br>女儿：长的帅不帅？<br>母亲：挺帅的。<br>女儿：收入高不？<br>母亲：不算很高，中等情况。<br>女儿：是公务员不？<br>母亲：是，在税务局上班呢。<br>女儿：那好，我去见见。</p></blockquote><p>这个女生的决策过程就是典型的分类决策树。相当于对年龄、外貌、收入和是否公务员等特征将男人分为两个类别：见或者不见。假设这个女生的决策逻辑如下：<br><img src="/images/tree1.png" alt="image"><br>上图完整表达了这个女孩决定是否见一个约会对象的策略，其中绿色结点（内部结点）表示判断条件，橙色结点（叶结点）表示决策结果，箭头表示在一个判断条件在不同情况下的决策路径，图中红色箭头表示了上面例子中女孩的决策过程。</p><p>这幅图基本可以算是一棵决策树，说它“基本可以算”是因为图中的判定条件没有量化，如收入高中低等等，还不能算是严格意义上的决策树，如果将所有条件量化，则就变成真正的决策树了。（以上的决策树模型纯属瞎编乱造，旨在直观理解决策树，不代表任何女生的择偶观，各位女同志无须在此挑刺。。。）</p><h3 id="决策树的学习"><a href="#决策树的学习" class="headerlink" title="决策树的学习"></a>决策树的学习</h3><p>决策树学习算法包含特征选择、决策树的生成与剪枝过程。决策树的学习算法通常是递归地选择最优特征，并用最优特征对数据集进行分割。开始时，构建根结点，选择最优特征，该特征有几种值就分割为几个子集，每个子集分别递归调用此方法，返回结点，返回的结点就是上一层的子结点。直到所有特征都已经用完，或者数据集只有一维特征为止。（这里就不介绍关于决策树的剪枝过程，日后再介绍）</p><h4 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h4><p>特征选择问题希望选取对训练数据具有良好分类能力的特征，这样可以提高决策树学习的效率。如果利用一个特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征是没有分类能力的（对象是否喜欢打游戏应该不会成为关键特征吧，也许也会……）。为了解决特征选择问题，找出最优特征，先要介绍一些信息论里面的概念。 </p><ol><li><p>熵（entropy）<br>熵是表示随机变量不确定性的度量。设$X$是一个取有限个值的离散随机变量，其概率分布为$$P(X=x_i)=p_i, i=1,2,…,n$$<br>则随机变量的熵定义为$$entropy(X) = -\sum_{i=1}^n P_ilog_2 P_i$$<br>另外，$0log0=0$，当对数的底为2时，熵的单位为bit；为e时，单位为nat。<br><strong>熵越大</strong>，随机变量的<strong>不确定性就越大</strong>。<br>从定义可验证$0&lt;=H(p)&lt;=logn$.<br>python实现计算如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">calc_entropy = <span class="keyword">lambda</span> P_: sum(map(<span class="keyword">lambda</span> p: -p * np.log2(p), P_))</span><br><span class="line"><span class="comment"># 其中P_为X的概率分布，p为X取某个随机变量的概率</span></span><br></pre></td></tr></table></figure></li><li><p>条件熵（conditional entropy）<br>设有随机变量$(X,Y)$，其联合概率分布为$$P(X=x_i,Y=y_i)=p_{ij}, i=1,2,…,n;j=1,2,…,m$$条件熵$H(Y|X)$表示在<strong>已知随机变量X的条件下</strong>随机变量Y的不确定性。随机变量X给定的条件下随机变量Y的条件熵$H(Y|X)$，定义为X给定条件下Y的条件概率分布的熵对X的数学期望$$entropy(Y|X) = \sum_{i=1}^k p_i H(Y|X=x_i)$$这里$p_i=P(X=x_i), i=1,2,…,n$。<br>用python实现求条件熵时，只需要把P_更换为feat_value_cate_P再调用calc_entropy，然后把所有分组的概率与对应的熵相乘再相加：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># group为Y对于这个特征X取不同的值的分组</span></span><br><span class="line"><span class="keyword">for</span> feat_value, group <span class="keyword">in</span> groups:</span><br><span class="line">    feat_value_P = len(group) / len(df)  <span class="comment"># 特征X取某值的概率</span></span><br><span class="line">    feat_value_cate_P = group[cate].value_counts() / group[cate].count()  <span class="comment"># 特征X取某值对应不同的类别的概率</span></span><br><span class="line">    feat_value_entropy += feat_value_P * calc_entropy(feat_value_cate_P)</span><br></pre></td></tr></table></figure></li><li><p>信息增益（information gain）<br>信息增益表示<strong>得知特征X的信息而使得类Y的信息的不确定性减少的程度</strong>。特征A对训练数据集D的信息增益$g(D,A)$，定义为集合D的<code>经验熵H(D)</code>与特征A给定条件下D的<code>经验条件熵H(D|A)</code>之<strong>差</strong>，即$$g(D,A)=H(D)−H(D|A)$$<br>这个差又称为互信息，表示由于特征A而使得对数据集D的分类不确定性减少的程度。信息增益大的特征具有更强的分类能力。<br>设训练数据为$D$，$|D|$表示其样本容量，即样本个数。设$D$有$K$个类$C_k$，$k=1,2,…,K$，$|C_k|$为属于类$C_k$的样本个数，$\sum_{k=1}^K |C_k|=|D|$. 设特征A有n个不同的取值${a_1,a_2,…a_n}$, 根据特征A 的取值将D划分为n个子集$D_1,D_2,…,D_n$, $|D_i|$为的样本$D_i$个数，$\sum_{i=1}^n |D_i|=|D|$。记子集$D_i$中属于类$C_k$的样本的集合为$D_{ik}$，即$D_{ik}=D_i\bigcap C_k$，$|D_{ik}|$为$D_{ik}$的样本个数。</p><blockquote><p>计算信息增益的算法如下： </p></blockquote></li></ol><ul><li>输入：训练数据集$D$和特征$A$；</li><li>输出：特征A对训练数据集$D$的信息增益$g(D,A)$.</li><li>计算数据集D的经验熵H(D)<br>$$H(D)=-\sum_{i=1}^k \frac{|C_k|}{|D|} log_2 \frac {|C_k|}{|D|}$$</li><li>计算特征A对数据集D的经验条件熵$H(D|A)$<br>$$H(D|A)=\sum_{i=1}^n \frac{|D_i|}{|D|}H(D_i)=\sum_{i=1}^n  \frac{|D_i|}{|D|}  \sum_{i=1}^K \frac{|D_{ik}|}{|D|} log_2 \frac{|D_{ik}|}{|D|}$$</li><li>计算信息增益$$g(D,A)=H(D)−H(D|A)$$</li></ul><h4 id="决策树的生成"><a href="#决策树的生成" class="headerlink" title="决策树的生成"></a>决策树的生成</h4><p>本次我们只介绍ID3算法，ID3算法由Ross Quinlan发明，建立在“奥卡姆剃刀”的基础上：越是小型的决策树越优于大的决策树（be simple简单理论）。ID3算法中根据信息增益评估和选择特征，每次选择信息增益最大的特征作为判断模块建立子结点。ID3算法可用于划分标称型数据集，没有剪枝的过程，为了去除过度数据匹配的问题，可通过裁剪合并相邻的无法产生大量信息增益的叶子节点（例如设置信息增益阀值）。使用信息增益的话其实是有一个缺点，那就是它偏向于具有大量值的属性。就是说在训练集中，某个属性所取的不同值的个数越多，那么越有可能拿它来作为分裂属性，而这样做有时候是没有意义的，另外ID3不能处理连续分布的数据特征，于是就有了C4.5算法。CART算法也支持连续分布的数据特征。<br><img src="/images/treeid3.jpg" alt="image"><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select</span><span class="params">(df, features, cate , H_D)</span>:</span></span><br><span class="line">    gain_dict = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> feat <span class="keyword">in</span> features:</span><br><span class="line">        groups = df.groupby(feat)</span><br><span class="line">        feat_value_entropy = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> feat_value, group <span class="keyword">in</span> groups:</span><br><span class="line">            feat_value_P = len(group) / len(df)  <span class="comment"># 特征取某值的概率</span></span><br><span class="line">            feat_value_cate_P = group[cate].value_counts() / group[cate].count()  <span class="comment"># 特征取某值对应不同的类别的概率</span></span><br><span class="line">            feat_value_entropy += feat_value_P * calc_entropy(feat_value_cate_P)</span><br><span class="line">        imfor_gain = H_D - feat_value_entropy</span><br><span class="line">        gain_dict[feat] = imfor_gain</span><br><span class="line">    <span class="keyword">return</span> max(gain_dict, key=<span class="keyword">lambda</span> x:gain_dict[x])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_tree</span><span class="params">(df, cate, H_D )</span>:</span></span><br><span class="line">    feat = df.columns[:<span class="number">-1</span>].tolist()</span><br><span class="line">    cate_values = df[cate].unique()</span><br><span class="line">    <span class="keyword">if</span> len(cate_values)==<span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> cate_values[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">if</span> len(feat) == <span class="number">0</span>: <span class="comment"># 用完所有特征后</span></span><br><span class="line">        temp = df[cate].value_counts().to_dict()</span><br><span class="line">        <span class="keyword">return</span> max(temp, key=<span class="keyword">lambda</span> x:temp[x]) <span class="comment"># 取最多的类别作为返回值</span></span><br><span class="line">    best_feat = select(df, feat, cate, H_D)</span><br><span class="line">    my_tree = &#123; best_feat:&#123;&#125; &#125;</span><br><span class="line">    unique_feat_values = df[best_feat].unique()</span><br><span class="line">    <span class="keyword">for</span> feat_value <span class="keyword">in</span> unique_feat_values:</span><br><span class="line">        df_ = df[df[best_feat] == feat_value].copy()</span><br><span class="line">        df_ =  df_.drop(best_feat, axis=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 这里一定不能是df，必须是一个新的df_，才能使递归*中的feat*越来越小</span></span><br><span class="line">        my_tree[best_feat][feat_value] = create_tree(df_, cate, H_D )</span><br><span class="line">    <span class="keyword">return</span> my_tree</span><br></pre></td></tr></table></figure></p><p>我们这里用Python语言的字典套字典类型存储树的信息，简单方便。当然也可以定义一个新的数据结构存储树。<br>来生成一个树：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">df = pd.read_excel(<span class="string">"TreeData.xlsx"</span>, index_col=<span class="string">"id"</span>)</span><br><span class="line">cate = df.columns[<span class="number">-1</span>]</span><br><span class="line">P_cate = df[cate].value_counts() / df[cate].count()</span><br><span class="line">H_D = calc_entropy(P_cate)</span><br><span class="line">tree = create_tree(df,cate, H_D)</span><br><span class="line">print(tree)</span><br><span class="line"><span class="comment"># &#123;'有自己的房子': &#123;'否': &#123;'有工作': &#123;'否': '否', '是': '是'&#125;&#125;, '是': '是'&#125;&#125;</span></span><br></pre></td></tr></table></figure></p><h4 id="决策树的可视化"><a href="#决策树的可视化" class="headerlink" title="决策树的可视化"></a>决策树的可视化</h4><p>我们主要用python的matplotlib来处理图像，它的annotate很方便用于注释。（以下代码来源：机器学习实战，我对其简单的更改了一下）<br>先获得叶子节点个数和树的深度：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_leafs_num</span><span class="params">(tree_)</span>:</span></span><br><span class="line">    num_leafs = <span class="number">0</span></span><br><span class="line">    first_key = list(tree_.keys())[<span class="number">0</span>]</span><br><span class="line">    second_dict = tree_[first_key]</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> second_dict.keys():</span><br><span class="line">        <span class="keyword">if</span> type(second_dict[key]).__name__ == <span class="string">"dict"</span>:</span><br><span class="line">            num_leafs += get_leafs_num(second_dict[key])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            num_leafs += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> num_leafs</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_tree_depth</span><span class="params">(tree_)</span>:</span></span><br><span class="line">    max_depth = <span class="number">0</span></span><br><span class="line">    first_key = list(tree_.keys())[<span class="number">0</span>]</span><br><span class="line">    second_dict = tree_[first_key]</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> second_dict.keys():</span><br><span class="line">        <span class="keyword">if</span> type(second_dict[key]).__name__ == <span class="string">"dict"</span>: <span class="comment"># 如果还是字典，继续深入</span></span><br><span class="line">            this_depth = <span class="number">1</span> + get_tree_depth(second_dict[key])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            this_depth = <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> this_depth &gt; max_depth:</span><br><span class="line">            max_depth = this_depth</span><br><span class="line">    <span class="keyword">return</span> max_depth</span><br></pre></td></tr></table></figure></p><p>然后再画图：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.rcParams[<span class="string">'font.sans-serif'</span>]=[<span class="string">'SimHei'</span>] </span><br><span class="line">plt.rcParams[<span class="string">'axes.unicode_minus'</span>]=<span class="keyword">False</span> </span><br><span class="line"></span><br><span class="line">decision_node=&#123;<span class="string">"boxstyle"</span>: <span class="string">"sawtooth"</span>, <span class="string">"fc"</span>: <span class="string">"0.8"</span>, &#125;</span><br><span class="line">leaf_node=&#123;<span class="string">"boxstyle"</span>: <span class="string">"round4"</span>, <span class="string">"fc"</span>: <span class="string">"0.8"</span>&#125;</span><br><span class="line">arrow_args=&#123;<span class="string">"arrowstyle"</span>: <span class="string">"&lt;-"</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_node</span><span class="params">(node_txt, centerPt, parentPt, node_type)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> ax1</span><br><span class="line">    ax1.annotate(node_txt, xy=parentPt, xycoords=<span class="string">'axes fraction'</span>,xytext=centerPt,</span><br><span class="line">        textcoords=<span class="string">'axes fraction'</span>,va=<span class="string">"center"</span>, ha=<span class="string">"center"</span>, bbox=node_type, arrowprops=arrow_args)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_mid_text</span><span class="params">(cntrPt, parentPt, txt_string)</span>:</span>  <span class="comment"># 在两个节点之间的线上写上字</span></span><br><span class="line">    <span class="keyword">global</span> ax1</span><br><span class="line">    xMid = (parentPt[<span class="number">0</span>] - cntrPt[<span class="number">0</span>]) / <span class="number">2.0</span> + cntrPt[<span class="number">0</span>]</span><br><span class="line">    yMid = (parentPt[<span class="number">1</span>] - cntrPt[<span class="number">1</span>]) / <span class="number">2.0</span> + cntrPt[<span class="number">1</span>]</span><br><span class="line">    ax1.text(xMid, yMid, txt_string)  <span class="comment"># text() 的使用</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_tree</span><span class="params">( tree_, parent_point, node_txt)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> ax1,xOff,yOff,totalD,totalW</span><br><span class="line">    num_leafs = get_leafs_num(tree_)</span><br><span class="line">    depth = get_tree_depth(tree_)</span><br><span class="line">    first_key = list(tree_.keys())[<span class="number">0</span>]</span><br><span class="line">    center_point = (xOff + (<span class="number">1.0</span> + float(num_leafs)) / <span class="number">2.0</span> / totalW, yOff)</span><br><span class="line">    plot_mid_text( center_point, parent_point, node_txt)  <span class="comment"># 在父子节点间填充文本信息</span></span><br><span class="line">    plot_node(first_key, center_point, parent_point, decision_node)  <span class="comment"># 绘制带箭头的注解</span></span><br><span class="line">    second_dict = tree_[first_key]</span><br><span class="line">    yOff = yOff - <span class="number">1.0</span> / totalD</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> second_dict.keys():</span><br><span class="line">        <span class="keyword">if</span> type(second_dict[key]).__name__ == <span class="string">'dict'</span>:  <span class="comment"># 判断是不是字典，</span></span><br><span class="line">            plot_tree(second_dict[key], center_point, str(key))  <span class="comment"># 递归绘制树形图</span></span><br><span class="line">        <span class="keyword">else</span>:  <span class="comment"># 如果是叶节点</span></span><br><span class="line">            xOff = xOff + <span class="number">1.0</span> / totalW</span><br><span class="line">            plot_node(second_dict[key], (xOff, yOff), center_point, leaf_node)</span><br><span class="line">            plot_mid_text((xOff, yOff), center_point, str(key))</span><br><span class="line">    yOff = yOff + <span class="number">1.0</span> / totalD</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_tree</span><span class="params">(tree_)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> ax1,xOff,yOff,totalD,totalW</span><br><span class="line">    fig = plt.figure(<span class="number">1</span>, facecolor=<span class="string">'white'</span>)</span><br><span class="line">    fig.clf()  <span class="comment"># 清空绘图区</span></span><br><span class="line">    axprops = dict(xticks=[], yticks=[])</span><br><span class="line">    ax1 = plt.subplot(<span class="number">111</span>, frameon=<span class="keyword">False</span>, **axprops)</span><br><span class="line">    totalW = float(get_leafs_num(tree_))</span><br><span class="line">    totalD = float(get_tree_depth(tree_))</span><br><span class="line">    xOff = <span class="number">-0.5</span> / totalW  <span class="comment"># 追踪已经绘制的节点位置 初始值为 将总宽度平分 在取第一个的一半</span></span><br><span class="line">    yOff = <span class="number">1.0</span></span><br><span class="line">    plot_tree(tree_, (<span class="number">0.5</span>, <span class="number">1.0</span>), <span class="string">''</span>)  <span class="comment"># 调用函数，并指出根节点源坐标</span></span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure></p><p>下面用一个实例来可视化一下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tree = &#123;<span class="string">'有自己的房子'</span>: &#123;<span class="string">'否'</span>: &#123;<span class="string">'有工作'</span>: &#123;<span class="string">'否'</span>: <span class="string">'否'</span>, <span class="string">'是'</span>: <span class="string">'是'</span>&#125;&#125;, <span class="string">'是'</span>: <span class="string">'是'</span>&#125;&#125;</span><br><span class="line"></span><br><span class="line">print(tree)</span><br><span class="line">show_tree(tree)</span><br></pre></td></tr></table></figure></p><p>可视化结果如下：<br><img src="/images/tree2.png" alt="image"></p><blockquote><ul><li>由于篇幅过长，完整代码（结构化封装）就不在这里给出，详情参见我的<br><a href="https://github.com/Interesting6/my_machine_learning/blob/master/my_decision_tree_id3.py" target="_blank" rel="noopener">GitHub</a>。</li></ul></blockquote><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>这里我们采用uci的lense<a href="http://archive.ics.uci.edu/ml/datasets/Lenses" target="_blank" rel="noopener">隐形眼镜测试集</a>,总样本为24个，四个特征，三个类别。我们通过网络爬虫，直接从该网址抓取数据，并转换为dataframe类型。</p><blockquote><p>– 3 Classes:<br>     1 : the patient should be fitted with hard contact lenses,<br>     2 : the patient should be fitted with soft contact lenses,<br>     3 : the patient should not be fitted with contact lenses.</p></blockquote><blockquote><p>– 4 Features:</p><pre><code>1. age of the patient: (1) young, (2) pre-presbyopic, (3) presbyopic2. spectacle prescription:  (1) myope, (2) hypermetrope3. astigmatic:     (1) no, (2) yes4. tear production rate:  (1) reduced, (2) normal</code></pre></blockquote><p>下面我们给出代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ID3 <span class="keyword">import</span> ID3_tree</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data</span><span class="params">()</span>:</span></span><br><span class="line">    url = <span class="string">"http://archive.ics.uci.edu/ml/machine-learning-databases/lenses/lenses.data"</span></span><br><span class="line">    data = requests.get(url).text</span><br><span class="line">    verctors = data.split(<span class="string">'\n'</span>)</span><br><span class="line">    verctors = [ver.split() <span class="keyword">for</span> ver <span class="keyword">in</span> verctors]</span><br><span class="line">    features = [<span class="string">"id"</span>,<span class="string">"age"</span>,<span class="string">"prescript"</span>,<span class="string">"astigmatic"</span>,<span class="string">"tearRate"</span>,<span class="string">"category"</span>]</span><br><span class="line">    df = pd.DataFrame(verctors,columns=features, dtype=int)</span><br><span class="line">    df = df.set_index(<span class="string">"id"</span>).dropna()</span><br><span class="line">    key_list = [&#123;<span class="string">"1"</span>:<span class="string">"young"</span>, <span class="string">"2"</span>:<span class="string">"pre-presbyopic"</span>, <span class="string">"3"</span>:<span class="string">"presbyopic"</span>&#125;, &#123;<span class="string">"1"</span>: <span class="string">"myope"</span>, <span class="string">"2"</span>: <span class="string">"hypermetrope"</span>&#125;</span><br><span class="line">    , &#123;<span class="string">"1"</span>: <span class="string">"no"</span>, <span class="string">"2"</span>: <span class="string">"yes"</span>&#125;, &#123;<span class="string">"1"</span>: <span class="string">"reduced"</span>, <span class="string">"2"</span>:<span class="string">"normal"</span>&#125;,&#123;<span class="string">"1"</span>:<span class="string">"hard"</span>,<span class="string">"2"</span>:<span class="string">"soft"</span>,<span class="string">"3"</span>:<span class="string">"no lenses"</span>&#125;]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        df.iloc[:,i] = df.iloc[:,i].apply(<span class="keyword">lambda</span> x:key_list[i][x])</span><br><span class="line">    <span class="keyword">return</span> df</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    df = get_data()</span><br><span class="line">    <span class="comment"># print(df)</span></span><br><span class="line">    id3_tree = ID3_tree(df)</span><br><span class="line">    id3_tree = id3_tree.train(df)</span><br><span class="line">    my_tree = id3_tree.my_tree</span><br><span class="line">    print(my_tree)</span><br><span class="line">    id3_tree.show_tree(my_tree)</span><br></pre></td></tr></table></figure></p><p>得出的决策树如下：<br><img src="/images/tree3.png" alt="image"></p><h2 id="后续"><a href="#后续" class="headerlink" title="后续"></a>后续</h2><p>由于本文只给出了ID3算法生成决策树和决策树的可视化，日后我将继续给出C4.5的生成决策树算法、决策树的减枝问题与及CART分类和回归树的构造。然后我们还可以把它拓宽，引入集成学习的随机森林。</p><p>作者时间精力有限，我就先写到这里啦。如有疑问，记得联系我哦。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><blockquote><p>《统计学习方法》李航 著  清华大学出版社<br>《机器学习实战》Peter Harrington 著 人民邮电出版社</p></blockquote><!-- ## 最后如果你觉得本文对你有帮助的话，不如给作者一点打赏吧~| ![image](/images/alipay.jpg) | ![image](/images/wechatpay.png) |谢谢！ -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;  本文主要讲述一下决策树的基本算法–ID3生成决策树算法。一开始看例子的时候，我觉得决策树好简单呀，应该实现起来用&lt;code&gt;pandas
      
    
    </summary>
    
      <category term="machine_leanring" scheme="http://yoursite.com/categories/machine-leanring/"/>
    
      <category term="code" scheme="http://yoursite.com/categories/machine-leanring/code/"/>
    
    
      <category term="machine_leanring" scheme="http://yoursite.com/tags/machine-leanring/"/>
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>Radial basis function kernel</title>
    <link href="http://yoursite.com/2018/03/18/Radial-basis-function-kernel/"/>
    <id>http://yoursite.com/2018/03/18/Radial-basis-function-kernel/</id>
    <published>2018-03-18T10:54:27.000Z</published>
    <updated>2018-03-23T08:15:06.901Z</updated>
    
    <content type="html"><![CDATA[<h2 id="高斯径向基函数"><a href="#高斯径向基函数" class="headerlink" title="高斯径向基函数"></a>高斯径向基函数</h2><h4 id="简介："><a href="#简介：" class="headerlink" title="简介："></a>简介：</h4><p>在机器学习中，（高斯）径向基函数核（英语：Radial basis function kernel），或称为RBF核，是一种常用的核函数。它是支持向量机分类中最为常用的核函数。—— <a href="https://zh.wikipedia.org/wiki/%E5%BE%84%E5%90%91%E5%9F%BA%E5%87%BD%E6%95%B0%E6%A0%B8" target="_blank" rel="noopener">维基百科</a></p><h2 id="高斯径向基函数-1"><a href="#高斯径向基函数-1" class="headerlink" title="高斯径向基函数"></a>高斯径向基函数</h2><h4 id="高斯径向基函数公式如下："><a href="#高斯径向基函数公式如下：" class="headerlink" title="高斯径向基函数公式如下："></a>高斯径向基函数公式如下：</h4><p>$$K(x_1,x_2)=\exp{(-\frac{\parallel x_1-x_2 \parallel^2 }{2\sigma^2})}, \sigma&gt;0$$</p><h4 id="那么它有什么几何意义呢"><a href="#那么它有什么几何意义呢" class="headerlink" title="那么它有什么几何意义呢?"></a>那么它有什么几何意义呢?</h4><p>先看看x经过映射以后，在高维空间里这个点到原点的距离公式：<br> $$ \parallel x_i-0\parallel^2 = \parallel x_i \parallel^2=\left \langle  \Phi (x_i), \Phi (x_i)\right \rangle=K(x_i,x_i)=1$$<br>这表明样本x映射到高维空间后，在高维空间中的点$\Phi (x_i)$到高维空间中原点的距离为1，也即$\Phi (x_i)$存在于一个<strong>超球面</strong>上。</p><h4 id="为什么核函数能映射到高维空间呢？"><a href="#为什么核函数能映射到高维空间呢？" class="headerlink" title="为什么核函数能映射到高维空间呢？"></a>为什么核函数能映射到高维空间呢？</h4><p>先考虑普通的多项式核函数：$K(x,y)=(x^T\cdot y+m)^p$,其中$x,y\in \mathbb{R}^n$，多项式参数$p,m\in \mathbb{R}$<br>考虑$x=(x_1,x_2),y=(y_1,y_2),m=0,p=2$，即$K(x,y)=(x^T\cdot y)^2=x_1^2y_1^2+2x_1x_2y_1y_2+x_2^2y_2^2$<br>现在回到之前的映射$k(x,y)=\left \langle \Phi(x),\Phi(y)\right \rangle$，并取$\Phi(x)=(x_1^2,\sqrt{2}x_1x_2,x_2^2)$<br>则有$k(x,y)=\left \langle \Phi(x),\Phi(y)\right \rangle=x_1^2y_1^2+2x_1x_2y_1y_2+x_2^2y_2^2=(x^T\cdot y)^2=K(x,y)$<br>这就是前面的$K(x,y)$，因此，该核函数就将2维映射到了3维空间。</p><h4 id="径向基核又为什么能够映射到无限维空间呢？"><a href="#径向基核又为什么能够映射到无限维空间呢？" class="headerlink" title="径向基核又为什么能够映射到无限维空间呢？"></a>径向基核又为什么能够映射到无限维空间呢？</h4><p>看完了普通多项式核函数由2维向3维的映射，再来看看高斯径向基函数会把2维平面上一点映射到多少维。<br>$$\begin{eqnarray}<br>K(x,y) &amp; = &amp; \exp(| x_1-x_2 |^2 ) \<br>&amp; = &amp; \exp(-(x_1-y_1)^2-(x_2-y_2)^2) \<br>&amp; = &amp; \exp(-x_1^2+2x_1y_1-y_1^2-x_2^2+2x_2y_2-y_2^2) \<br>&amp; = &amp; \exp(-|x|^2)\exp(-|y|^2)\exp(2x^Ty)\<br>\end{eqnarray}$$<br>将最后一项泰勒展开你就会恍然大悟：<br>$$K(x,y)=\exp(-|x|^2)\exp(-|y|^2)\sum_{n=0}^{\infty}\frac{(2x^Ty)^n}{n!}$$</p><p>再具体一点：<br>高斯核是这样定义的：$K(x_1,x_2)=\exp{(-\frac{\parallel x_1-x_2 \parallel^2 }{2\sigma^2})}, \sigma&gt;0$<br>尽管我不会解释它（但相信我）高斯核可以简单修正为这个样子：$K(x_1,x_2)=\exp(-\frac{x_1\cdot x_2}{\sigma^2}),\sigma&gt;0$，这里$x_1\cdot x_2$可解释为内积。<br>再用泰勒展开就会得到$K(x_1,x_2)=\sum_{n=0}^{\infty}\frac{(x_1\cdot x_2)^n}{\sigma^nn!}$<br>求和号里面的元素是不是看起来很熟悉呢？<br>没错，这就是一个n次多项式核。因为每一个多项式核都将一个向量投影到更高维的空间中，因此高斯核是那些$degree\geq0$的<strong>多项式核</strong>的<strong>组合</strong>，所以我们说高斯核是投影到无穷维空间中。</p><ul><li>参考<br>Quora上的问题：<a href="https://www.quora.com/Machine-Learning/Why-does-the-RBF-radial-basis-function-kernel-map-into-infinite-dimensional-space-mentioned-many-times-in-machine-learning-lectures" target="_blank" rel="noopener">https://www.quora.com/Machine-Learning/Why-does-the-RBF-radial-basis-function-kernel-map-into-infinite-dimensional-space-mentioned-many-times-in-machine-learning-lectures</a></li></ul><h4 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h4><p>初学者使用markdown与LaTeX的语法真不适应啊，就写到这里吧，写篇博客太累了orz饭都没吃，关于RBF神经网络的话，以后用到再来讲吧，累死我惹。<br>如有疑问，欢迎咨询，联系方式见“关于”页面。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;高斯径向基函数&quot;&gt;&lt;a href=&quot;#高斯径向基函数&quot; class=&quot;headerlink&quot; title=&quot;高斯径向基函数&quot;&gt;&lt;/a&gt;高斯径向基函数&lt;/h2&gt;&lt;h4 id=&quot;简介：&quot;&gt;&lt;a href=&quot;#简介：&quot; class=&quot;headerlink&quot; title=
      
    
    </summary>
    
      <category term="math" scheme="http://yoursite.com/categories/math/"/>
    
      <category term="theory" scheme="http://yoursite.com/categories/math/theory/"/>
    
    
      <category term="machine_leanring" scheme="http://yoursite.com/tags/machine-leanring/"/>
    
      <category term="math" scheme="http://yoursite.com/tags/math/"/>
    
  </entry>
  
  <entry>
    <title>python 的闭包、装饰器</title>
    <link href="http://yoursite.com/2018/03/17/python-%E7%9A%84%E9%97%AD%E5%8C%85%E3%80%81%E8%A3%85%E9%A5%B0%E5%99%A8/"/>
    <id>http://yoursite.com/2018/03/17/python-的闭包、装饰器/</id>
    <published>2018-03-17T09:33:14.000Z</published>
    <updated>2018-03-17T10:10:55.183Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、闭包（Closure）"><a href="#一、闭包（Closure）" class="headerlink" title="一、闭包（Closure）"></a>一、闭包（Closure）</h1><h2 id="什么是闭包？"><a href="#什么是闭包？" class="headerlink" title="什么是闭包？"></a>什么是闭包？</h2><blockquote><p>在计算机科学中，闭包（英语：Closure），又称词法闭包（Lexical Closure）或函数闭包（function closures），是引用了自由变量的函数。这个被引用的自由变量将和这个函数一同存在，即使已经离开了创造它的环境也不例外。所以，有另一种说法认为闭包是由函数和与其相关的引用环境组合而成的实体。闭包在运行时可以有多个实例，不同的引用环境和相同的函数组合可以产生不同的实例。—— <a href="https://zh.wikipedia.org/wiki/闭包_(计算机科学" target="_blank" rel="noopener">维基百科</a>)</p></blockquote><p>这里我给个简单的解释：一个<strong>闭包</strong>就是你调用了一个<em>函数A</em>，这个<em>函数A</em>返回了一个<em>函数B</em>给你。这个<strong>返回的函数B</strong>就叫做闭包。你在<strong>调用函数A</strong>的时候<strong>传递的参数</strong>就是<strong>自由变量</strong>。</p><h2 id="举个例子："><a href="#举个例子：" class="headerlink" title="举个例子："></a>举个例子：</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">(name)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">inner_func</span><span class="params">(age)</span>:</span></span><br><span class="line">        <span class="keyword">print</span> <span class="string">'name:'</span>, name, <span class="string">'age:'</span>, age</span><br><span class="line">    <span class="keyword">return</span> inner_func</span><br><span class="line"></span><br><span class="line">bb = func(<span class="string">'matrix'</span>)</span><br><span class="line">bb(<span class="number">26</span>)  <span class="comment"># &gt;&gt;&gt; name: matrix age: 26</span></span><br></pre></td></tr></table></figure><p>这里面调用func的时候就产生了一个闭包– <em>inner_func</em>,并且该闭包持有自由变量– <em>name</em>，因此这也意味着，当函数func的生命周期结束之后，<strong>name这个变量依然存在</strong>，因为它被闭包引用了，所以不会被回收。</p><ul><li>也有人说这种内部函数inner_func可以使用外部函数的变量name的行为就叫闭包。</li></ul><h1 id="二、装饰器（Decorator）"><a href="#二、装饰器（Decorator）" class="headerlink" title="二、装饰器（Decorator）"></a>二、装饰器（Decorator）</h1><h2 id="什么是装饰器？"><a href="#什么是装饰器？" class="headerlink" title="什么是装饰器？"></a>什么是装饰器？</h2><blockquote><p>“装饰器的功能是将被装饰的函数当作参数传递给与装饰器对应的函数（名称相同的函数），并返回包装后的被装饰的函数”</p></blockquote><p>听起来有点绕，没关系，直接看示意图,其中a为 与<em>装饰器@a</em>对应的函数，<em>b</em>为装饰器修饰的函数，<em>装饰器@a</em>的作用是：<br><img src="/images/decorator.png" alt="image"><br><strong>简而言之：@a 就是将 b 传递给 a()，并返回新的 b = a(b)</strong></p><h2 id="举个例子"><a href="#举个例子" class="headerlink" title="举个例子"></a>举个例子</h2><ol><li><p>先导入包：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> reduce</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line">logging.basicConfig(level=logging.INFO)</span><br></pre></td></tr></table></figure></li><li><p>定义一个检查参数的装饰器：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">checkParams</span><span class="params">(fn)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapper</span><span class="params">(*numbers)</span>:</span></span><br><span class="line">        temp = map(<span class="keyword">lambda</span> x:isinstance(x,(int,)),numbers) <span class="comment"># 检查参数是否都为整型</span></span><br><span class="line">        <span class="keyword">if</span> reduce(<span class="keyword">lambda</span> x,y: x <span class="keyword">and</span> y, temp): <span class="comment"># 若都为整型</span></span><br><span class="line">            <span class="keyword">return</span> fn(*numbers)             <span class="comment"># 则调用fn(*numbers)返回计算结果</span></span><br><span class="line">        <span class="comment">#否则通过logging记录错误信息，并友好退出</span></span><br><span class="line">        logging.warning(<span class="string">"variable numbers cannot be added"</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="keyword">return</span> wrapper     <span class="comment">#fn引用gcd，被封存在闭包的执行环境中返回</span></span><br></pre></td></tr></table></figure></li><li><p>然后定义求最大公约数的函数（能求多个的最大公约数）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gcd</span><span class="params">(*numbers)</span>:</span></span><br><span class="line">    <span class="string">"""return the greatest common divisor of the given integers."""</span></span><br><span class="line">    <span class="keyword">return</span> reduce(math.gcd, numbers)</span><br></pre></td></tr></table></figure></li><li><p>调用</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;gcd = checkParams(gcd)</span><br><span class="line">&gt;&gt;&gt;gcd(3, <span class="string">'hello'</span>)</span><br><span class="line"><span class="comment"># 输出 WARNING:root: variable numbers cannot be added</span></span><br></pre></td></tr></table></figure></li></ol><p>注意checkParams函数：</p><blockquote><ul><li>首先看参数fn，当我们调用checkParams(gcd)的时候，它将成为函数对象gcd的一个本地(Local)引用；</li><li>在checkParams内部，我们定义了一个wrapper函数，添加了参数类型检查的功能，然后调用了fn(*numbers)，根据LEGB法则，解释器将搜索几个作用域，并最终在(Enclosing层) checkParams函数的本地作用域中找到fn；</li><li>注意最后的return wrapper，这将创建一个闭包，fn变量(gcd函数对象的一个引用)将会封存在闭包的执行环境中，不会随着checkParams的返回而被回收；</li></ul></blockquote><p>当调用gcd = checkParams(gcd)时，gcd指向了新的wrapper对象，它添加了参数检查和记录日志的功能，同时又能够通过封存的fn，继续调用原始的gcd进行最大公约数运算。</p><p>因此调用gcd(3, ‘hello’)将不会返回计算结果，而是打印出日志：root: variable numbers cannot be added。</p><p>有人觉得add = checkParams(add)这样的写法未免太过麻烦，于是python提供了一种更优雅的写法，被称为<strong>语法糖</strong>：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@checkParams</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lcm</span><span class="params">(*numbers)</span>:</span></span><br><span class="line">    <span class="string">"""return lowest common multiple."""</span></span><br><span class="line">    f = <span class="keyword">lambda</span> a,b:int((a*b)/gcd(a,b))</span><br><span class="line">    <span class="keyword">return</span> reduce(f, numbers)</span><br></pre></td></tr></table></figure></p><p>其实这只是一种写法上的优化，解释器仍然会将它转化为gcd = checkParams(gcd)来执行。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;一、闭包（Closure）&quot;&gt;&lt;a href=&quot;#一、闭包（Closure）&quot; class=&quot;headerlink&quot; title=&quot;一、闭包（Closure）&quot;&gt;&lt;/a&gt;一、闭包（Closure）&lt;/h1&gt;&lt;h2 id=&quot;什么是闭包？&quot;&gt;&lt;a href=&quot;#什么是
      
    
    </summary>
    
      <category term="math" scheme="http://yoursite.com/categories/math/"/>
    
      <category term="python" scheme="http://yoursite.com/categories/math/python/"/>
    
    
      <category term="fun math" scheme="http://yoursite.com/tags/fun-math/"/>
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>hexo常用命令笔记</title>
    <link href="http://yoursite.com/2018/03/13/hexo%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2018/03/13/hexo常用命令笔记/</id>
    <published>2018-03-13T14:59:57.000Z</published>
    <updated>2018-03-13T15:10:19.665Z</updated>
    
    <content type="html"><![CDATA[<h1 id="hexo"><a href="#hexo" class="headerlink" title="hexo"></a>hexo</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo -g <span class="comment">#安装</span></span><br><span class="line">npm update hexo -g <span class="comment">#升级</span></span><br><span class="line">hexo init <span class="comment">#初始化</span></span><br></pre></td></tr></table></figure><h1 id="简写"><a href="#简写" class="headerlink" title="简写"></a>简写</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hexo n <span class="string">"我的博客"</span> == hexo new <span class="string">"我的博客"</span> <span class="comment">#新建文章</span></span><br><span class="line">hexo p == hexo publish</span><br><span class="line">hexo g == hexo generate<span class="comment">#生成</span></span><br><span class="line">hexo s == hexo server <span class="comment">#启动服务预览</span></span><br><span class="line">hexo d == hexo deploy<span class="comment">#部署</span></span><br></pre></td></tr></table></figure><h1 id="服务器"><a href="#服务器" class="headerlink" title="服务器"></a>服务器</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hexo server <span class="comment">#Hexo 会监视文件变动并自动更新，您无须重启服务器。</span></span><br><span class="line">hexo server -s <span class="comment">#静态模式</span></span><br><span class="line">hexo server -p 5000 <span class="comment">#更改端口</span></span><br><span class="line">hexo server -i 192.168.1.1 <span class="comment">#自定义 IP</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo clean #清除缓存 网页正常情况下可以忽略此条命令</span><br><span class="line">hexo g #生成静态网页</span><br><span class="line">hexo d #开始部署</span><br></pre></td></tr></table></figure><h1 id="监视文件变动"><a href="#监视文件变动" class="headerlink" title="监视文件变动"></a>监视文件变动</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hexo generate #使用 Hexo 生成静态文件快速而且简单</span><br><span class="line">hexo generate --watch #监视文件变动</span><br></pre></td></tr></table></figure><h1 id="完成后部署"><a href="#完成后部署" class="headerlink" title="完成后部署"></a>完成后部署</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 以下两个命令的作用是相同的</span><br><span class="line">hexo generate --deploy</span><br><span class="line">hexo deploy --generate</span><br></pre></td></tr></table></figure><h2 id="简写-1"><a href="#简写-1" class="headerlink" title="简写"></a>简写</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hexo deploy -g</span><br><span class="line">hexo server -g</span><br></pre></td></tr></table></figure><h1 id="草稿"><a href="#草稿" class="headerlink" title="草稿"></a>草稿</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo publish [layout] &lt;title&gt;</span><br></pre></td></tr></table></figure><h1 id="模版"><a href="#模版" class="headerlink" title="模版"></a>模版</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hexo new &quot;postName&quot; #新建文章</span><br><span class="line">hexo new page &quot;pageName&quot; #新建页面</span><br><span class="line">hexo generate #生成静态页面至public目录</span><br><span class="line">hexo server #开启预览访问端口（默认端口4000，&apos;ctrl + c&apos;关闭server）</span><br><span class="line">hexo deploy #将.deploy目录部署到GitHub</span><br></pre></td></tr></table></figure><p>如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo new [layout] &lt;title&gt;</span><br><span class="line">hexo new photo &quot;My Gallery&quot;</span><br><span class="line">hexo new &quot;Hello World&quot; --lang tw</span><br></pre></td></tr></table></figure></p><table><thead><tr><th>变量</th><th style="text-align:center">描述</th></tr></thead><tbody><tr><td>layout</td><td style="text-align:center">布局</td></tr><tr><td>title</td><td style="text-align:center">标题</td></tr><tr><td>date</td><td style="text-align:center">文件建立日期</td></tr></tbody></table><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">title: 使用Hexo搭建个人博客</span><br><span class="line">layout: post</span><br><span class="line">date: 2014-03-03 19:07:43</span><br><span class="line">comments: true</span><br><span class="line">categories: Blog</span><br><span class="line">tags: [Hexo]</span><br><span class="line">keywords: Hexo, Blog</span><br><span class="line">description: 生命在于折腾，又把博客折腾到Hexo了。给Hexo点赞。</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;hexo&quot;&gt;&lt;a href=&quot;#hexo&quot; class=&quot;headerlink&quot; title=&quot;hexo&quot;&gt;&lt;/a&gt;hexo&lt;/h1&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre
      
    
    </summary>
    
      <category term="hexo" scheme="http://yoursite.com/categories/hexo/"/>
    
      <category term="Instructions" scheme="http://yoursite.com/categories/hexo/Instructions/"/>
    
    
      <category term="hexo" scheme="http://yoursite.com/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>my_knn</title>
    <link href="http://yoursite.com/2018/03/13/my-knn/"/>
    <id>http://yoursite.com/2018/03/13/my-knn/</id>
    <published>2018-03-13T13:58:30.000Z</published>
    <updated>2018-03-23T08:14:41.549Z</updated>
    
    <content type="html"><![CDATA[<h1 id="关于"><a href="#关于" class="headerlink" title="关于"></a>关于</h1><p>在一个月前，需要用<code>1nn</code>做二分类的测试的时候，开始因为用<code>sklearn</code>训练数据时用错了数据集，百思不得其解，于是自己写了个<code>knn</code>来训练，当时写好后，才真正把原理给弄懂了orz，原来是数据集训练时用错了。。。改正后对比了一下自己的<code>knn</code>和<code>sklearn</code>的<code>knn</code>的准确率都差不多（也就是说测试通过啦），就上传到了我的GitHub。</p><p>当时我虽然有个用腾讯云搭建的博客，但基本上都没在上面写过了orz，本博客当时还没有问世，正好基于GitHub的服务器最近搭了这个博客，空空的也不好，最近老师第一讲就讲knn，那就把之前的代码贴上吧。</p><h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><p>对于一个输入的测试数据，计算该样本点到训练数据各样本点的距离，然后对所有距离由小到大排列，取前k个数据；统计该k个数据中对应的标签出现次数最多的标签，则该测试样本就被标记为该标签。</p><h1 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h1><ul><li>输入: 训练数据集：$T={(X_1,y_1),(X_2,y_2),…,(X_N,y_N)}$, 其中$X_i={x_i^1,x_i^2,…,x_i^n}$,有n个特征，N个样本点;</li><li>输入：最近邻个数k，及要预测的样本点$X_0={x_0^1,,x_0^2,…,x_0^n}$;</li><li>计算：样本点X_0到训练数据集T中各样本点的距离（一般为欧氏距离）;</li><li>排序：将以上算出的距离由小到大排序，并选出前k个距离数据;</li><li>统计：统计前k个距离数据中各个标签对应的个数，选出个数最多的那个标签，即为该样本点预测的结果。</li></ul><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">my_knn</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""docstring for my_knn"""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, k)</span>:</span></span><br><span class="line">        super(my_knn, self).__init__()</span><br><span class="line">        self.k = k</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, X_train, y_train)</span>:</span></span><br><span class="line">        self.X_train, self.y_train = np.array(X_train), np.array(y_train)</span><br><span class="line">        <span class="keyword">if</span> len(self.X_train) != len(self.y_train):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"X_test,y_test or y_train was not equail!"</span></span><br><span class="line">                             <span class="string">"The length of X_test,y_test is %s"</span></span><br><span class="line">                             <span class="string">"But the length of y_train is %s"</span> % (len(self.X_train), len(self.y_train)))</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict_one</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        dist2xtrain = np.sum((X - self.X_train)**<span class="number">2</span>, axis=<span class="number">1</span>)**<span class="number">0.5</span></span><br><span class="line">        index = dist2xtrain.argsort() <span class="comment"># 从小到大（近到远）</span></span><br><span class="line">        label_count = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.k):</span><br><span class="line">            label = self.y_train[index[i]]</span><br><span class="line">            label_count[label] = label_count.get(label, <span class="number">0</span>) + <span class="number">1</span></span><br><span class="line">        <span class="comment"># 将label_count的值从大到小排列label_count的键</span></span><br><span class="line">        y_predict = sorted(label_count, key=<span class="keyword">lambda</span> x: label_count[x], reverse=<span class="keyword">True</span>)[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> y_predict</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict_all</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> np.array(list(map(self.predict_one, X)))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calc_accuracy</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">        predict = self.predict_all(X)</span><br><span class="line">        total = X.shape[<span class="number">0</span>]</span><br><span class="line">        right = sum(predict == y)</span><br><span class="line">        accuracy = right/total</span><br><span class="line">        <span class="keyword">return</span> accuracy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    data_set = load_iris()</span><br><span class="line">    datas = data_set[<span class="string">"data"</span>]</span><br><span class="line">    labels = data_set[<span class="string">'target'</span>]</span><br><span class="line">    X_train, X_test, y_train, y_test = train_test_split(datas, labels, test_size=<span class="number">0.4</span>, random_state=<span class="number">0</span>)</span><br><span class="line">    knn = my_knn(<span class="number">1</span>)</span><br><span class="line">    knn = knn.train(X_train,y_train)</span><br><span class="line">    accuracy = knn.calc_accuracy(X_test,y_test)</span><br><span class="line">    print(<span class="string">"%.3f%%"</span> % (accuracy * <span class="number">100</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line">    neigh = KNeighborsClassifier(n_neighbors=<span class="number">1</span>)</span><br><span class="line">    neigh.fit(X_train, y_train)</span><br><span class="line">    print(neigh.score(X_train,y_train))</span><br><span class="line">    print(neigh.score(X_test, y_test))</span><br></pre></td></tr></table></figure><h1 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h1><p>关于对knn的kd树加速这部分还需要日后的后续学习，这里就先不说啦（其实是我也不会23333）。<br>由于我对markdown语法不太熟悉，写起文章来的有点别扭还望理解（逃。</p><h1 id="写给自己"><a href="#写给自己" class="headerlink" title="写给自己"></a>写给自己</h1><p>  还是要多花点时间学习啊！一个多月没学习就忘得差不多了orz,还好看一下就能回想起来。多练习吧！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;关于&quot;&gt;&lt;a href=&quot;#关于&quot; class=&quot;headerlink&quot; title=&quot;关于&quot;&gt;&lt;/a&gt;关于&lt;/h1&gt;&lt;p&gt;在一个月前，需要用&lt;code&gt;1nn&lt;/code&gt;做二分类的测试的时候，开始因为用&lt;code&gt;sklearn&lt;/code&gt;训练数据时用错了数
      
    
    </summary>
    
      <category term="machine_leanring" scheme="http://yoursite.com/categories/machine-leanring/"/>
    
      <category term="code" scheme="http://yoursite.com/categories/machine-leanring/code/"/>
    
    
      <category term="machine_leanring" scheme="http://yoursite.com/tags/machine-leanring/"/>
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>my_bayes</title>
    <link href="http://yoursite.com/2018/03/13/my-bayes/"/>
    <id>http://yoursite.com/2018/03/13/my-bayes/</id>
    <published>2018-03-13T13:09:38.000Z</published>
    <updated>2018-03-23T08:14:51.626Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本周一晚上老师讲到了<code>naive bayes（朴素贝叶斯分类器）</code>，于是自己用python来实现了一下。现在这个脚本对于比较大的数据可能会计算的比较慢，还需要以后慢慢再研究一下里面的加速。</p><p>本程序主要利用了pandas里dataframe的groupby分组函数，大大的方便了对数据的统计。对于条件概率，有不同的标签，不同的特征和特征里的不同数据，我们采用了<code>dict</code>数据结构，第一层key为标签，value是一个新的dict；第二层（前面那个新的dict）的key为特征，value是一个Series或者字典；第三层的key/index为特征的取值，value为频数/概率。、、（虽然看起来比较拗口，但我感觉这样能够比较清晰的分清了各个条件概率了，如果你有更好的方法，欢迎留言给我，谢谢。）</p><h1 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h1><blockquote><ul><li>输入：训练数据集及其标签集，要预测的数据集</li><li>统计各标签出现的频数，并拉普拉斯平滑，计算先验概率</li><li>统计在各标签下各个特征的频数，并拉普拉斯平滑，计算条件概率</li><li>查找要预测数据集各特征在不同标签下的条件概率和先验概率相乘得到（半）后验概率</li><li>对半后验概率进行从大到小排序，选出最大值对应的标签，即为预测结果</li><li>实例化测试<br>ps：这里半后验概率为我自己的定义：$P(Y_j) *\prod_{i=1}^N P(A_i|Y_j) ; i:1\to n_{feature}; j:1\to n_{label}$</li></ul></blockquote><h1 id="解释"><a href="#解释" class="headerlink" title="解释"></a>解释</h1><p>本程序主要分为一下部分：</p><blockquote><ul><li>定义一个bayes分类器（类）</li><li>计算先验概率</li><li>计算所有条件概率</li><li>进行调用训练</li><li>对测试数据进行预测</li><li>实例化测试</li></ul></blockquote><p>以上各对应之下的各个函数：（废话不多说，直接上代码）</p><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">my_naive_bayes</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, df)</span>:</span></span><br><span class="line">        super(my_naive_bayes, self).__init__()</span><br><span class="line">        self.df = df</span><br><span class="line">        self.X_train = df.iloc[:,:<span class="number">-1</span>]</span><br><span class="line">        self.y_train = df.iloc[:,<span class="number">-1</span>]</span><br><span class="line">        self.label_set = set(self.y_train)</span><br><span class="line">        self.features = df.columns[:<span class="number">-1</span>]</span><br><span class="line">        self.label_name = df.columns[<span class="number">-1</span>]</span><br><span class="line">        self.feature_dict = &#123;&#125;</span><br><span class="line">        self.n_sample = len(df)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_prior_p</span><span class="params">(self, g)</span>:</span></span><br><span class="line">        n = len(g)</span><br><span class="line">        prior_p = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> label <span class="keyword">in</span> self.label_set:</span><br><span class="line">            prior_p[label] = g.size()[label] / self.n_sample</span><br><span class="line">        <span class="keyword">return</span> prior_p</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_cond_p</span><span class="params">(self, g)</span>:</span></span><br><span class="line">        cond_p = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> label, group <span class="keyword">in</span> g:</span><br><span class="line">            cond_p[label] = &#123;&#125;</span><br><span class="line">            <span class="keyword">for</span> feature <span class="keyword">in</span> self.features:</span><br><span class="line">                counts = group[feature].value_counts()</span><br><span class="line">                cond_p[label][feature] = counts / sum(counts)</span><br><span class="line">        <span class="keyword">return</span> cond_p</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, )</span>:</span></span><br><span class="line">        <span class="keyword">for</span> feature <span class="keyword">in</span> self.features:</span><br><span class="line">            self.feature_dict[feature] = set(self.df[feature])</span><br><span class="line">        g = self.df.groupby(self.label_name)</span><br><span class="line"></span><br><span class="line">        self.prior_p = self.get_prior_p(g)</span><br><span class="line">        self.cond_p = self.get_cond_p(g)</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict_one</span><span class="params">(self, test_X)</span>:</span></span><br><span class="line">        semi_post_p = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> label <span class="keyword">in</span> self.label_set:</span><br><span class="line">            temp = <span class="number">1</span></span><br><span class="line">            <span class="keyword">for</span> feature <span class="keyword">in</span> self.features:</span><br><span class="line">                temp = temp * self.cond_p[label][feature][test_X[feature]]</span><br><span class="line">            semi_post_p[label] = self.prior_p[label] * temp</span><br><span class="line">        <span class="keyword">return</span> max(semi_post_p, key=semi_post_p.get)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">df = pd.read_excel(<span class="string">"bayes_data.xlsx"</span>,index_col=<span class="string">"index"</span>)</span><br><span class="line"><span class="comment"># n = len(df)</span></span><br><span class="line"><span class="comment"># train_n = int(n*0.6)</span></span><br><span class="line"><span class="comment"># train_df = df[:train_n]</span></span><br><span class="line"><span class="comment"># test_df = df[train_n:]</span></span><br><span class="line">bayes = my_naive_bayes(df)</span><br><span class="line">bayes = bayes.train()</span><br><span class="line">test_x = df.loc[<span class="number">6</span>]</span><br><span class="line">label = bayes.predict_one(test_x)</span><br><span class="line">print(label)</span><br></pre></td></tr></table></figure><h1 id="最后，好好学习，天天向上！"><a href="#最后，好好学习，天天向上！" class="headerlink" title="最后，好好学习，天天向上！"></a>最后，好好学习，天天向上！</h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;本周一晚上老师讲到了&lt;code&gt;naive bayes（朴素贝叶斯分类器）&lt;/code&gt;，于是自己用python来实现了一下。现在这个脚本对
      
    
    </summary>
    
      <category term="machine_leanring" scheme="http://yoursite.com/categories/machine-leanring/"/>
    
      <category term="code" scheme="http://yoursite.com/categories/machine-leanring/code/"/>
    
    
      <category term="machine_leanring" scheme="http://yoursite.com/tags/machine-leanring/"/>
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>18_03_07</title>
    <link href="http://yoursite.com/2018/03/07/18-03-07/"/>
    <id>http://yoursite.com/2018/03/07/18-03-07/</id>
    <published>2018-03-07T08:27:50.000Z</published>
    <updated>2018-03-07T08:43:35.364Z</updated>
    
    <content type="html"><![CDATA[<p>今天是开学第三天，各种事情莫名烦躁。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;今天是开学第三天，各种事情莫名烦躁。&lt;/p&gt;

      
    
    </summary>
    
      <category term="diary" scheme="http://yoursite.com/categories/diary/"/>
    
    
      <category term="diary" scheme="http://yoursite.com/tags/diary/"/>
    
      <category term="life-style" scheme="http://yoursite.com/tags/life-style/"/>
    
  </entry>
  
  <entry>
    <title>SVM explained-well</title>
    <link href="http://yoursite.com/2018/03/02/SVM-explained-well/"/>
    <id>http://yoursite.com/2018/03/02/SVM-explained-well/</id>
    <published>2018-03-02T09:40:40.000Z</published>
    <updated>2018-03-02T09:52:39.291Z</updated>
    
    <content type="html"><![CDATA[<p>Support vector machines (SVM)</p><p>User <a href="http://www.reddit.com/user/copperking" target="_blank" rel="noopener">copperking</a> stepped up to the plate:</p><p>“We have 2 colors of balls on the table that we want to separate.</p><p><img src="/images/SVM/svm1-300x225.png" alt="image"></p><p>We get a stick and put it on the table, this works pretty well right?</p><p><img src="/images/SVM/svm2-300x225.png" alt="image"></p><p>Some villain comes and places more balls on the table, it kind of works but one of the balls is on the wrong side and there is probably a better place to put the stick now.</p><p><img src="/images/SVM/svm3-300x225.png" alt="image"></p><p>SVMs try to put the stick in the best possible place by having as big a gap on either side of the stick as possible.</p><p><img src="/images/SVM/svm4-300x225.png" alt="image"></p><p>Now when the villain returns the stick is still in a pretty good spot.</p><p><img src="/images/SVM/svm5-300x225.png" alt="image"></p><p>There is another trick in the SVM toolbox that is even more important. Say the villain has seen how good you are with a stick so he gives you a new challenge.</p><p><img src="/images/SVM/svm6-300x225.png" alt="image">svm7-300x167.png</p><p>There’s no stick in the world that will let you split those balls well, so what do you do? You flip the table of course! Throwing the balls into the air. Then, with your pro ninja skills, you grab a sheet of paper and slip it between the balls.</p><p><img src="/images/SVM/svm7-300x167.png" alt="image"></p><p>Now, looking at the balls from where the villain is standing, they balls will look split by some curvy line.</p><p><img src="/images/SVM/svm8-300x225.png" alt="image"></p><p>Boring adults the call balls data, the stick a classifier, the biggest gap trick optimization, call flipping the table kernelling and the piece of paper a hyperplane.”</p><p>I think the last step is the most beautiful no mater in mathematic or machine learning! Hope it will help you.</p><p>source: <a href="http://bytesizebio.net/2014/02/05/support-vector-machines-explained-well/" target="_blank" rel="noopener">http://bytesizebio.net/2014/02/05/support-vector-machines-explained-well/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Support vector machines (SVM)&lt;/p&gt;
&lt;p&gt;User &lt;a href=&quot;http://www.reddit.com/user/copperking&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;copperking&lt;/a&gt; s
      
    
    </summary>
    
      <category term="theory" scheme="http://yoursite.com/categories/theory/"/>
    
    
      <category term="machine_leanring" scheme="http://yoursite.com/tags/machine-leanring/"/>
    
      <category term="fun math" scheme="http://yoursite.com/tags/fun-math/"/>
    
  </entry>
  
  <entry>
    <title>new-post</title>
    <link href="http://yoursite.com/2018/02/27/new-post/"/>
    <id>http://yoursite.com/2018/02/27/new-post/</id>
    <published>2018-02-27T06:29:07.000Z</published>
    <updated>2018-03-13T14:58:29.904Z</updated>
    
    <content type="html"><![CDATA[<h1 id="this-is-a-test-blog"><a href="#this-is-a-test-blog" class="headerlink" title="this is a test blog."></a>this is a test blog.</h1><p>welcome to treamy’s world.</p><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><h3 id="add-some-code"><a href="#add-some-code" class="headerlink" title="add some code"></a>add some code</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(<span class="string">"My New Post"</span>)</span><br></pre></td></tr></table></figure><h3 id="标签页面"><a href="#标签页面" class="headerlink" title="标签页面"></a>标签页面</h3><p>1&gt;运行以下命令<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new page <span class="string">"tags"</span></span><br></pre></td></tr></table></figure></p><p>同时，在/source目录下会生成一个tags文件夹，里面包含一个index.md文件</p><h3 id="推送到服务器上"><a href="#推送到服务器上" class="headerlink" title="推送到服务器上"></a>推送到服务器上</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo g -d</span><br></pre></td></tr></table></figure><p>先generate一下生成静态页面，再deploy部署到服务器。<br>好像<code>hexo d -g</code>也可以。。一次记错了写成这个也行。。我也不知道为啥在一个网页上看到说他们左右是相同的，还是用前面那个较好解读的吧。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;this-is-a-test-blog&quot;&gt;&lt;a href=&quot;#this-is-a-test-blog&quot; class=&quot;headerlink&quot; title=&quot;this is a test blog.&quot;&gt;&lt;/a&gt;this is a test blog.&lt;/h1&gt;&lt;p&gt;
      
    
    </summary>
    
    
      <category term="test" scheme="http://yoursite.com/tags/test/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://yoursite.com/2018/02/27/hello-world/"/>
    <id>http://yoursite.com/2018/02/27/hello-world/</id>
    <published>2018-02-27T06:23:46.596Z</published>
    <updated>2018-02-27T06:23:46.596Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
  </entry>
  
</feed>
