<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">

<script>
    (function(){
        if(''){
            if (prompt('请输入文章密码') !== ''){
                alert('密码错误！');
                history.back();
            }
        }
    })();
</script>








<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="machine_leanring,python," />





  <link rel="alternate" href="/atom.xml" title="Treamy's website" type="application/atom+xml" />






<meta name="description" content="前言  本文主要讲述一下决策树的基本算法–ID3生成决策树算法。一开始看例子的时候，我觉得决策树好简单呀，应该实现起来用pandas也能像实现朴素贝叶斯一样容易实现，可是到实践的时候才发现，这个实现起来也好难啊orz。刚开始尝试直接通过算gini指数用CART算法生成树，但发现当两个的gini指数相同时我的程序就没法择优选择了。。。这个还有待改进。。最后参考了一下机器学习实战这本书把id3生成和可">
<meta name="keywords" content="machine_leanring,python">
<meta property="og:type" content="article">
<meta property="og:title" content="my decision tree id3">
<meta property="og:url" content="http://yoursite.com/2018/03/27/my-decision-tree-id3/index.html">
<meta property="og:site_name" content="Treamy&#39;s website">
<meta property="og:description" content="前言  本文主要讲述一下决策树的基本算法–ID3生成决策树算法。一开始看例子的时候，我觉得决策树好简单呀，应该实现起来用pandas也能像实现朴素贝叶斯一样容易实现，可是到实践的时候才发现，这个实现起来也好难啊orz。刚开始尝试直接通过算gini指数用CART算法生成树，但发现当两个的gini指数相同时我的程序就没法择优选择了。。。这个还有待改进。。最后参考了一下机器学习实战这本书把id3生成和可">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/images/tree1.png">
<meta property="og:image" content="http://yoursite.com/images/treeid3.jpg">
<meta property="og:image" content="http://yoursite.com/images/tree2.png">
<meta property="og:image" content="http://yoursite.com/images/tree3.png">
<meta property="og:updated_time" content="2018-04-25T11:20:45.726Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="my decision tree id3">
<meta name="twitter:description" content="前言  本文主要讲述一下决策树的基本算法–ID3生成决策树算法。一开始看例子的时候，我觉得决策树好简单呀，应该实现起来用pandas也能像实现朴素贝叶斯一样容易实现，可是到实践的时候才发现，这个实现起来也好难啊orz。刚开始尝试直接通过算gini指数用CART算法生成树，但发现当两个的gini指数相同时我的程序就没法择优选择了。。。这个还有待改进。。最后参考了一下机器学习实战这本书把id3生成和可">
<meta name="twitter:image" content="http://yoursite.com/images/tree1.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/03/27/my-decision-tree-id3/"/>





  <title>my decision tree id3 | Treamy's website</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>
    <a href="https://github.com/Interesting6"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://camo.githubusercontent.com/38ef81f8aca64bb9a64448d0d70f1308ef5341ab/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f72696768745f6461726b626c75655f3132313632312e706e67" alt="Fork me on GitHub" data-canonical-src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png"></a>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Treamy's website</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-schedule">
          <a href="/schedule/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-calendar"></i> <br />
            
            日程表
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/03/27/my-decision-tree-id3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Treamy">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Treamy's website">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">my decision tree id3</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-03-27T10:41:18+08:00">
                2018-03-27
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-leanring/" itemprop="url" rel="index">
                    <span itemprop="name">machine_leanring</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-leanring/code/" itemprop="url" rel="index">
                    <span itemprop="name">code</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>  本文主要讲述一下决策树的基本算法–ID3生成决策树算法。一开始看例子的时候，我觉得决策树好简单呀，应该实现起来用<code>pandas</code>也能像实现朴素贝叶斯一样容易实现，可是到实践的时候才发现，这个实现起来也好难啊orz。刚开始尝试直接通过算gini指数用<code>CART</code>算法生成树，但发现当两个的gini指数相同时我的程序就没法择优选择了。。。这个还有待改进。。最后参考了一下机器学习实战这本书把id3生成和可视化决策树实现了（不得不说这本书可视化部分写得我都看不懂了。。。）。</p>
<h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点和有向边组成。结点有两种类型：内部节点和叶节点，内部节点表示一个特征或属性，叶节点表示一个类。 </p>
<h3 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h3><p>分类决策树的核心思想就是在一个数据集中找到一个最优特征，然后从这个特征的选值中找一个最优候选值(这段话稍后解释)，根据这个最优候选值将数据集分为两个子数据集，然后递归上述操作，直到满足指定条件为止。</p>
<h3 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h3><h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><blockquote>
<p>1：理解和解释起来简单，且决策树模型可以想象<br>2：需要准备的数据量不大，而其他的技术往往需要很大的数据集，需要创建虚拟变量，去除不完整的数据，但是该算法对于丢失的数据不能进行准确的预测<br>3：决策树算法的时间复杂度(即预测数据)是用于训练决策树的数据点的对数<br>4：能够处理数字和数据的类别（需要做相应的转变），而其他算法分析的数据集往往是只有一种类型的变量<br>5：能够处理多输出的问题<br>6：使用白盒模型，如果给定的情况是在一个模型中观察到的，该条件的解释很容易解释的布尔逻辑，相比之下，在一个黑盒子模型（例如人工神经网络），结果可能更难以解释<br>7：可能使用统计检验来验证模型，这是为了验证模型的可靠性<br>8：从数据结果来看，它执行的效果很好，虽然它的假设有点违反真实模型</p>
</blockquote>
<h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><blockquote>
<p>1：决策树算法学习者可以创建复杂的树，但是没有推广依据，这就是所谓的过拟合，为了避免这种问题，出现了剪枝的概念，即设置一个叶子结点所需要的最小数目或者设置树的最大深度<br>2：决策树的结果可能是不稳定的，因为在数据中一个很小的变化可能导致生成一个完全不同的树，这个问题可以通过使用集成决策树来解决<br>3：众所周知，学习一恶搞最优决策树的问题是NP——得到几方面完全的优越性，甚至是一些简单的概念。因此，实际决策树学习算法是基于启发式算法，如贪婪算法，寻求在每个节点上的局部最优决策。这样的算法不能保证返回全局最优决策树。这可以减轻训练多棵树的合奏学习者，在那里的功能和样本随机抽样更换。<br>4：这里有一些概念是很难的理解的，因为决策树本身并不难很轻易的表达它们，比如说异或校验或复用的问题。<br>5：决策树学习者很可能在某些类占主导地位时创建有有偏异的树，因此建议用平衡的数据训练决策树<br>–当然最重要的还是容易<strong>过拟合</strong>！，所以迫切需要剪纸或者集成学习。</p>
</blockquote>
<h3 id="举个栗子"><a href="#举个栗子" class="headerlink" title="举个栗子"></a>举个栗子</h3><p>各位立志于脱单的单身男女在找对象的时候就已经完完全全使用了决策树的思想。假设一位母亲在给女儿介绍对象时，有这么一段对话：</p>
<blockquote>
<p>母亲：给你介绍个对象。<br>女儿：年纪多大了？<br>母亲：26。<br>女儿：长的帅不帅？<br>母亲：挺帅的。<br>女儿：收入高不？<br>母亲：不算很高，中等情况。<br>女儿：是公务员不？<br>母亲：是，在税务局上班呢。<br>女儿：那好，我去见见。</p>
</blockquote>
<p>这个女生的决策过程就是典型的分类决策树。相当于对年龄、外貌、收入和是否公务员等特征将男人分为两个类别：见或者不见。假设这个女生的决策逻辑如下：<br><img src="/images/tree1.png" alt="image"><br>上图完整表达了这个女孩决定是否见一个约会对象的策略，其中绿色结点（内部结点）表示判断条件，橙色结点（叶结点）表示决策结果，箭头表示在一个判断条件在不同情况下的决策路径，图中红色箭头表示了上面例子中女孩的决策过程。</p>
<p>这幅图基本可以算是一棵决策树，说它“基本可以算”是因为图中的判定条件没有量化，如收入高中低等等，还不能算是严格意义上的决策树，如果将所有条件量化，则就变成真正的决策树了。（以上的决策树模型纯属瞎编乱造，旨在直观理解决策树，不代表任何女生的择偶观，各位女同志无须在此挑刺。。。）</p>
<h3 id="决策树的学习"><a href="#决策树的学习" class="headerlink" title="决策树的学习"></a>决策树的学习</h3><p>决策树学习算法包含特征选择、决策树的生成与剪枝过程。决策树的学习算法通常是递归地选择最优特征，并用最优特征对数据集进行分割。开始时，构建根结点，选择最优特征，该特征有几种值就分割为几个子集，每个子集分别递归调用此方法，返回结点，返回的结点就是上一层的子结点。直到所有特征都已经用完，或者数据集只有一维特征为止。（这里就不介绍关于决策树的剪枝过程，日后再介绍）</p>
<h4 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h4><p>特征选择问题希望选取对训练数据具有良好分类能力的特征，这样可以提高决策树学习的效率。如果利用一个特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征是没有分类能力的（对象是否喜欢打游戏应该不会成为关键特征吧，也许也会……）。为了解决特征选择问题，找出最优特征，先要介绍一些信息论里面的概念。 </p>
<ol>
<li><p>熵（entropy）<br>熵是表示随机变量不确定性的度量。设$X$是一个取有限个值的离散随机变量，其概率分布为$$P(X=x_i)=p_i, i=1,2,…,n$$<br>则随机变量的熵定义为$$entropy(X) = -\sum_{i=1}^n P_ilog_2 P_i$$<br>另外，$0log0=0$，当对数的底为2时，熵的单位为bit；为e时，单位为nat。<br><strong>熵越大</strong>，随机变量的<strong>不确定性就越大</strong>。<br>从定义可验证$0&lt;=H(p)&lt;=logn$.<br>python实现计算如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">calc_entropy = <span class="keyword">lambda</span> P_: sum(map(<span class="keyword">lambda</span> p: -p * np.log2(p), P_))</span><br><span class="line"><span class="comment"># 其中P_为X的概率分布，p为X取某个随机变量的概率</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>条件熵（conditional entropy）<br>设有随机变量$(X,Y)$，其联合概率分布为$$P(X=x_i,Y=y_i)=p_{ij}, i=1,2,…,n;j=1,2,…,m$$条件熵$H(Y|X)$表示在<strong>已知随机变量X的条件下</strong>随机变量Y的不确定性。随机变量X给定的条件下随机变量Y的条件熵$H(Y|X)$，定义为X给定条件下Y的条件概率分布的熵对X的数学期望$$entropy(Y|X) = \sum_{i=1}^k p_i H(Y|X=x_i)$$这里$p_i=P(X=x_i), i=1,2,…,n$。<br>用python实现求条件熵时，只需要把P_更换为feat_value_cate_P再调用calc_entropy，然后把所有分组的概率与对应的熵相乘再相加：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># group为Y对于这个特征X取不同的值的分组</span></span><br><span class="line"><span class="keyword">for</span> feat_value, group <span class="keyword">in</span> groups:</span><br><span class="line">    feat_value_P = len(group) / len(df)  <span class="comment"># 特征X取某值的概率</span></span><br><span class="line">    feat_value_cate_P = group[cate].value_counts() / group[cate].count()  <span class="comment"># 特征X取某值对应不同的类别的概率</span></span><br><span class="line">    feat_value_entropy += feat_value_P * calc_entropy(feat_value_cate_P)</span><br></pre></td></tr></table></figure>
</li>
<li><p>信息增益（information gain）<br>信息增益表示<strong>得知特征X的信息而使得类Y的信息的不确定性减少的程度</strong>。特征A对训练数据集D的信息增益$g(D,A)$，定义为集合D的<code>经验熵H(D)</code>与特征A给定条件下D的<code>经验条件熵H(D|A)</code>之<strong>差</strong>，即$$g(D,A)=H(D)−H(D|A)$$<br>这个差又称为互信息，表示由于特征A而使得对数据集D的分类不确定性减少的程度。信息增益大的特征具有更强的分类能力。<br>设训练数据为$D$，$|D|$表示其样本容量，即样本个数。设$D$有$K$个类$C_k$，$k=1,2,…,K$，$|C_k|$为属于类$C_k$的样本个数，$\sum_{k=1}^K |C_k|=|D|$. 设特征A有n个不同的取值${a_1,a_2,…a_n}$, 根据特征A 的取值将D划分为n个子集$D_1,D_2,…,D_n$, $|D_i|$为的样本$D_i$个数，$\sum_{i=1}^n |D_i|=|D|$。记子集$D_i$中属于类$C_k$的样本的集合为$D_{ik}$，即$D_{ik}=D_i\bigcap C_k$，$|D_{ik}|$为$D_{ik}$的样本个数。</p>
<blockquote>
<p>计算信息增益的算法如下： </p>
</blockquote>
</li>
</ol>
<ul>
<li>输入：训练数据集$D$和特征$A$；</li>
<li>输出：特征A对训练数据集$D$的信息增益$g(D,A)$.</li>
<li>计算数据集D的经验熵H(D)<br>$$H(D)=-\sum_{i=1}^k \frac{|C_k|}{|D|} log_2 \frac {|C_k|}{|D|}$$</li>
<li>计算特征A对数据集D的经验条件熵$H(D|A)$<br>$$H(D|A)=\sum_{i=1}^n \frac{|D_i|}{|D|}H(D_i)=\sum_{i=1}^n  \frac{|D_i|}{|D|}  \sum_{i=1}^K \frac{|D_{ik}|}{|D|} log_2 \frac{|D_{ik}|}{|D|}$$</li>
<li>计算信息增益$$g(D,A)=H(D)−H(D|A)$$</li>
</ul>
<h4 id="决策树的生成"><a href="#决策树的生成" class="headerlink" title="决策树的生成"></a>决策树的生成</h4><p>本次我们只介绍ID3算法，ID3算法由Ross Quinlan发明，建立在“奥卡姆剃刀”的基础上：越是小型的决策树越优于大的决策树（be simple简单理论）。ID3算法中根据信息增益评估和选择特征，每次选择信息增益最大的特征作为判断模块建立子结点。ID3算法可用于划分标称型数据集，没有剪枝的过程，为了去除过度数据匹配的问题，可通过裁剪合并相邻的无法产生大量信息增益的叶子节点（例如设置信息增益阀值）。使用信息增益的话其实是有一个缺点，那就是它偏向于具有大量值的属性。就是说在训练集中，某个属性所取的不同值的个数越多，那么越有可能拿它来作为分裂属性，而这样做有时候是没有意义的，另外ID3不能处理连续分布的数据特征，于是就有了C4.5算法。CART算法也支持连续分布的数据特征。<br><img src="/images/treeid3.jpg" alt="image"><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select</span><span class="params">(df, features, cate , H_D)</span>:</span></span><br><span class="line">    gain_dict = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> feat <span class="keyword">in</span> features:</span><br><span class="line">        groups = df.groupby(feat)</span><br><span class="line">        feat_value_entropy = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> feat_value, group <span class="keyword">in</span> groups:</span><br><span class="line">            feat_value_P = len(group) / len(df)  <span class="comment"># 特征取某值的概率</span></span><br><span class="line">            feat_value_cate_P = group[cate].value_counts() / group[cate].count()  <span class="comment"># 特征取某值对应不同的类别的概率</span></span><br><span class="line">            feat_value_entropy += feat_value_P * calc_entropy(feat_value_cate_P)</span><br><span class="line">        imfor_gain = H_D - feat_value_entropy</span><br><span class="line">        gain_dict[feat] = imfor_gain</span><br><span class="line">    <span class="keyword">return</span> max(gain_dict, key=<span class="keyword">lambda</span> x:gain_dict[x])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_tree</span><span class="params">(df, cate, H_D )</span>:</span></span><br><span class="line">    feat = df.columns[:<span class="number">-1</span>].tolist()</span><br><span class="line">    cate_values = df[cate].unique()</span><br><span class="line">    <span class="keyword">if</span> len(cate_values)==<span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> cate_values[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">if</span> len(feat) == <span class="number">0</span>: <span class="comment"># 用完所有特征后</span></span><br><span class="line">        temp = df[cate].value_counts().to_dict()</span><br><span class="line">        <span class="keyword">return</span> max(temp, key=<span class="keyword">lambda</span> x:temp[x]) <span class="comment"># 取最多的类别作为返回值</span></span><br><span class="line">    best_feat = select(df, feat, cate, H_D)</span><br><span class="line">    my_tree = &#123; best_feat:&#123;&#125; &#125;</span><br><span class="line">    unique_feat_values = df[best_feat].unique()</span><br><span class="line">    <span class="keyword">for</span> feat_value <span class="keyword">in</span> unique_feat_values:</span><br><span class="line">        df_ = df[df[best_feat] == feat_value].copy()</span><br><span class="line">        df_ =  df_.drop(best_feat, axis=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 这里一定不能是df，必须是一个新的df_，才能使递归*中的feat*越来越小</span></span><br><span class="line">        my_tree[best_feat][feat_value] = create_tree(df_, cate, H_D )</span><br><span class="line">    <span class="keyword">return</span> my_tree</span><br></pre></td></tr></table></figure></p>
<p>我们这里用Python语言的字典套字典类型存储树的信息，简单方便。当然也可以定义一个新的数据结构存储树。<br>来生成一个树：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">df = pd.read_excel(<span class="string">"TreeData.xlsx"</span>, index_col=<span class="string">"id"</span>)</span><br><span class="line">cate = df.columns[<span class="number">-1</span>]</span><br><span class="line">P_cate = df[cate].value_counts() / df[cate].count()</span><br><span class="line">H_D = calc_entropy(P_cate)</span><br><span class="line">tree = create_tree(df,cate, H_D)</span><br><span class="line">print(tree)</span><br><span class="line"><span class="comment"># &#123;'有自己的房子': &#123;'否': &#123;'有工作': &#123;'否': '否', '是': '是'&#125;&#125;, '是': '是'&#125;&#125;</span></span><br></pre></td></tr></table></figure></p>
<h4 id="决策树的可视化"><a href="#决策树的可视化" class="headerlink" title="决策树的可视化"></a>决策树的可视化</h4><p>我们主要用python的matplotlib来处理图像，它的annotate很方便用于注释。（以下代码来源：机器学习实战，我对其简单的更改了一下）<br>先获得叶子节点个数和树的深度：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_leafs_num</span><span class="params">(tree_)</span>:</span></span><br><span class="line">    num_leafs = <span class="number">0</span></span><br><span class="line">    first_key = list(tree_.keys())[<span class="number">0</span>]</span><br><span class="line">    second_dict = tree_[first_key]</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> second_dict.keys():</span><br><span class="line">        <span class="keyword">if</span> type(second_dict[key]).__name__ == <span class="string">"dict"</span>:</span><br><span class="line">            num_leafs += get_leafs_num(second_dict[key])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            num_leafs += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> num_leafs</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_tree_depth</span><span class="params">(tree_)</span>:</span></span><br><span class="line">    max_depth = <span class="number">0</span></span><br><span class="line">    first_key = list(tree_.keys())[<span class="number">0</span>]</span><br><span class="line">    second_dict = tree_[first_key]</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> second_dict.keys():</span><br><span class="line">        <span class="keyword">if</span> type(second_dict[key]).__name__ == <span class="string">"dict"</span>: <span class="comment"># 如果还是字典，继续深入</span></span><br><span class="line">            this_depth = <span class="number">1</span> + get_tree_depth(second_dict[key])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            this_depth = <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> this_depth &gt; max_depth:</span><br><span class="line">            max_depth = this_depth</span><br><span class="line">    <span class="keyword">return</span> max_depth</span><br></pre></td></tr></table></figure></p>
<p>然后再画图：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.rcParams[<span class="string">'font.sans-serif'</span>]=[<span class="string">'SimHei'</span>] </span><br><span class="line">plt.rcParams[<span class="string">'axes.unicode_minus'</span>]=<span class="keyword">False</span> </span><br><span class="line"></span><br><span class="line">decision_node=&#123;<span class="string">"boxstyle"</span>: <span class="string">"sawtooth"</span>, <span class="string">"fc"</span>: <span class="string">"0.8"</span>, &#125;</span><br><span class="line">leaf_node=&#123;<span class="string">"boxstyle"</span>: <span class="string">"round4"</span>, <span class="string">"fc"</span>: <span class="string">"0.8"</span>&#125;</span><br><span class="line">arrow_args=&#123;<span class="string">"arrowstyle"</span>: <span class="string">"&lt;-"</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_node</span><span class="params">(node_txt, centerPt, parentPt, node_type)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> ax1</span><br><span class="line">    ax1.annotate(node_txt, xy=parentPt, xycoords=<span class="string">'axes fraction'</span>,xytext=centerPt,</span><br><span class="line">        textcoords=<span class="string">'axes fraction'</span>,va=<span class="string">"center"</span>, ha=<span class="string">"center"</span>, bbox=node_type, arrowprops=arrow_args)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_mid_text</span><span class="params">(cntrPt, parentPt, txt_string)</span>:</span>  <span class="comment"># 在两个节点之间的线上写上字</span></span><br><span class="line">    <span class="keyword">global</span> ax1</span><br><span class="line">    xMid = (parentPt[<span class="number">0</span>] - cntrPt[<span class="number">0</span>]) / <span class="number">2.0</span> + cntrPt[<span class="number">0</span>]</span><br><span class="line">    yMid = (parentPt[<span class="number">1</span>] - cntrPt[<span class="number">1</span>]) / <span class="number">2.0</span> + cntrPt[<span class="number">1</span>]</span><br><span class="line">    ax1.text(xMid, yMid, txt_string)  <span class="comment"># text() 的使用</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_tree</span><span class="params">( tree_, parent_point, node_txt)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> ax1,xOff,yOff,totalD,totalW</span><br><span class="line">    num_leafs = get_leafs_num(tree_)</span><br><span class="line">    depth = get_tree_depth(tree_)</span><br><span class="line">    first_key = list(tree_.keys())[<span class="number">0</span>]</span><br><span class="line">    center_point = (xOff + (<span class="number">1.0</span> + float(num_leafs)) / <span class="number">2.0</span> / totalW, yOff)</span><br><span class="line">    plot_mid_text( center_point, parent_point, node_txt)  <span class="comment"># 在父子节点间填充文本信息</span></span><br><span class="line">    plot_node(first_key, center_point, parent_point, decision_node)  <span class="comment"># 绘制带箭头的注解</span></span><br><span class="line">    second_dict = tree_[first_key]</span><br><span class="line">    yOff = yOff - <span class="number">1.0</span> / totalD</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> second_dict.keys():</span><br><span class="line">        <span class="keyword">if</span> type(second_dict[key]).__name__ == <span class="string">'dict'</span>:  <span class="comment"># 判断是不是字典，</span></span><br><span class="line">            plot_tree(second_dict[key], center_point, str(key))  <span class="comment"># 递归绘制树形图</span></span><br><span class="line">        <span class="keyword">else</span>:  <span class="comment"># 如果是叶节点</span></span><br><span class="line">            xOff = xOff + <span class="number">1.0</span> / totalW</span><br><span class="line">            plot_node(second_dict[key], (xOff, yOff), center_point, leaf_node)</span><br><span class="line">            plot_mid_text((xOff, yOff), center_point, str(key))</span><br><span class="line">    yOff = yOff + <span class="number">1.0</span> / totalD</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_tree</span><span class="params">(tree_)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> ax1,xOff,yOff,totalD,totalW</span><br><span class="line">    fig = plt.figure(<span class="number">1</span>, facecolor=<span class="string">'white'</span>)</span><br><span class="line">    fig.clf()  <span class="comment"># 清空绘图区</span></span><br><span class="line">    axprops = dict(xticks=[], yticks=[])</span><br><span class="line">    ax1 = plt.subplot(<span class="number">111</span>, frameon=<span class="keyword">False</span>, **axprops)</span><br><span class="line">    totalW = float(get_leafs_num(tree_))</span><br><span class="line">    totalD = float(get_tree_depth(tree_))</span><br><span class="line">    xOff = <span class="number">-0.5</span> / totalW  <span class="comment"># 追踪已经绘制的节点位置 初始值为 将总宽度平分 在取第一个的一半</span></span><br><span class="line">    yOff = <span class="number">1.0</span></span><br><span class="line">    plot_tree(tree_, (<span class="number">0.5</span>, <span class="number">1.0</span>), <span class="string">''</span>)  <span class="comment"># 调用函数，并指出根节点源坐标</span></span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure></p>
<p>下面用一个实例来可视化一下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tree = &#123;<span class="string">'有自己的房子'</span>: &#123;<span class="string">'否'</span>: &#123;<span class="string">'有工作'</span>: &#123;<span class="string">'否'</span>: <span class="string">'否'</span>, <span class="string">'是'</span>: <span class="string">'是'</span>&#125;&#125;, <span class="string">'是'</span>: <span class="string">'是'</span>&#125;&#125;</span><br><span class="line"></span><br><span class="line">print(tree)</span><br><span class="line">show_tree(tree)</span><br></pre></td></tr></table></figure></p>
<p>可视化结果如下：<br><img src="/images/tree2.png" alt="image"></p>
<blockquote>
<ul>
<li>由于篇幅过长，完整代码（结构化封装）就不在这里给出，详情参见我的<br><a href="https://github.com/Interesting6/my_machine_learning/blob/master/my_decision_tree_id3.py" target="_blank" rel="noopener">GitHub</a>。</li>
</ul>
</blockquote>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>这里我们采用uci的lense<a href="http://archive.ics.uci.edu/ml/datasets/Lenses" target="_blank" rel="noopener">隐形眼镜测试集</a>,总样本为24个，四个特征，三个类别。我们通过网络爬虫，直接从该网址抓取数据，并转换为dataframe类型。</p>
<blockquote>
<p>– 3 Classes:<br>     1 : the patient should be fitted with hard contact lenses,<br>     2 : the patient should be fitted with soft contact lenses,<br>     3 : the patient should not be fitted with contact lenses.</p>
</blockquote>
<blockquote>
<p>– 4 Features:</p>
<pre><code>1. age of the patient: (1) young, (2) pre-presbyopic, (3) presbyopic
2. spectacle prescription:  (1) myope, (2) hypermetrope
3. astigmatic:     (1) no, (2) yes
4. tear production rate:  (1) reduced, (2) normal
</code></pre></blockquote>
<p>下面我们给出代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ID3 <span class="keyword">import</span> ID3_tree</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data</span><span class="params">()</span>:</span></span><br><span class="line">    url = <span class="string">"http://archive.ics.uci.edu/ml/machine-learning-databases/lenses/lenses.data"</span></span><br><span class="line">    data = requests.get(url).text</span><br><span class="line">    verctors = data.split(<span class="string">'\n'</span>)</span><br><span class="line">    verctors = [ver.split() <span class="keyword">for</span> ver <span class="keyword">in</span> verctors]</span><br><span class="line">    features = [<span class="string">"id"</span>,<span class="string">"age"</span>,<span class="string">"prescript"</span>,<span class="string">"astigmatic"</span>,<span class="string">"tearRate"</span>,<span class="string">"category"</span>]</span><br><span class="line">    df = pd.DataFrame(verctors,columns=features, dtype=int)</span><br><span class="line">    df = df.set_index(<span class="string">"id"</span>).dropna()</span><br><span class="line">    key_list = [&#123;<span class="string">"1"</span>:<span class="string">"young"</span>, <span class="string">"2"</span>:<span class="string">"pre-presbyopic"</span>, <span class="string">"3"</span>:<span class="string">"presbyopic"</span>&#125;, &#123;<span class="string">"1"</span>: <span class="string">"myope"</span>, <span class="string">"2"</span>: <span class="string">"hypermetrope"</span>&#125;</span><br><span class="line">    , &#123;<span class="string">"1"</span>: <span class="string">"no"</span>, <span class="string">"2"</span>: <span class="string">"yes"</span>&#125;, &#123;<span class="string">"1"</span>: <span class="string">"reduced"</span>, <span class="string">"2"</span>:<span class="string">"normal"</span>&#125;,&#123;<span class="string">"1"</span>:<span class="string">"hard"</span>,<span class="string">"2"</span>:<span class="string">"soft"</span>,<span class="string">"3"</span>:<span class="string">"no lenses"</span>&#125;]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        df.iloc[:,i] = df.iloc[:,i].apply(<span class="keyword">lambda</span> x:key_list[i][x])</span><br><span class="line">    <span class="keyword">return</span> df</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    df = get_data()</span><br><span class="line">    <span class="comment"># print(df)</span></span><br><span class="line">    id3_tree = ID3_tree(df)</span><br><span class="line">    id3_tree = id3_tree.train(df)</span><br><span class="line">    my_tree = id3_tree.my_tree</span><br><span class="line">    print(my_tree)</span><br><span class="line">    id3_tree.show_tree(my_tree)</span><br></pre></td></tr></table></figure></p>
<p>得出的决策树如下：<br><img src="/images/tree3.png" alt="image"></p>
<h2 id="后续"><a href="#后续" class="headerlink" title="后续"></a>后续</h2><p>由于本文只给出了ID3算法生成决策树和决策树的可视化，日后我将继续给出C4.5的生成决策树算法、决策树的减枝问题与及CART分类和回归树的构造。然后我们还可以把它拓宽，引入集成学习的随机森林。</p>
<p>作者时间精力有限，我就先写到这里啦。如有疑问，记得联系我哦。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><blockquote>
<p>《统计学习方法》李航 著  清华大学出版社<br>《机器学习实战》Peter Harrington 著 人民邮电出版社</p>
</blockquote>
<!-- ## 最后
如果你觉得本文对你有帮助的话，不如给作者一点打赏吧~
| ![image](/images/alipay.jpg) | ![image](/images/wechatpay.png) |
谢谢！ -->

      
    </div>
    
    
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>如果你觉得本文对你有帮助的话，不如给作者一点打赏吧~ 谢谢！</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>打赏</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechatpay.png" alt="Treamy 微信支付"/>
        <p>微信支付</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/images/alipay.jpg" alt="Treamy 支付宝"/>
        <p>支付宝</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    

    <div>
      
        

      
    </div>

    <div>
      
        <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">
            -------------本文结束
            <i class="fa fa-paw"></i>
            感谢您的阅读-------------
        </div>
    
</div>

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/machine-leanring/" rel="tag"><i class="fa fa-tag"></i> machine_leanring</a>
          
            <a href="/tags/python/" rel="tag"><i class="fa fa-tag"></i> python</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/03/18/Radial-basis-function-kernel/" rel="next" title="Radial basis function kernel">
                <i class="fa fa-chevron-left"></i> Radial basis function kernel
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/04/02/梯度下降法/" rel="prev" title="梯度下降法">
                梯度下降法 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpg"
                alt="Treamy" />
            
              <p class="site-author-name" itemprop="name">Treamy</p>
              <p class="site-description motion-element" itemprop="description">Live and Learn</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">13</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">10</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">9</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#前言"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#正文"><span class="nav-number">2.</span> <span class="nav-text">正文</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#定义"><span class="nav-number">2.1.</span> <span class="nav-text">定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#核心思想"><span class="nav-number">2.2.</span> <span class="nav-text">核心思想</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#优缺点"><span class="nav-number">2.3.</span> <span class="nav-text">优缺点</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#优点"><span class="nav-number">2.3.1.</span> <span class="nav-text">优点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#缺点"><span class="nav-number">2.3.2.</span> <span class="nav-text">缺点</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#举个栗子"><span class="nav-number">2.4.</span> <span class="nav-text">举个栗子</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#决策树的学习"><span class="nav-number">2.5.</span> <span class="nav-text">决策树的学习</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#特征选择"><span class="nav-number">2.5.1.</span> <span class="nav-text">特征选择</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#决策树的生成"><span class="nav-number">2.5.2.</span> <span class="nav-text">决策树的生成</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#决策树的可视化"><span class="nav-number">2.5.3.</span> <span class="nav-text">决策树的可视化</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#测试"><span class="nav-number">2.6.</span> <span class="nav-text">测试</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#后续"><span class="nav-number">3.</span> <span class="nav-text">后续</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考文献"><span class="nav-number">4.</span> <span class="nav-text">参考文献</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Treamy</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>




<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<span class="post-meta-divider">|</span> &nbsp本站总访问量 <span id="busuanzi_value_site_pv"></span> &nbsp&nbsp&nbsp
您是第<span id="busuanzi_value_site_uv"></span>个来到的小伙伴

        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

  
  <script type="text/javascript" opacity='0.8' zIndex="-2" count="60" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js">
  </script>
  
</body>
</html>
<!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/love.js"></script>
