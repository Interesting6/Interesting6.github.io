<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Random Forest]]></title>
    <url>%2F2018%2F05%2F18%2FRandom-Forest%2F</url>
    <content type="text"><![CDATA[之前我们讲到了决策树，现在我们来讲一下随机森林，把这段知识补上。 首先什么是随机森林？随机森林就是用随机的方式建立一个森林，在森林里有很多决策树组成，并且每一棵决策树之间是没有关联的。当有一个新样本的时候，我们让森林的每一棵决策树分别进行判断，看看这个样本属于哪一类，然后用投票的方式，哪一类被选择的多，作为最终的分类结果。在回归问题中，随机森林输出所有决策树输出的平均值。 随机森林既可以用于分类，也可以用于回归。 它是一种降维手段，用于处理缺失值和异常值。 它是集成学习的重要方法。 两个随机抽取 样本有放回随机抽取固定数目 构建决策树时，特征随机抽取 解释：两个随机性的引入对随机森林的分类性能至关重要。由于它们的引入，使得随机森林不容易陷入过拟合，并且具有很好得抗噪能力（比如：对缺省值不敏感） 随机森林算法是如何工作的？在随机森林中，每一个决策树“种植”和“生长”的四个步骤： 假设我们设定训练集中的样本个数为N，然后通过有放回的有放回的随机选择N个样本(一次)，用这选择好了的N个样本用来训练一个决策树，作为决策树根节点处的样本； 当每个样本有M个属性时，在决策树的每个节点需要分裂时，随机从这M个属性中选取出m个属性，满足条件m &lt;&lt; M。然后从这m个属性中采用某种策略（比如说信息增益）来选择1个属性作为该节点的分裂属性。 每棵决策树都最大可能地进行生长而不进行剪枝； 通过对所有的决策树进行加总来预测新的数据（在分类时采用多数投票，在回归时采用平均）。 随机森林的优缺点优点： 由于每次不再考虑全部的属性，而是一个属性子集，所以相比于Bagging计算开销更小，训练效率更高 对高维数据的处理能力强，可以处理成千上万的输入变量，是一个非常不错的降维方法 能够输出特征的重要程度, 基于oob误分类率和基于Gini系数的变化 有效的处理缺省值 缺点： 在噪声较大的时候容易过拟合 重要参数随机森林分类效果（错误率）与两个因素有关： 森林中任意两棵树的相关性：相关性越大，错误率越大； 森林中每棵树的分类能力：每棵树的分类能力越强，整个森林的错误率越低。 减小特征选择个数m，树的相关性和分类能力也会相应的降低；增大m，两者也会随之增大。所以关键问题是如何选择最优的m（或者是范围），这也是随机森林唯一的一个参数。在学习如何选择参数前首先介绍oob的概念。 oob：袋外错误率为了选择最优的m，这里需要利用的是袋外错误率oob（out-of-bag error）。我们知道，在构建每个决策树的时候，采用的是随机有放回的抽取，所以对于每棵树来说，都有一些样本(约占1/3)实际上没有参与树的生成，所以这些样本成为袋外样本，即oob。与交叉验证类似，我们可以将这个决策树的obb样本作为这棵树的验证集，对oob集合进行估计的步骤入下： 对每个样本，计算它作为oob样本的树对它的分类情况 多数投票作为该样本的分类结果 用误分个数占样本总数的比率作为随机森林的oob误分率 oob误分率是随机森林泛化误差的一个无偏估计，它的结果近似于需要大量计算的k折交叉验证。所以没有必要对它进行交叉验证或者用一个独立的测试集来获得误差的一个无偏估计。它可以在内部进行评估，也就是说在生成的过程中就可以对误差建立一个无偏估计。 当我们知道了oob的计算方法，我们可以通过选取不同的m，计算oob_error，找出oob_error最小时对应的m的值。这和交叉验证的思想非常的相似。 RF特征重要性的度量方法 对于每一棵决策树，计算其oob_error_0 选取一个特征，随机对特征加入噪声干扰，再次计算oob error_1 特征的重要性=∑(oob_error_1-oob_error_0)/随机森林中决策树的个数 对随机森林中的特征变量按照特征重要性降序排序。 然后重复以上步骤，直到选出m个特征。 解释：用这个公式来度量特征重要性，原因是：给某个特征随机的加入噪声后，如果oob error增大，说明这个特征对样本分类的结果影响比较大，说明重要程度比较高。 RF特征选择首先特征选择的目标有两个： 1：找到与分类结果高度相关的特征变量。 2：选择出数目较少的特征变量并且能够充分的预测应变量的结果。 特征选择的步骤： （1）对于每一棵决策树，计算其oob_error （2）随机的修改OOB中的每个特征xi的值，计算oob_error_2，再次计算重要性 （3）按照特征的重要性排序，然后剔除后面不重要的特征 （4）然后重复以上步骤，直到选出m个特征。 几个问题###（1）为什么要随机抽取样本？ 答：如果不进行随机抽样，对于每个树的训练集都是相同的，训练出来的结果也是一样的，所以此时进行投票决策没有意义。 ###（2）为什么要有放回的去抽样呢? 答：如果不是有放回的抽样，那么每一棵树的训练样本是不同的，都是没有交集的，那么每棵树都是有偏的，都是片面的，树与树之间并不是完全公平的。我们需要的是，没颗决策树是公平的，然后让它们投票决策得出结果，并且这样可以防止过度拟合。 ###（3）这里指的有放回的抽样，是每次抽一个放回，还是一次抽n个再放回？ 答: 构造子数据集，子数据集的数据量是和原始数据集相同的。不同子数据集的元素可以重复，同一个子数据集中的元素也可以重复。 (4)随机森林为什么可以用于处理缺失值和异常值的数据？答：随机森林的容错率高，由于随机森林多棵树，如果一个数据缺失了的字段刚好有些树不需要这些特征，于是可以用该些树进行预测，从而得到结果。而决策树的话很有可能无法进行预测。 (5)随机森林为什么不容易过拟合，为什么对噪声不敏感？答：三个随机性的引入，即产生决策树的样本是随机生成，构建决策树的特征值是随机选取，树产生过程中裂变的时候是选择N个最佳方向中的随机一个裂变的。当随机森林产生的树的数目趋近无穷的时候，理论上根据大数定理可以证明训练误差与测试误差是收敛到一起的。 当然实际过程中，由于不可能产生无穷的决策树，模型参数的设置问题会影响在相同运行时间内拟合结果的过拟合程度的不同。但总而言之，调整参数后，随机森林可以有效的降低过拟合的程度。 另外多颗决策树综合决策，以多数为输出代表，也能在一定程度上减少误差。]]></content>
      <categories>
        <category>machine_leanring</category>
        <category>theory</category>
      </categories>
      <tags>
        <tag>machine_leanring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[高斯混合模型]]></title>
    <url>%2F2018%2F05%2F17%2F%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[混合高斯模型要解决的问题与一些假设观察到从K个类中产生的样本$X={x_1^T,x_2^T,…,x_M^T}^T\in R^{M\times N}$，M个样本，每个样本N维，但是每一个样本没有对应的标签标记该样本是属于哪一个类，如何确定这些样本最有可能的分布方式？这就是一个无监督学习问题。使用混合高斯模型解决该聚类问题时，假设每一个类内部的样本服从高斯分布，（并且类与类之间的互相独立？）。 基于以上假设如何对观察到的样本进行建模？信息不完全，需要引入隐变量来表示每个样本具体属于哪一个类。引入变量$z_{mk}$表示第m个样本是否属于第k类，若第m个样本属于第k类则$z_{mk}=1$，否则$z_{mk}=0$。显然，对于一个样本，其对应的k个$z$中只有一个为1，其他都为0，因为一个样本只能属于一个类。所以有$\sum_{k=1}^{K}z_{mk}=1$。 对于某一个样本$x$，其被观察到的概率为$p(x)$，则其可由$x$与$z$的联合分布中消去$z$得到，即$p(x)=\sum_{z}{p(x,z)} =\sum_{k=1}^{K}{p(x,z_{k}=1)}$。 由混合高斯模型的假设可知$p(x|z_{k}=1)=\mathcal{N}(x|\mu_k,\Sigma_k) $，即每个类的内部的样本服从高斯分布。 如果再假设先验：对于任意一个样本$x$，其属于第k类的概率为$p(z_k=1)=\pi_k$。可以理解为任意取一个样本$x$，忽略其本身的特征，其属于第k类的概率为$\pi_k$。也可看做第k类样本在总体样本中所占的比例，所以有$\sum_{k=1}^K\pi_k=1$。 则根据以上的假设可以得到$x$与$z$的联合分布为 $p(x,z_k=1)=p(x|z_k=1)p(z_k=1)=\pi_k \mathcal{N}(x|\mu_k,\Sigma_k)$。 所以$p(x)=\sum_{z}{p(x,z)} =\sum_{k=1}^{K}{p(x,z_{k}=1)}=\sum_{k=1}^K\pi_k \mathcal{N}(x|\mu_k,\Sigma_k)$。 至此，我们已经对样本建模完成，若果我们知道隐变量$z$的值的话，这个问题就直接是一个分类问题，以上就是一个高斯判别式模型GDA，直接求出$\mu_k$与$\Sigma_k$就可以得到我们需要的总体样本的分布$p(x)$。 对于一个新样本需要预测其所属的类别，可以通过贝叶斯公式得到：观察到一个样本$x$，其属于第k类的概率为$$p(z_k=1|x)=\frac{p(x,z_k=1)}{p(x)} =\frac{\pi_k\mathcal{N}(x|\mu_k,\Sigma_k)}{\sum_{k=1}^K\pi_k\mathcal{N}(x|\mu_k,\Sigma_k)}$$为方便将$p(z_k=1|x)$记做$\gamma(z_k)$。 但是我们不知道z的值，无法将每一个样本$x_m$对应到相应的$z_k$上去（即不知道哪个$z_{mk}=1$），所以无法计算出$\pi_k$，也无法根据似然函数去直接计算$\mu_k$与$\Sigma_k$。 直观理解如图1，图中的点在我们看来明显分成两个聚类。这两个聚类中的点分别通过两个不同的正态分布随机生成而来。但是如果没有GMM，那么只能用一个的二维高斯分布来描述图1中的数据。图1中的椭圆即为二倍标准差的正态分布椭圆。这显然不太合理，毕竟肉眼一看就觉得应该把它们分成两类。这时候就可以使用GMM了！如图2，数据在平面上的空间分布和图1一样，这时使用两个二维高斯分布来描述图2中的数据，分别记为$\mathcal{N}(\mu_1,\Sigma_1)$和$\mathcal{N}(\mu_2,\Sigma_2)$. 图中的两个椭圆分别是这两个高斯分布的二倍标准差椭圆。可以看到使用两个二维高斯分布来描述图中的数据显然更合理。实际上图中的两个聚类的中的点是通过两个不同的正态分布随机生成而来。如果将两个二维高斯分布$\mathcal{N}(\mu_1,\Sigma_1)$和$\mathcal{N}(\mu_2,\Sigma_2)$合成一个二维的分布，那么就可以用合成后的分布来描述图2中的所有点。最直观的方法就是对这两个二维高斯分布做线性组合，用线性组合后的分布来描述整个集合中的数据。这就是高斯混合模型（GMM）。 为什么无法根据似然函数计算$\mu_k$与$\Sigma_k$？写出似然函数：$$p(X|\pi,\mu,\Sigma)=\prod_{m=1}^{M}p(x_m)= \prod_{m=1}^{M}(\sum_{k=1}^{K}\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k))$$ 取对数：$$\theta(\pi,\mu,\Sigma;X)=ln\ p(X|\pi,\mu,\Sigma)=ln\prod_{m=1}^{M}p(x_m)= ln\prod_{m=1}^{M}(\sum_{k=1}^{K}\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k))=\sum_{m=1}^Mln(\sum_{k=1}^{K}\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k))$$ 缺少隐变量$z$的信息会导致： $\pi_k$无法计算; $ln$中的求和符号无法消除（在GDA的似然中样本$x_m$不属于的类根本就不会出现在$ln$中）。 对$\mu_k$求偏导$$\frac{\partial ln\ p(X|\pi,\mu,\Sigma)}{\partial \mu_k}=\frac{\partial \sum_{m=1}^Mln(\sum_{k=1}^{K}\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k))}{\partial \mu_k}$$$$=\sum_{m=1}^M\frac{\partial ln(\sum_{k=1}^{K}\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k))}{\partial \mu_k}=\sum_{m=1}^M\frac{\pi_k}{\sum_{k=1}^{K}\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k)}\frac{\partial \mathcal{N}(x_m|\mu_k,\Sigma_k)}{\partial \mu_k}$$$$=\sum_{m=1}^M\frac{\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k)}{\sum_{k=1}^{K}\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k)}\frac{\partial (-\frac{1}{2}(x_m-\mu_k)^T\Sigma_k^{-1}(x_m-\mu_k))}{\partial \mu_k}$$$$=\sum_{m=1}^M\frac{\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k)}{\sum_{k=1}^{K}\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k)}(-\frac{1}{2})\Sigma_k(x_m-\mu_k) $$ 令$\frac{\partial ln\ p(X|\pi,\mu,\Sigma)}{\partial \mu_k}=0$得$$-\sum_{m=1}^M\frac{\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k)}{\sum_{k=1}^{K}\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k)}\Sigma_k(x_m-\mu_k)=0$$ 假设$\Sigma_k$可逆，则两边同乘以$\Sigma_k^{-1}$可消去$\Sigma_k$，则可得$$\mu_k\sum_{m=1}^M\frac{\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k)}{\sum_{k=1}^{K}\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k)}=\sum_{m=1}^M\frac{\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k)}{\sum_{k=1}^{K}\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k)}x_m$$ 所以$$\mu_k=\frac{\sum_{m=1}^M\gamma(z_{mk})x_m}{\sum_{m=1}^M\gamma(z_{mk})}$$ 观察该公式，每个类的类中心相当于所有样本的加权平均数，对于第k类，每个样本的权重$w_{km}=\frac{\gamma(z_{mk})}{\sum_{m=1}^M\gamma(z_{mk})}$。 对$\Sigma_k$求偏导$$\frac{\partial ln\ p(X|\pi,\mu,\Sigma)}{\partial \Sigma_k}=\frac{\partial \sum_{m=1}^Mln(\sum_{k=1}^{K}\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k))}{\partial \Sigma_k}$$$$=\sum_{m=1}^M\frac{\partial ln(\sum_{k=1}^{K}\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k))}{\partial \Sigma_k}=\sum_{m=1}^M\frac{\pi_k}{\sum_{k=1}^{K}\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k)}\frac{\partial \mathcal{N}(x_m|\mu_k,\Sigma_k)}{\partial \Sigma_k}$$$$=\sum_{m=1}^M\frac{\pi_k}{\sum_{k=1}^{K}\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k)}\frac{1}{(2\pi)^{N/2}}\frac{\partial\frac{1}{|\Sigma_k|^{1/2}}exp{-\frac{1}{2}(x_m-\mu_k)^T\Sigma_k^{-1}(x_m-\mu_k)}}{\partial \Sigma_k}$$$$=……….$$ 最后令$\frac{\partial ln\ p(X|\pi,\mu,\Sigma)}{\partial \Sigma_k}=0$得到$$\Sigma_k\sum_{m=1}^M\frac{\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k)}{\sum_{k=1}^{K}\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k)}=\sum_{m=1}^M\frac{\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k)}{\sum_{k=1}^{K}\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k)}(x_m-\mu_k)(x_m-\mu_k)^T$$ 所以$$\Sigma_k=\sum_{m=1}^M\frac{\gamma(z_{mk})(x_m-\mu_k)(x_m-\mu_k)^T}{\sum_{m=1}^M\gamma(z_{mk})}$$ 以上求导可以看出$\mu_k$与$\Sigma_k$互相嵌套，无法得到一个闭式解(解析解)。 在似然函数中$\pi_k$也属于未知量，也需要对其求导进行优化 在优化$\pi_k$使似然函数最小化的同时，$\pi_k$还需要满足条件$\sum_{k=1}^K\pi_k=1$。所以可以通过拉格朗日乘子将该约束加入似然函数得$$\theta(\pi,\mu,\Sigma;X)=ln\ p(X|\pi,\mu,\Sigma)+\lambda(\sum_{k=1}^K\pi_k-1)=\sum_{m=1}^Mln(\sum_{k=1}^{K}\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k))+\lambda(\sum_{k=1}^K\pi_k-1)$$ 对\pi_k求偏导$$\frac{\partial \theta(\pi,\mu,\Sigma;X)}{\partial \pi_k}=\sum_{m=1}^M\frac{\partial ln(\sum_{k=1}^{K}\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k))}{\partial \pi_k}+\lambda\frac{\sum_{k=1}^K\pi_k-1}{\partial \pi_k}=\sum_{m=1}^M\frac{\mathcal{N}(x_m|\mu_k,\Sigma_k)}{\sum_{k=1}^{K}\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k)}+\lambda$$ 令$\frac{\partial \theta(\pi,\mu,\Sigma;X)}{\partial \pi_k}=0$得$$\sum_{m=1}^M\frac{\mathcal{N}(x_m|\mu_k,\Sigma_k)}{\sum_{k=1}^{K}\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k)}+\lambda=0\Rightarrow \sum_{m=1}^M\frac{\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k)}{\sum_{k=1}^{K}\pi_k\mathcal{N}(x_m|\mu_k,\Sigma_k)}+\lambda\pi_k=0$$ 所以$\sum_{m=1}^M\gamma(z_{mk})+\lambda\pi_k=0$ 所以$$\sum_{k=1}^K(\sum_{m=1}^M\gamma(z_{mk})+\lambda\pi_k)=0\Rightarrow \sum_{k=1}^K\sum_{m=1}^M\gamma(z_{mk})+\lambda\sum_{k=1}^K\pi_k=0\Rightarrow \lambda=-\sum_{k=1}^K\sum_{m=1}^M\gamma(z_{mk})$$ 带入原式得$$\pi_k=\frac{\sum_{m=1}^M\gamma(z_{mk})}{\sum_{k=1}^K\sum_{m=1}^M\gamma(z_{mk})}$$ EM方法根据最初推导的模型，如果我们知道\pi_k，\mu_k与\Sigma_k，那么我们就相当于解决了问题，可以根据这三个参数得到所需的概率密度，从而可以对原样本进行划分，对新样本进行预测。然而经过最大似然求解，我们发现无法得到闭式解。 观察$\pi_k$，$\mu_k$与$\Sigma_k$的表达式，发现我们之所以我们计算出$\pi_k$，$\mu_k$与$\Sigma_k$是因为我们无法得到$\gamma(z_k)$，而无法计算$\gamma(z_k)$又是因为$\gamma(z_k)$依赖于$\pi_k$，$\mu_k$与$\Sigma_k$。 其依赖关系如下图 则从上图很容易想到一个迭代过程： 初始化$\pi_k$，$\mu_k$与$\Sigma_k$。 计算$\gamma(z_k)$，即预测对于一个样本x他属于哪个类$p(z_k=1|x)$。 对所有样本预测完成后根据$\gamma(z_k)$重新计算$\pi_k$，$\mu_k$与$\Sigma_k$。 若没有达到退出条件，从1.步继续迭代。 初始化$\pi_k$，$\mu_k$与$\Sigma_k$步骤，可以选择向样本空间随机布置几个$\Sigma=I$的高斯分布得到，也可以使用K-means算法进行初始聚类后，对每个类生成一个初始高斯分布得到。 完整混合高斯模型的EM算法如下： 初始化$\pi_k$，$\mu_k$与$\Sigma_k$，可以选择向样本空间随机布置几个$\Sigma=I$的高斯分布得到，也可以使用K-means算法进行初始聚类后，对每个类生成一个初始高斯分布得到。 （E step）计算$\gamma(z_k)$，即预测对于一个样本$x$他属于哪个类$$p(z_k=1|x)=\frac{\pi_k\mathcal{N}(x|\mu_k,\Sigma_k)}{\sum_{k=1}^K\pi_k\mathcal{N}(x|\mu_k,\Sigma_k)}$$ （M step）对所有样本预测完成后根据$\gamma(z_k)$重新计算$\pi_k$，$\mu_k$与$\Sigma_k$。$$\mu_k^{new}=\frac{\sum_{m=1}^M\gamma(z_{mk})x_m}{\sum_{m=1}^M\gamma(z_{mk})},\Sigma_k^{new}=\sum_{m=1}^M\frac{\gamma(z_{mk})(x_m-\mu_k)(x_m-\mu_k)^T}{\sum_{m=1}^M\gamma(z_{mk})}，\pi_k^{new}=\frac{\sum_{m=1}^M\gamma(z_{mk})}{\sum_{k=1}^K\sum_{m=1}^M\gamma(z_{mk})}$$ 评估似然函数$$\theta(\pi^{new},\mu^{new},\Sigma^{new};X)=\sum_{m=1}^Mln(\sum_{k=1}^{K}\pi_k^{new}\mathcal{N}(x_m|\mu_k^{new},\Sigma_k^{new}))$$若似然函数的变化没有低于某个阈值，继续迭代：$$\pi_k=\pi_k^{new}，\mu_k=\mu_k^{new}，\Sigma_k=\Sigma_k^{new}。$$ 其中第2步被称为E step，即期望（分类）(expectation)步骤，其根据目前已知的参数$\pi_k$，$\mu_k$与$\Sigma_k$对样本的类型进行预测。第3步被称为M step，即最大化（似然函数）（maximization）步骤，根据2步得到的新的分类情况最大化似然函数$\theta(\pi^{new},\mu^{new},\Sigma^{new};X)$，即使用第3步中的三个公式计算$\pi_k$，$\mu_k$与$\Sigma_k$就相当于最大化了似然函数。 可以受到启示，要优化参数具有前面所展示的图中依赖关系的问题，均可以套用EM方法的迭代模式，对目标函数进行优化。 下面给出一个sklearn的例子1234567891011121314151617import numpy as npfrom sklearn import datasetsfrom sklearn.cluster import KMeansfrom sklearn.mixture import GaussianMixturex = np.array([[1.2,2.6],[2.8,3.9],[3.2,4],[3,3],[1,1],[-1,0.2],[-2,-1],[2,0]])#K-Meanskmeans=KMeans(n_clusters=2)kmeans.fit(x)print( 'K-Means均值 = \n', kmeans.cluster_centers_ )print(kmeans.predict(x))gmm=GaussianMixture(n_components=2,covariance_type='full')gmm.fit(x)print('GMM均值 = \n', gmm.means_ )print(gmm.fit(x))]]></content>
      <categories>
        <category>machine_leanring</category>
        <category>code</category>
      </categories>
      <tags>
        <tag>machine_leanring</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linear Regression]]></title>
    <url>%2F2018%2F05%2F15%2FLinear-Regression%2F</url>
    <content type="text"><![CDATA[假设$h(x)=y$，有大量样本，求$h(x)$是什么?那么可以假设$$h(x)=\theta_{0} x^0 +\theta _{1}x^1+\theta _{2}x^2+…+\theta _{n}x^n+…$$因为$x^0=1$所以$\theta_{0} *x^0$可以简化为$\theta_{0} $ 为了方便，这里先记只有$\theta_{0}$ 和$\theta_{1}$ 的情况，原公式就简化为了：$$h(x)=\theta_{0} +\theta_{1} *x$$ 假设实际结果为$y$，其中$y=f(x)$那么，$h(x)$在什么情况下最接近$f(x)$呢？ 当然是$h(x)-f(x)$最接近0的情况下啦。 那大量样本（m组样本）呢？我们给$h(x)-f(x)$做一个平方，即$(h(x)-f(x))^2$，这样保证它为正，为正了以后，就是求最小了对吧。 然后把这些样本相加：$\sum_{i}^{m}{} (h(x_{i} )-f(x_{i} ))^2$这个相加后的值最小的时候，就是我们的假定函数h（x）最接近实际函数f（x）的时候。 由于方程中没有$f(x)$，而$f(x)=y$，所以方程可以写为：$\sum_{i}^{m}{} (h(x_{i} )-y_{i} )^2$ 我们定义观测结果$y$和预测结果$y’$之间的差别为Rss:$$Rss = \sum_{i=1}^{m}({y_i-y_i’} )^2= \sum_{i=1}^{m}({y_i-h(x_i)} )^2 = (y-h(x))^T*(y-h(x))$$ 设若参数的矩阵为$\theta$,则$h(x)=\theta*x$ 那么$$Rss = (y-h(X))^T(y-h(X)) = (y-\theta x)^T(y-\theta x) $$ 按照我们的定义，这个Rss的意思是y和y’之间的差，那么当Rss无限趋近于0的时候，则y≈y’，即我们求得的预测结果就等于实际结果。 于是，令Rss等于某一极小值$\delta$ ，则$(y-\theta x)^T*(y-\theta x) = \delta $ 对参数$\theta$求导，得：$$\frac{d}{d\theta}(y-\theta x)^T(y-\theta x) = 2x^T(y-\theta x) = 0$$ 展开，得$ x^Ty=x^T\thetax=x^T x\theta$ 进而就可以得到$\theta ==(x^Tx)^{-1}x^T*y$ 于是我们就得到正规方程了。 当然还有用梯度下降方法来求解。这里就不做详解了。]]></content>
      <categories>
        <category>machine_leanring</category>
        <category>code</category>
      </categories>
      <tags>
        <tag>machine_leanring</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从形式文法到数学的极限(1)]]></title>
    <url>%2F2018%2F05%2F12%2F%E4%BB%8E%E5%BD%A2%E5%BC%8F%E6%96%87%E6%B3%95%E5%88%B0%E6%95%B0%E5%AD%A6%E7%9A%84%E6%9E%81%E9%99%90-1%2F</url>
    <content type="text"><![CDATA[从形式文法到数学的极限 (1) - 乔姆斯基和他的形式文法诺姆·乔姆斯基（Noam Chomsky），美国语言学家，为科学作出的最大贡献便是他提出的形式文法。这个概念一经提出，直接促成了不知道多少重要技术，其中收益最多的领域可能就是信息行业了。 语法分析器在计算机中，要想使写好的代码运行起来，必须要有一个叫做“编译”的中间过程，将代码转换成可直接由机器执行的文件（这里只涉及编译型语言）。为了让计算机在编译时读懂你写的代码，一个适当的语法分析器是必不可少的。 每种语言都有自己不同的语法，语法分析器按照各种语言定义的语法，逐字符地将源代码解析成一棵语法树。而用来描述这种语法的工具，就是乔姆斯基的形式文法。 巴克斯-瑙尔范式要定义一套语法很简单，下面是一组（简陋的）示例：1234567&lt;sentence&gt; := [ &lt;subject&gt; ] &lt;predicate&gt; &#123; &lt;object&gt; &#125;&lt;subject&gt; := &lt;nominal-phrase&gt;&lt;object&gt; := &lt;nominal-phrase&gt;&lt;nominal-phrase&gt; := &lt;quantifier&gt; &lt;noun&gt;&lt;quantifier&gt; := &lt;article&gt; | &lt;number&gt;&lt;article&gt; := &quot;a&quot; | &quot;an&quot; | &quot;the&quot;... 如你所见，所有的字面量都很直观、浅显，只是有几个符号需要解释一下： 每条语法的最左项是它的名字，用尖括号括起来，跟随一个冒号和一个等号，表示定义； 在语法定义里引用其他语法，也用尖括号括起要引用的名字即可； 若要表示出现与否均可的选项，用方括号括起，比如 &lt;sentence&gt; 里的 &lt;subject&gt;； 若要表示出现任意次数（包括 0 次），用大括号括起，比如 &lt;sentence&gt; 里的 &lt;object&gt;； 若要表示多个模式里任选，用竖杠分隔开； 若要表示欲匹配的字符串，用双引号括起。 这种书写记号是荷兰学者瑙尔（Naur）最先提出的，后经美国学者巴克斯（Bakus）完善，形成了现在的巴克斯-瑙尔范式。乔姆斯基在他自己的形式文法里，也采用了类似B-N 范式的书写方式。 他把所有出现在尖括号里的那些名称称作“非终结符”，意为可以继续向下分解成更小的单位；将出现在引号里的字符串称作“终结符”，意为最小单位，不可继续分解。在语言学中，通常用大写字母表示非终结符，而用小写字母表示终结符。 语法层级乔姆斯基给他的文法根据不同的规则划分了不同的等级，以表示它们所描述语言的“自由”程度。12340 型语言：任意语法所描述的语言都是 0 型的上下文有关（1 型）语言：对于所有语法都有 aAb := cBd 形式的语言，称之为上下文有关语言上下文无关（2 型）语言：对于所有语法都有 A := aBb 形式的语言，称之为上下文无关语言正则（3 型）语言：对于所有语法都有 A := aB 或 A := Ba 形式的语言，称之为正则语言 可以看出，从上到下对语法的限制愈发增多，语言的多样性也随之减少。对于 1 和 2 型语言的命名中的“有关”、“无关”，是指非终结符的解析是否受其邻接的终结符影响。 大部分自然语言都是上下文有关语言。比如英语中&quot;I broke a window&quot;和&quot;The window broke&quot;中的&quot;broke&quot;便是受到宾语有无的影响而分别成为了及物和不及物动词。 上下文无关语言主要被用于设计程序，这是因为解析 2 型语言时不需考虑语法结构所处的上下文，其语法解析器相对容易实现。 可以看出，正则语言的表达能力极为受限，因为其解析过程只能在每一步中增添一前缀或后缀。对于总是增加前缀的正则语言，我们称为左线性语言，反之称为右线性语言。事实上，对这种高度受限的语言，人们已经设计出来一些强大的用来解析它们的工具了，比如正则表达式。 语法推导对于任意一组语法和一个指定的非终结符，总是可以根据其中的规则将这个非终结符扩展成一个终结符串（可能是无限的）。比如对于这条（正则）文法：1&lt;A&gt; := &quot;a&quot; &lt;A&gt; | &quot;&quot; 不断地将 &lt;A&gt;替换为&quot;a&quot;，便可以推出下列字符串：1a, aa, aaa, aaaa, ... 如果一个字符串（包括终结符和非终结符）能由某组语法推导过来，就说这个字符串规约到这组语法。 数学推理在数学上，我们把最开始给定的那组语法叫做“公理”，把由所有终结符可能构成的字符串称作“命题”。所有的公理都是公认为真的命题，比如欧式几何里的“过两点必有一条直线”。 对于那些可以规约到公理的命题，我们说它们是也是真的，并称它们为“定理”。而公理到定理的推导过程，自然就叫“证明”。 所有的公理都可以在自己前面加上一个终结符“非”，来变成自己的反命题。对于所有能规约到公理的反命题的命题，我们说它们是假的。这样，所有命题就有了自己的真假，或者更专业一点地，叫它们“真值”。 对于任意的数学命题，只需要对它们进行规约，看看是否能回到最初的公理即可知道它们正确与否了。 这似乎看上去太美好了，你怎么能保证所有命题都一定能规约到公理或它们的反命题上去呢？有没有可能存在一个命题，它根本就不可能通过有限次规约，得到最初的公理呢？ 还真的有一个神人挖到了这么一个大坑，直接否定了数学的完备性（即所有命题均能证明或证伪）。至于这人是谁，以及这坑怎么挖的，请看下回分解。]]></content>
      <categories>
        <category>math</category>
      </categories>
      <tags>
        <tag>math, NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[支持向量机(SVM)详解]]></title>
    <url>%2F2018%2F05%2F07%2F%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA-SVM-%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[P.S. 鉴于个人水平有限，若有表述错误之处敬请指出，愿我们共同进步~ 基本型推导SVM一直是我最喜欢的几个算法之一，所以没事喜欢推一推。 这里尝试用简单直白的语言来对它的推导过程进行介绍~如果你觉得你已经把高数和高中数学忘得差不多了，我想这或许就是你一直在找的简明SVM推导过程介绍~😋 给定$D={(x_1,y_1),(x_2,y_2),…,(x_m,y_m)},y_i \in {-1,+1}$，SVM考虑基于训练集$D$在样本空间中找到一个划分超平面(hiperplane)，将不同类别的样本分开。 划分超平面方程：$${\bf \omega}^Tx+b=0 \tag{1}$$（别慌，它其实就是一条直线公式而已）其中$\omega=(\omega_1;\omega_2;…;\omega_d;)$为法向量，决定了超平面的方向；$b$为位移项，决定了超平面与原点之间的距离。通常划分超平面用$(\omega,b)$来表示，因为其可被法向量$\omega$和位移$b$确定。样本空间中任意点$x$到超平面$(\omega,b)$的距离可写为$$r=\frac{|\omega^Tx+b|}{||\omega||} \tag{2}$$（再次别慌，让我们转动我们掌握了初中数学知识的小脑瓜来想一下，其实这就是点到直线的距离公式对吧）假设超平面$(\omega,b)$能将训练样本正确分类，对于$(x_i,y_i)\in D$,有以下式子：$$\begin{cases}\omega^Tx_i+b &gt; 0, &amp; y_i = +1\\omega^Tx_i+b &lt; 0, &amp; y_i = -1\end{cases} \tag{3}$$这其实便是逻辑回归的思路，非黑即白，但是这明显对自己的算法太自信了对吧。我们可爱的支持向量机对此表示怀疑，并且多留了一个心眼，于是它令：$$\begin{cases}\omega^Tx_i+b \geq +1, &amp; y_i = +1\\omega^Tx_i+b \leq -1, &amp; y_i = -1\end{cases} \tag{4}$$ 如图所示： 其中距离划分超平面最近的几个训练样本点使上式的等号成立，这几个训练样本就被称作支持向量（support vector），两个异类支持向量到超平面的距离之和，也称为间隔（margin），为$$\gamma = \frac{2}{||\omega||} \tag{5}$$(其实就是 $\omega^Tx_i+b = +1 $和$ \omega^Tx_i+b = -1 $两个超平面之间的距离公式) 那么按我们的设想，我们做分类学习，实际上就是为了找到一个划分超平面将这些样本给隔开，那么什么样子的划分超平面是最有效的呢？从直观上来看，位于正负样本“正中间”的划分超平面，也就是上图红色的那一个划分超平面，应该是最好的，因为它的鲁棒性最好，对未知样本的泛化性最好。 那么如何去得到这个刚好位于正负样本“正中间”的划分超平面呢？思考一下，是不是只要我们让间隔最大，这样只要我们取间隔中间的那条直线，我们就可以找到这个最棒的划分超平面了？换句话说，对于基于SVM的分类学习而言，问题已经从找到一个最好的划分超平面转换为了找到样本空间里的最大化间隔。我们用数学语言来描述一下这个更新后的问题，就变成了找到能满足$式(4)$的约束条件的参数$\omega$和$b$，并且要使得$\gamma = \frac{2}{||\omega||}$中的$\gamma$最大。用数学公式来表示是这个样子:$$max_{\omega,b} \frac{2}{||\omega||},s.t. y_i(\omega^Tx_i+b) \geq 1, i = 1,2,…,m \tag{6}$$OK，你可能会喊：等等等等，$max_{\omega,b} \frac{2}{||\omega||}$是表示$\max ,\gamma$我懂，但是你这个条件$y_i(\omega^Tx_i+b) \geq 1, i = 1,2,…,m$是什么鬼你要不要解释一下？ 嗯咳，让我们暂停一下，回看一下约束条件(即正确分类的条件)：$$\begin{cases}\omega^Tx_i+b \geq +1, &amp; y_i = +1\\omega^Tx_i+b \leq -1, &amp; y_i = -1\end{cases} \tag{7}$$注意，是不是上下两个约束条件的左右式子相乘（$ \omega^Tx_i+b \geq +1 y_i = +1$ or $\omega^Tx_i+b \leq -1 y_i = -1$ ）就等于$y_i(\omega^Tx_i+b) \geq 1$了？ OK，让我们继续往下。对于式子$$max_{\omega,b} \frac{2}{||\omega||},s.t. y_i(\omega^Tx_i+b) \geq 1, i = 1,2,…,m \tag{8}$$显然，为了最大化间隔，仅需最大化$\frac{1}{||\omega||}$，为了使目标函数光滑从而方便计算，于是这等价于最小化$||\omega||^2$。于是上式我们可以重写为：$$min_{\omega,b}\frac{1}{2}||\omega||^2,s.t. y_i(\omega^Tx_i+b) \geq 1, i = 1,2,…,m \tag{9}$$这就是SVM的基本型。 我们需要注意到，它其实是一个凸二次规划问题（convex quadratic programming），可以用之前的拉格朗日乘子法求解。 SVM的拉格朗日函数首先回顾一下拉格朗日函数标准型$$\min \ f(x)； \ \ s.t. g_i(x) \leq 0； i = 1,…,m, h_i(x) = 0； i = 1,…,m \tag{10}$$你可以看到，SVM基本式相对拉格朗日函数标准型而言，有$f(\omega, b) = \frac{1}{2}||\omega||^2$$g(\omega, b) = 1-y_i(\omega^Tx_i+b) \leq 0,i=1,2,…,m.$$h(\omega, b) = 0$ 因此，我们将SVM的$f(\omega, b)、g(\omega, b)、h(\omega, b)$代入(10)后，就可以得到该问题的拉格朗日函数了，即$$L(\omega,b, \alpha) = \frac{1}{2}||\omega||^2 + \sum_{i=1}^m \alpha_i (1-y_i(\omega^Tx_i + b)) \tag{11}$$其中$\alpha = (\alpha_1; \alpha_2;…;\alpha_m)$,拉格朗日乘子$\alpha_i \geq 0$。 通常一个优化问题可以从两个角度来考虑，即主问题(primal problem)和对偶问题(dual problem)。在约束最优化问题中，常常利用拉格朗日对偶性将原始问题（主问题）转换成对偶问题，通过解对偶问题来得到原始问题的解。这样做是因为对偶问题的复杂度往往低于主问题。所以在求解SVM的时候，我们也会通过其拉格朗日对偶性，将该主问题$式(9)$转换成对偶问题，然后进行求解。 SVM的对偶问题需要说明的是，因为主问题本身是一个凸二次规划问题，因此它是能直接用现成的优化计算包求解的，使用拉格朗日乘子法得到其对偶问题是为了优化运算效率。 那么让我们回顾一下SVM的主问题$式(9)$和SVM的拉格朗日函数$式(10)$，其中$\alpha = (\alpha_1; \alpha_2;…;\alpha_m)$,拉格朗日乘子$\alpha_i \geq 0$。 你可以发现，SVM恰恰满足了前面博客中我们讲过的的强对偶性。 因此，考虑令$L(\omega,b, \alpha)$对$\omega$和$b$的偏导为0可得$$\omega = \sum_{i=1}^m \alpha_i y_i x_i \tag{12}$$$$0 = \sum_{i=1}^m \alpha_i y_i \tag{13}$$将式(12)代入(11)，就可以将$L(\omega,b, \alpha)$中的$\omega$和$b$消去，再考虑式(13)的约束，就得到式（9）的对偶问题。$$max_{\alpha} \sum_{i=1}^m \alpha_i - \frac{1}{2} \sum_{i=1}^m \sum_{j=1}^m \alpha_i \alpha_j y_i y_j x_i^T x_j \tag{14}$$$$s.t. \sum_{i=1}^m \alpha_i y_i = 0, \alpha_i \geq 0, i=1,2,…,m$$ 求解该模型，解出$\alpha$后，求出$\omega$和$b$即可得到模型$$f(x) = \omega^T x + b = \sum_{i=1}^m \alpha_i y_i x_i^Tx + b \tag{15}$$从对偶问题（14）解出的$\alpha_i$是式(11)中的拉格朗日乘子，它恰好对应着训练样本$(x_i,y_i)$。因为式(9)中有不等式约束，因此上述过程还需满足KKT条件，即$$\begin{cases}\alpha_i \geq 0\ y_i f(x_i)-1 \geq 0\ \alpha_i(y_i f(x_i)-1) = 0\end{cases} \tag{16}$$ 于是，对任意训练样本$(x_i,y_i)$，总有$\alpha_i=0$或$y_i f(x_i)=1$。 若$\alpha_i = 0$，则该样本将不会在式（4.1）的求和中出现，也就不会对$f(x)$产生任何影响； 若$\alpha_i &gt; 0$，则必有$y_i f(x_i)=1$，所对应的样本点位于最大间隔边界上，是一个支持向量。 这显示出支持向量机的一个重要性质： 训练完成后，大部分的训练样本都不需保留，最终模型仅与支持向量有关。 那么如何求解式（14）呢？不难发现，这是一个二次规划问题，可使用通用的二次规划算法来求解，这一部分可以参考刚才对【二次规划】的讲解，除此之外，还可以使用SMO等算法对其进行求解。关于SMO算法，我们就下次再说啦~ 编程实现见我GitHub。 参考资料 [1] 《统计学习方法》李航 著 清华大学出版社[2]《机器学习》 周志华 著 清华大学出版社[3] Stanford CS229 Machine Learning的教学资料：http://cs229.stanford.edu/section/cs229-cvxopt.pdf]]></content>
      <categories>
        <category>machine_leanring</category>
        <category>code</category>
      </categories>
      <tags>
        <tag>machine_leanring</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[拉格朗日对偶性]]></title>
    <url>%2F2018%2F05%2F02%2F%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7%2F</url>
    <content type="text"><![CDATA[预备知识凸集首先是凸集，这个的几何意义如图所示，指集合中任意两点间的线段永远在集合中的集合。 你可以看到，左图中任意两点，它们的线段一定会在C这个集合中，所以它就是一个凸集，但是对于右图而言，其线段就可能会有部分不在集合中，因此它不是凸集。 如果我们用数学定义来描述凸集的话，它是指这样的一个集合$C$：对任意$x,y \in C, \lambda \in R, 0 \leq \lambda \leq 1$，都有$$\lambda x + (1-\lambda)y \in C \tag{1}$$ 常见的凸集有： n维实数空间； 一些范数约束形式的集合； 仿射子空间； 凸集的交集； n维半正定矩阵集。 凸函数那么知道凸集是什么意思后，让我们看看凸函数。凸函数的几何意义是，函数任意两点(x, y)连线上的值一定大于对应自变量区(x, y)间内的函数值(f(x),f(y))，示例图如下： 你可以看到，将xy连接起来的直线上的值，一定大于xy两点范围内的函数值$f(x)$ 如果我们用数学定义来描述凸函数的话，凸函数$f(x)$是这样的一个函数：定义域为凸集$D(f)$，且对于任意$x,y \in D(f),\lambda \in R, 0 \leq \lambda \leq 1$，有$$f(\lambda x + (1-\lambda)y) \leq \lambda f(x) + (1-\lambda)f(y) \tag{2}$$ 常见的凸函数有: 指数函数族；非负对数函数；仿射函数；二次函数；常见的范数函数；凸函数非负加权的和等。 ##凸优化问题 讲解完凸集和凸函数的定义，终于到凸优化问题了。凸优化问题，实际上就是研究定义于凸集中的凸函数最小化问题。正式的说，凸优化问题在优化问题中的形式是这样的$$\min \ f(x), \ \ s.t. x \in C \tag{3}$$其中$f$是一个凸函数，$C$为凸集，$x$是优化变量。因为上面这个表达还是有些含糊，所以凸优化问题我们更常用这个形式来表示它$$\min \ f(x), \ \ s.t. g_i(x) \leq 0, i = 1,…,m, h_i(x) = 0, i = 1,…,m \tag{4}$$其中$f$是一个凸函数，$g_i$是一个凸函数, $h_i$为仿射函数，$x$是优化变量。 常见的凸优化问题有: 线性规划、二次规划、二次约束的二次规划、半正定规划。 二次规划二次规划（Quadratic Programming，简称QP）是一类典型的优化问题，包括凸二次规划和非凸二次优化。在此类问题中，目标函数是变量的二次函数，而约束条件是变量的线性不等式。假定变量个数为$d$，约束条件的个数是$m$，则标准的二次规划问题形如$$min_x \frac{1}{2} x^TQx + c^Tx,\ s.t. Ax \leq b \tag{5}$$其中$x$为$d$维向量，$ Q\in R^{d \times d}$为实对称矩阵，$ A \in R^{m \times d}$为实矩阵，$b \in R^m$和$c \in R^d$为实向量，$Ax \leq b$的每一行对应一个约束。 拉格朗日乘子法拉格朗日乘子法（Largrange multipliers）是一种寻找多元函数在一组约束下的极值的方法。通过引入拉格朗日乘子，可将有$d$个变量与$k$个约束条件的最优化问题转化为具有$d+k$个变量的无约束优化问题求解。这样说你可能会觉得很抽象，那么让我们先举一个稍微简单一点的例子。 等式约束的优化问题先考虑一个等式约束的优化问题，假定$x$为$d$维向量，我们想要寻找$x$的某个取值$x^*$，使得目标函数$f(x)$最小且同时满足$g(x)=0$的约束。从几何角度看，这个问题的目标是在方程$g(x)=0$确定的$d-1$维曲面上寻找能使目标函数最小化的点。如图所示 通过上面的描述，我们不难得到如下结论：对于约束曲面上的任意点$x$，该点的梯度$\nabla g(x)$正交于约束曲面；在最优点$x^$，目标函数在该点的梯度$\nabla f(x^)$正交于约束曲面； 由此可知，在最优点$x^$，如上图所示，梯度$\nabla g(x)$和$\nabla f(x)$的方向必相同或相反，即存在$\lambda \neq 0$($\lambda$称为拉格朗日乘子)使得$$\nabla f(x^) + \lambda \nabla g(x^*) = 0 \tag{6}$$基于上面这个式子，我们可以定义拉格朗日函数$$L(x,\lambda) = f(x) + \lambda g(x) \tag{7}$$ 你可能会疑惑，为什么拉格朗日函数会是这个样子。那么让我们对这个函数进行求导看看：当$L(x,\lambda)$对$x$求导时：(因为要求最优解，所以我们需要置$\nabla_x L(x, \lambda)$为0)$$\nabla_x L(x, \lambda) = \nabla f(x) + \lambda \nabla g(x) = 0 \tag{8}$$当$L(x,\lambda)$对$\lambda$求导时：（我们同样置$\nabla_{\lambda} L(x, \lambda)$为0）$$\nabla_{\lambda} L(x, \lambda) = 0 + g(x) = 0 \tag{9}$$ 注意，在一开始，我们的目标是寻找$x$的某个取值$x^*$，使得目标函数$f(x)$最小且同时满足$g(x)=0$的约束。那么通过拉格朗日函数的两次对不同变量的求导，你可以发现式(8) = 式(6)，这是为了使得目标函数$f(x)$最小 式(9) = 约束条件$g(x)=0$，这是为了满足约束条件 因此，原约束优化问题就转化为了对拉格朗日函数$L(x, \lambda)$的无约束优化问题。 不等式约束的优化问题现在考虑不等式约束$g(x) \leq 0$，如图4所示，此时最优点$x^*$要么在$g(x) &lt; 0 $的区域里，要么在边界$g(x) = 0$上。 对于$g(x)&lt;0$的情形，约束$g(x) \leq 0$不起作用，可直接通过条件$\nabla f(x)=0$来获得最优点，这等价于将$\lambda$置0，然后对$\nabla_x L(x, \lambda)$置0得到最优点对于$g(x)=0$的情形，类似于上面我们对等式约束的分析，但需注意的是，此时$\nabla f(x^)$的方向必与$\nabla g(x^)$相反，即存在常数$\lambda &gt; 0$使得$\nabla f(x^) + \lambda \nabla g(x^) = 0$ 对上面描述的两种情况进行整合，在约束$g(x) \leq 0$下最小化$f(x)$的任务，可以转化为在如下约束下最小化$L(x,\lambda) = f(x) + \lambda g(x)$的任务：$$\begin{cases}g(x) \leq 0\ \lambda \geq 0\ \lambda_j g_j(x) = 0\end{cases} \tag{10}$$ 式(2.10)就是大名鼎鼎的Karush-Kuhn-Tucker（简称KKT）条件。多个约束条件的优化问题 在对等式约束优化问题和不等式约束优化问题进行讲解后，我们可以将其推广到多个约束。考虑具有$m$个等式约束和$n$个不等式约束，且可行域$D \subset R^d$非空的优化问题$$min_x \ f(x),\ \ s.t. \ h_i(x)=0 \ (i = 1,…,m), \ g_j(x) \leq 0 \ (j = 1,…,m) \tag{11}$$引入拉格朗日乘子$\lambda = (\lambda_1, \lambda_2, …, \lambda_m)^T$和$\mu = (\mu_1,\mu_2,…,\mu_n)^T$，相应的拉格朗日函数为$$L(x,\lambda, \mu) = f(x) + \sum_{i=1}^m \lambda_i h_i(x) +\sum_{j=1}^n \mu_j g_j(x) \tag{12}$$由不等式约束引入的KKT条件(j=1,2,…,n)为$$\begin{cases}g_j(x) \leq 0\ \mu_j \geq 0\ \mu_j g_j(x) = 0\end{cases} \tag{13}$$ 对偶问题前面我们说过，一个优化问题可以从两个角度来考虑，即主问题(primal problem)和对偶问题(dual problem)。在约束最优化问题中，常常利用拉格朗日对偶性将原始问题（主问题）转换成对偶问题，通过解对偶问题来得到原始问题的解。这样做是因为对偶问题的复杂度往往低于主问题。 多约束对偶问题多约束对偶问题的通用解 多约束对偶问题的主问题:$$min_x \ f(x),\ \ s.t. \ h_i(x)=0 \ (i = 1,…,m), \ g_j(x) \leq 0 \ (j = 1,…,m) \tag{11}$$其相应的拉格朗日函数为$$L(x,\lambda, \mu) = f(x) + \sum_{i=1}^m \lambda_i h_i(x) +\sum_{j=1}^n \mu_j g_j(x) \tag{12}$$ 基于(12)，对主问题(11)的拉格朗日对偶函数$\Gamma: R^m \cdot R^n \rightarrow R$定义为$$\Gamma(\lambda,\mu) = inf_{x \in D} L(x,\lambda,\mu) = inf_{x \in D} (f(x) + \sum_{i=1}^m \lambda_i h_i(x) +\sum_{j=1}^n \mu_j g_j(x)) \tag{13}$$若$\tilde{x} \in D$为主问题(11)可行域中的点，则对任意$\mu \geq 0$和$\lambda$都有$$\sum_{i=1}^m \lambda_i h_i(x) + \sum_{j=1}^n \mu_j g_j(x) \leq 0 \tag{14}$$进而有$$\Gamma(\lambda,\mu) = inf_{x \in D} L(x,\lambda,\mu) \leq L(\tilde{x},\lambda,\mu) \leq f(\tilde{x}) \tag{15}$$ 若主问题(2.11)的最优值为$p^$，则对任意$\mu \geq 0$和$\lambda$都有$$\Gamma(\lambda,\mu) \leq p^ \tag{3.4}$$即对偶函数给出了主问题最优值的下界。显然，这个下界取决于$\mu$和$\lambda$的值。于是一个很自然的问题就是：基于对偶函数能获得的最好下界是什么？这就引出了优化问题$$max_{\lambda, \mu} \Gamma(\lambda, \mu) \ \ s.t. \mu \geq 0 \tag{16}$$ 式(16)就是主问题（11）的对偶问题，其中$\mu$和$\lambda$称为对偶变量（dual variable）。无论主问题（11）的凸性如何，对偶问题（16）始终是凸优化问题。 考虑式(16)的最优值$d^$，显然有$d^ \leq p^*$，这称为“弱对偶性(weak duality)”成立； 若$d^ = p^$，则称为“强对偶性（strong duality）”成立，此时由对偶问题能获得主问题的最优下界； 对于一般的优化问题，强对偶性通常不成立，但是若主问题是凸优化问题，如式(11)中$f(x)$和$g_j(x)$均为凸函数，$h_i(x)$为仿射函数，且其可行域中至少有一点使不等式约束严格成立，则此时强对偶性成立。 P.S. 在强对偶性成立时，将拉格朗日函数分别对元变量和对偶变量求导，再同时令导数等于0，即可得到原变量与对偶变量的数值关系。于是对偶问题解决了，主问题也就解决了。 参考资料 《统计学习方法》李航 著 清华大学出版社]]></content>
      <categories>
        <category>machine_leanring</category>
      </categories>
      <tags>
        <tag>machine_leanring</tag>
        <tag>math</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[logistic regression]]></title>
    <url>%2F2018%2F04%2F23%2Flogistic-regression%2F</url>
    <content type="text"><![CDATA[1 逻辑斯蒂分布(logistic distribution)定义：设X是连续随机变量，X服从logistic分布是指X具有下列分布函数和密度函数： $$ F = P(X \leq x)=\frac{1}{1+exp(-\frac{(x-\mu)}{\gamma})} $$$$ f= F’(x) = \frac{exp(-\frac{(x-\mu)}{\gamma})}{\gamma[1+exp(-\frac{(x-\mu)}{\gamma})]^2} $$式中，$\mu$为位置参数，$\gamma &gt; 0$为形状参数。 其分布图形如下： F曲线在中心附近增长速度较快，在两端增长速度较慢。形状参数$\gamma$值越小，曲线在中心附近增长得越快。 2 二项逻辑斯蒂回归模型定义：二项逻辑斯蒂回归模型是如下的条件概率分布$$P(y=1\mid x) = \frac{\exp(\omega^\top x+b)}{1 + \exp(\omega^\top x+b)} \tag{1}$$$$P(y=0\mid x) = \frac{1}{1 + \exp(\omega^\top x+b)} \tag{2}$$这里，$x\in R^n$是输入，$y\in {0,1 }$是输出，$\omega \in R^n$和$b\in R$是参数，$\omega$称为权值向量，$b$称为偏置，$\omega^\top x$为$w$和$x$的内积。 对于给定的输入实例$x$，按照(1)式和(2)式可以分别求得$P(y=1\mid x)$和$P(y=0\mid x)$。逻辑斯蒂回归比较这两个条件概率值的大小，将实例$x$分到概率值较大的那一类。 定义：一个事件的几率(odds)是指该事件发生的概率与该事件不发生的概率的比值。如果事件发生的概率为$p$，那么该事件的几率是$\frac{p}{1-p}$，该事件的对数几率(log odds)或者logit函数是$$logit(p)=log\frac{p}{1-p}$$对于逻辑斯蒂回归而言，由(1),(2)式得$$log\frac{P(y=1\mid x)}{1-P(y=1\mid x)}=\omega^\top x+b$$也就是说，在逻辑斯蒂回归模型中，输出$y=1$的对数几率是由输入$x$的线性函数表示的模型。 3 模型参数估计对于给定的训练数据集$T={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}$，其中$x_i\in R^n$，$y_i\in {0,1 }$，可以应用极大似然估计法估计模型参数，从而得到逻辑斯蒂回归模型。 设：$P(y=1\mid x)=\pi(x)$，$P(y=0\mid x)=1-\pi(x)$则似然函数为：$$\prod_{i=1}^N [\pi(x_i)]^{y_i}[1-\pi(x_i)]^{1-y_i}$$对数似然函数为：$$\begin{aligned}L(\omega,b)&amp;=\sum_{i=1}^N [y_i\ln(\pi(x_i))+(1-y_i)\ln(1-\pi(x_i))] \&amp;= \sum_{i=1}^N [y_i\ln\frac{\pi(x_i)}{1-\pi(x_i)}+\ln(1-\pi(x_i))] \&amp;=\sum_{i=1}^N \left [ y_i(\omega^\top x+b) -\ln \left ( 1+ exp(\omega^\top x+b) \right )\right ]\end{aligned}$$这样，问题就变成了以对数似然函数为目标函数的最优化问题，逻辑斯蒂回归学习中通常采用的方法是梯度下降法及牛顿法。 4 梯度下降(上升)法求解利用梯度下降(上升)求解对数似然函数：$L(\omega,b)$因为要使得似然函数最大，我们使用梯度上升法。为了计算方便，我们将权值向量和输入向量加以扩充，仍记作$\omega,x$，即$$\omega=(\omega^{(1)},\omega^{(2)},…,\omega^{(n)},b),\;x=(x^{(1)},x^{(2)},…,x^{(n)},1)$$ 梯度上升求解:这时$$\omega_{new}^\top x_{new}=\omega_{old}^\top x_{old}+b_{old}$$我们令：$$z=\omega^\top x; z_i=\omega^\top x_i;z_i^{(k)}=\omega_k^\top x_i$$$$\pi(z) = \frac{exp(z)}{1+exp(z)}= \frac{1}{1+exp(-z)} $$ 于是有$$l(\omega)=\sum_{i=1}^N \left [ y_i(\omega^\top x) -\ln \left ( 1+ exp(\omega^\top x) \right )\right ]$$先求各个偏导数：$$\begin{aligned}\frac{\partial l(\omega)}{\partial \omega^{(j)}}&amp;=\frac{\partial }{\partial \omega^{(j)}}\left ( \sum_{i=1}^N \left [ y_i(\omega^\top x) -\ln \left ( 1+ exp(\omega^\top x) \right )\right ]\right ) \ &amp;= \sum_{i=1}^N \left [ y_i x_i^{(j)} - \frac{exp(w^\top x_i)}{1+exp(w^\top x_i)} x_i^{(j)}\right ] \ &amp;= \sum_{i=1}^N \left ( y_i - \frac{exp(w^\top x_i)}{1+exp(w^\top x_i)} \right ) x_i^{(j)} \ &amp;= \sum_{i=1}^N ( y_i - \pi(z_i) ) x_i^{(j)}\end{aligned}$$ 得到参数的迭代公式：$$\omega_{k+1}^{(j)} = \omega_{k}^{(j)} +\lambda_k \cdot (-\sum_{i=1}^N ( y_i - \pi(z_i^{(k)}) ) ) x_i^{(j)} $$令$$s^{(k)}=(s_1^{(k)},s_2^{(k)},…,s_N^{(k)}),s_i^{(k)}= y_i - \pi_k(z_i^{(k)}) $$则$$\begin{aligned}\triangledown l(\omega_{k}) &amp;= ( \frac{\partial l(\omega_{k})}{\partial \omega_{k}^{(0)}}, \frac{\partial l(\omega_{k})}{\partial \omega_{k}^{(1)}},…, \frac{\partial l(\omega_{k})}{\partial \omega_{k}^{(n)}} ) \ &amp;= [\sum_{i=1}^N ( y_i - \pi(z_i^{(k)}) ) x_i^{(j)}],j=0,1,…,n \&amp;=[\sum_{i=1}^N s_i^{(k)} x_i^{(j)}] \&amp;=s^{(k)}\cdot x\\end{aligned}$$ 注意梯度上升为正梯度方向,即 $ P^{(k)} = \triangledown l(\omega_{k})$即有： $$\omega_{k+1} = \omega_{k} +\lambda_k P^{(k)} = \omega_{k} +\lambda_k \cdot (s^{(k)}\cdot x) $$ 求解一维搜索$$l(\omega_{k}+\lambda_k P^{(k)})=\max_{\lambda \geqslant 0}l(\omega_{k}+\lambda \cdot P^{(k)})$$ 得 $$\lambda_k=\frac{ - \triangledown l(\omega_{k})^\top \triangledown l(\omega_{k}) }{\triangledown l(\omega_{k})^\top H(\omega_{k}) \triangledown l(\omega_{k})} $$ 其中 $$H(\omega_{k})=\begin{bmatrix}\frac{\partial^2 l(\omega_{k})}{\partial \omega_{k}^{(p)}\partial \omega_{k}^{(q)}}\end{bmatrix} ;p,q \in {0,1,2,..,n}$$ $$\frac{\partial^2 l(\omega_{k})}{\partial \omega_{k}^{(p)}\partial \omega_{k}^{(q)}} = \sum_{i=1}^N \pi’(\omega_k x_i) ( x_i^{(p)} x_i^{(q)}) $$ $$\pi’(z) = \frac{exp(-z)}{(1+exp(-z))^2}=\pi(z)(1-\pi(z))$$ 5 模型的优缺点缺点： 逻辑回归需要大样本量，因为最大似然估计在低样本量的情况下不如最小二乘法有效。 为防止过拟合和欠拟合，应该让模型构建的变量是显著的。 对模型中自变量多重共线性较为敏感，需要对自变量进行相关性分析，剔除线性相关的变量。 优点：模型更简单，好理解，实现起来，特别是大规模线性分类时比较方便 6 模型实现见我GitHub。 7 最后在写的过程中才发现，没写一次都要花挺长的时间去理解以及使用markdown码上数学公式，但是这都很大的促进了我对原理的理解！ 参考资料 《统计学习方法》李航 著 清华大学出版社《机器学习实战》Peter Harrington 著 人民邮电出版社《运筹学》第四版 清华大学出版社]]></content>
      <categories>
        <category>machine_leanring</category>
        <category>code</category>
      </categories>
      <tags>
        <tag>machine_leanring</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[my kmeans]]></title>
    <url>%2F2018%2F04%2F15%2Fmy-kmeans%2F</url>
    <content type="text"></content>
      <categories>
        <category>machine_leanring</category>
        <category>code</category>
      </categories>
      <tags>
        <tag>machine_leanring</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[梯度下降法]]></title>
    <url>%2F2018%2F04%2F02%2F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%2F</url>
    <content type="text"><![CDATA[梯度下降法原理&amp;#8195&amp;#8195梯度下降法是求解无约束最优化问题的一种最常用的迭代法。顾名思义，梯度下降法的计算过程就是沿梯度下降的方向求解极小值（也可以沿梯度上升方向求解极大值）。 &amp;#8195&amp;#8195假设$f(x)$是$R^n$上具有一阶连续偏导数的函数，要求解的无约束最优化问题是$$\min_{x\in R^n} f(x)$$$x^*$表示目标函数的极小值点。 选取适当的初值$x^{(0)}$，不断迭代，更新$x$的值，进行目标函数的极小化，直到收敛。由于负梯度方向是使函数值下降最快的方向，在迭代的每一步，以负梯度方向更新$x$的值，从而达到减少函数值的目的。 &amp;#8195&amp;#8195由于$f(x)$具有一阶连续偏导数，若第k次迭代值为$x^{(k)}$，则可将$f(x)$在$x^{(k)}$附近进行一阶泰勒展开：$$f(x)=f(x^{(k)})+g_k^ \top \cdot(x-x^{(k)})$$其中$g_k =g(x^{(k)})= -\triangledown f(x^{(k)}) $为$f(x)$在$x^{(k)}$的梯度(梯度上升为正号)。&amp;#8195&amp;#8195求出第k+1次迭代值$x^{(k+1)}$:$$x^{(k+1)}\leftarrow x^{(k)}+\lambda_k P^{(k)} $$ 其中，$p^{(k)}$是搜索方向，取负梯度方向$P^{(k)}=-\triangledown f(x^{(k)}) $，$\lambda_k$是步长，由一维搜索确定，即$\lambda_k$使得$$f(x^{(k)}+\lambda_k P^{(k)})=\min_{\lambda \geqslant 0}f(x^{(k)}+\lambda \cdot p^{(k)})$$ 一维搜索有个十分重要的性质：在搜索方向上所得最优点处目标函数的梯度和该搜索方向正交。定理: 设目标函数$f(x)$具有一阶连续偏导数，$x^{(k+1)}$由如下规则产生$$ \left{ \begin{array}{c} \lambda_k: \min_\lambda f(x^{(k)}+\lambda P^{(k)}) \ x^{(k+1)} = x^{(k)}+\lambda_k P^{(k)} \end{array} \right.$$&#160;则有$$-\triangledown f(x^{(k+1)}) P^{(k)}=0 \tag{1}$$证明: 构造函数$\varphi(\lambda)=f(x^{(k)}+\lambda P^{(k)})$，则得$$ \left{ \begin{array}{c} \varphi(\lambda_k)= \min_\lambda \varphi(\lambda) \ x^{(k+1)} = x^{(k)}+\lambda_k P^{(k)} \end{array} \right.$$即$\lambda_k$为$\varphi(\lambda)$的极小值点。此外$\varphi’(\lambda)=\triangledown f(x^{(k)}+\lambda P^{(k)})^\top P^{(k)}$。由$\varphi’(\lambda)|_{\lambda=\lambda_k}=0$可得$$\triangledown f(x^{(k)}+\lambda_k P^{(k)})^\top P^{(k)}=\triangledown f(x^{(k+1)})^\top P^{(k)}=0$$定理得证。 为什么$P^{(k)}=-\triangledown f(x^{(k)})$?&amp;#8195&amp;#8195因为对于充分小的$\lambda$，只要$$f(x^{(k)})^\top P^{(k)}&lt;0 \tag{2}$$就可以保证$$f(x^{(k)}+\lambda_k P^{(k)})&lt;f(x^{(k)}) \tag{3}$$&amp;#8195&amp;#8195现在考察不同的$P^{(k)}$。假定$P^{(k)}$的模一定(且不为零)，并设$\triangledown f(x^{(k)})$(否则，$x^{(k)}$是平稳点)，使得(2)式成立的$P^{(k)}$有无限多个，为了使目标函数数值能得到尽量大的改善，必须寻求使$f(x^{(k)})^\top P^{(k)} $取最小值的$P^{(k)}$，因为有$$f(x^{(k)})^\top P^{(k)}=|f(x^{(k)})| \cdot |P^{(k)}| \cdot \cos\theta$$式中$\theta$为$f(x^{(k)})$和$P^{(k)}$的夹角。当$P^{(k)}$与$f(x^{(k)})$反向时，$\theta=180°,\cos\theta=-1$。这时式(2)成立，且其左端取得最小值。 一维搜索&amp;#8195&amp;#8195为了得到下一个近似极小值点，在选定了搜索方向之后，还要确定步长$\lambda$。当采用可接受点算法时，就是取某一$\lambda$进行试算，看是否满足不等式(3)，若上述不等式成立，就可以迭代下去。否则缩小$\lambda$使得其满足不等式(3)。&amp;#8195&amp;#8195另一种方法就是在负梯度方向的一维搜索，来确定使$f(x^{(k)})$最小的$\lambda_k$。最常用的有试探法(斐波拉契，0.618法)，插值法(抛物线插值，三次插值)，微积分中的求根法(切线法、二分法等)等。 &amp;#8195&amp;#8195若$f(x)$具有二阶连续偏导数，在$x^{(k)}$作$f(x^{(k)}-\lambda \triangledown f(x^{(k)}) )$的泰勒展开：$$f(x^{(k)}-\lambda \triangledown f(x^{(k)})) \approx f(x^{(k)}) -\triangledown f(x^{(k)})^\top \lambda \triangledown f(x^{(k)}) + \frac{1}{2}\lambda \triangledown f(x^{(k)})^\top H(x^{(k)}) \lambda \triangledown f(x^{(k)})$$对$\lambda$求导并且令其等于零，则得近似最佳步长$$\lambda_k=\frac{ \triangledown f(x^{(k)})^\top \triangledown f(x^{(k)}) }{\triangledown f(x^{(k)})^\top H(x^{(k)}) \triangledown f(x^{(k)})} \tag{4}$$其中$$H(x^{(k)})=\begin{bmatrix}\frac{\partial^2 f(x^{(k)}))}{\partial x_1^2} &amp; \frac{\partial^2 f(x^{(k)}))}{\partial x_1 \partial x_2} &amp; … &amp;\frac{\partial^2 f(x^{(k)}))}{\partial x_1 \partial x_n} \\frac{\partial^2 f(x^{(k)}))}{\partial x_2 \partial x_1} &amp; \frac{\partial^2 f(x^{(k)}))}{\partial x_2^2} &amp; …&amp;\frac{\partial^2 f(x^{(k)}))}{\partial x_2 \partial x_n} \…&amp;&amp;&amp; \\frac{\partial^2 f(x^{(k)}))}{\partial x_n \partial x_1} &amp; \frac{\partial^2 f(x^{(k)}))}{\partial x_n \partial x_2} &amp; …&amp;\frac{\partial^2 f(x^{(k)}))}{\partial x_n^2} \\end{bmatrix}$$为$f(x)$在点$x^{(k)}$处的海赛(Hesse)矩阵。可见近似最佳步长不只与梯度有关，还与海赛矩阵H也有关系，计算起来比较麻烦。&amp;#8195&amp;#8195有时，将搜索方向$P^{(k)}$的模长规格化为1，在这种情况下$$P^{(k)}=\frac{-\triangledown f(x^{(k)})}{ |\triangledown f(x^{(k)})|}$$同时，式(4)变为$$\lambda_k=\frac{ \triangledown f(x^{(k)})^\top \triangledown f(x^{(k)}) |\triangledown f(x^{(k)})| }{\triangledown f(x^{(k)})^\top H(x^{(k)}) \triangledown f(x^{(k)})} $$ 固定步长有一点需要注意的是步长a固定时的大小，如果a太小，则会迭代很多次才找到最优解，若a太大，可能跳过最优，从而找不到最优解。 算法梯度下降法算法如下： 输入：目标函数$f(x)$，梯度函数$g(x)=-\triangledown f(x)$，计算精度$\varepsilon $; 输出：$f(x)$的极小值点$x^$。(1). 取初始值$x^{(0)}\in R^n$，置k=0;(2). 计算$f(x^{(k)})$;(3). 计算梯度$g_k =g(x^{(k)})$，当$\left | g_k\right | &lt; \varepsilon$时，停止迭代，令$x^=x^{(k)}$；否则令$P^{(k)}=-g(x^{(k)})$，求$\lambda_k$，使得$$f(x^{(k)}+\lambda_k p_k)=\min_{\lambda \geqslant 0}f(x^{(k)}+\lambda \cdot P^{(k)})$$(4). 置$x^{(k+1)}=x^{(k)}+\lambda_k P^{(k)} $，计算$f(x^{(k+1)})$，当$\left | f(x^{(k+1)})-f(x^{(k)}) \right | &lt; \varepsilon$或$\left | x^{(k+1)} - x^{(k)}\right | &lt; \varepsilon$时，停止迭代，令$x^*=x^{(k)}$；否则置$k=k+1$，转(3)。 随机梯度下降名字中已经体现了核心思想，即随机选取一个点做梯度下降，而不是遍历所有样本后进行参数迭代。 因为梯度下降法的代价函数计算需要遍历所有样本，而且是每次迭代都要遍历，直至达到局部最优解，在样本量庞大时就显得收敛速度比较慢了，计算量非常庞大。 随机梯度下降仅以当前样本点进行最小值求解，通常无法达到真正局部最优解，但可以比较接近。属于大样本兼顾计算成本的折中方案。 最后&amp;#8195&amp;#8195当目标函数是凸函数时，梯度下降法的解是全局最优解。一般情况下，其解不保证是全局最优解。梯度下降法的收敛速度也未必是很快的。 参考资料 《统计学习方法》李航 著 清华大学出版社《运筹学》第四版 清华大学出版社]]></content>
      <categories>
        <category>math</category>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>math</tag>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[my decision tree id3]]></title>
    <url>%2F2018%2F03%2F27%2Fmy-decision-tree-id3%2F</url>
    <content type="text"><![CDATA[前言 本文主要讲述一下决策树的基本算法–ID3生成决策树算法。一开始看例子的时候，我觉得决策树好简单呀，应该实现起来用pandas也能像实现朴素贝叶斯一样容易实现，可是到实践的时候才发现，这个实现起来也好难啊orz。刚开始尝试直接通过算gini指数用CART算法生成树，但发现当两个的gini指数相同时我的程序就没法择优选择了。。。这个还有待改进。。最后参考了一下机器学习实战这本书把id3生成和可视化决策树实现了（不得不说这本书可视化部分写得我都看不懂了。。。）。 正文定义分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点和有向边组成。结点有两种类型：内部节点和叶节点，内部节点表示一个特征或属性，叶节点表示一个类。 核心思想分类决策树的核心思想就是在一个数据集中找到一个最优特征，然后从这个特征的选值中找一个最优候选值(这段话稍后解释)，根据这个最优候选值将数据集分为两个子数据集，然后递归上述操作，直到满足指定条件为止。 优缺点优点 1：理解和解释起来简单，且决策树模型可以想象2：需要准备的数据量不大，而其他的技术往往需要很大的数据集，需要创建虚拟变量，去除不完整的数据，但是该算法对于丢失的数据不能进行准确的预测3：决策树算法的时间复杂度(即预测数据)是用于训练决策树的数据点的对数4：能够处理数字和数据的类别（需要做相应的转变），而其他算法分析的数据集往往是只有一种类型的变量5：能够处理多输出的问题6：使用白盒模型，如果给定的情况是在一个模型中观察到的，该条件的解释很容易解释的布尔逻辑，相比之下，在一个黑盒子模型（例如人工神经网络），结果可能更难以解释7：可能使用统计检验来验证模型，这是为了验证模型的可靠性8：从数据结果来看，它执行的效果很好，虽然它的假设有点违反真实模型 缺点 1：决策树算法学习者可以创建复杂的树，但是没有推广依据，这就是所谓的过拟合，为了避免这种问题，出现了剪枝的概念，即设置一个叶子结点所需要的最小数目或者设置树的最大深度2：决策树的结果可能是不稳定的，因为在数据中一个很小的变化可能导致生成一个完全不同的树，这个问题可以通过使用集成决策树来解决3：众所周知，学习一恶搞最优决策树的问题是NP——得到几方面完全的优越性，甚至是一些简单的概念。因此，实际决策树学习算法是基于启发式算法，如贪婪算法，寻求在每个节点上的局部最优决策。这样的算法不能保证返回全局最优决策树。这可以减轻训练多棵树的合奏学习者，在那里的功能和样本随机抽样更换。4：这里有一些概念是很难的理解的，因为决策树本身并不难很轻易的表达它们，比如说异或校验或复用的问题。5：决策树学习者很可能在某些类占主导地位时创建有有偏异的树，因此建议用平衡的数据训练决策树–当然最重要的还是容易过拟合！，所以迫切需要剪纸或者集成学习。 举个栗子各位立志于脱单的单身男女在找对象的时候就已经完完全全使用了决策树的思想。假设一位母亲在给女儿介绍对象时，有这么一段对话： 母亲：给你介绍个对象。女儿：年纪多大了？母亲：26。女儿：长的帅不帅？母亲：挺帅的。女儿：收入高不？母亲：不算很高，中等情况。女儿：是公务员不？母亲：是，在税务局上班呢。女儿：那好，我去见见。 这个女生的决策过程就是典型的分类决策树。相当于对年龄、外貌、收入和是否公务员等特征将男人分为两个类别：见或者不见。假设这个女生的决策逻辑如下：上图完整表达了这个女孩决定是否见一个约会对象的策略，其中绿色结点（内部结点）表示判断条件，橙色结点（叶结点）表示决策结果，箭头表示在一个判断条件在不同情况下的决策路径，图中红色箭头表示了上面例子中女孩的决策过程。 这幅图基本可以算是一棵决策树，说它“基本可以算”是因为图中的判定条件没有量化，如收入高中低等等，还不能算是严格意义上的决策树，如果将所有条件量化，则就变成真正的决策树了。（以上的决策树模型纯属瞎编乱造，旨在直观理解决策树，不代表任何女生的择偶观，各位女同志无须在此挑刺。。。） 决策树的学习决策树学习算法包含特征选择、决策树的生成与剪枝过程。决策树的学习算法通常是递归地选择最优特征，并用最优特征对数据集进行分割。开始时，构建根结点，选择最优特征，该特征有几种值就分割为几个子集，每个子集分别递归调用此方法，返回结点，返回的结点就是上一层的子结点。直到所有特征都已经用完，或者数据集只有一维特征为止。（这里就不介绍关于决策树的剪枝过程，日后再介绍） 特征选择特征选择问题希望选取对训练数据具有良好分类能力的特征，这样可以提高决策树学习的效率。如果利用一个特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征是没有分类能力的（对象是否喜欢打游戏应该不会成为关键特征吧，也许也会……）。为了解决特征选择问题，找出最优特征，先要介绍一些信息论里面的概念。 熵（entropy）熵是表示随机变量不确定性的度量。设$X$是一个取有限个值的离散随机变量，其概率分布为$$P(X=x_i)=p_i, i=1,2,…,n$$则随机变量的熵定义为$$entropy(X) = -\sum_{i=1}^n P_ilog_2 P_i$$另外，$0log0=0$，当对数的底为2时，熵的单位为bit；为e时，单位为nat。熵越大，随机变量的不确定性就越大。从定义可验证$0&lt;=H(p)&lt;=logn$.python实现计算如下： 12calc_entropy = lambda P_: sum(map(lambda p: -p * np.log2(p), P_))# 其中P_为X的概率分布，p为X取某个随机变量的概率 条件熵（conditional entropy）设有随机变量$(X,Y)$，其联合概率分布为$$P(X=x_i,Y=y_i)=p_{ij}, i=1,2,…,n;j=1,2,…,m$$条件熵$H(Y|X)$表示在已知随机变量X的条件下随机变量Y的不确定性。随机变量X给定的条件下随机变量Y的条件熵$H(Y|X)$，定义为X给定条件下Y的条件概率分布的熵对X的数学期望$$entropy(Y|X) = \sum_{i=1}^k p_i H(Y|X=x_i)$$这里$p_i=P(X=x_i), i=1,2,…,n$。用python实现求条件熵时，只需要把P_更换为feat_value_cate_P再调用calc_entropy，然后把所有分组的概率与对应的熵相乘再相加： 12345# group为Y对于这个特征X取不同的值的分组for feat_value, group in groups: feat_value_P = len(group) / len(df) # 特征X取某值的概率 feat_value_cate_P = group[cate].value_counts() / group[cate].count() # 特征X取某值对应不同的类别的概率 feat_value_entropy += feat_value_P * calc_entropy(feat_value_cate_P) 信息增益（information gain）信息增益表示得知特征X的信息而使得类Y的信息的不确定性减少的程度。特征A对训练数据集D的信息增益$g(D,A)$，定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，即$$g(D,A)=H(D)−H(D|A)$$这个差又称为互信息，表示由于特征A而使得对数据集D的分类不确定性减少的程度。信息增益大的特征具有更强的分类能力。设训练数据为$D$，$|D|$表示其样本容量，即样本个数。设$D$有$K$个类$C_k$，$k=1,2,…,K$，$|C_k|$为属于类$C_k$的样本个数，$\sum_{k=1}^K |C_k|=|D|$. 设特征A有n个不同的取值${a_1,a_2,…a_n}$, 根据特征A 的取值将D划分为n个子集$D_1,D_2,…,D_n$, $|D_i|$为的样本$D_i$个数，$\sum_{i=1}^n |D_i|=|D|$。记子集$D_i$中属于类$C_k$的样本的集合为$D_{ik}$，即$D_{ik}=D_i\bigcap C_k$，$|D_{ik}|$为$D_{ik}$的样本个数。 计算信息增益的算法如下： 输入：训练数据集$D$和特征$A$； 输出：特征A对训练数据集$D$的信息增益$g(D,A)$. 计算数据集D的经验熵H(D)$$H(D)=-\sum_{i=1}^k \frac{|C_k|}{|D|} log_2 \frac {|C_k|}{|D|}$$ 计算特征A对数据集D的经验条件熵$H(D|A)$$$H(D|A)=\sum_{i=1}^n \frac{|D_i|}{|D|}H(D_i)=\sum_{i=1}^n \frac{|D_i|}{|D|} \sum_{i=1}^K \frac{|D_{ik}|}{|D|} log_2 \frac{|D_{ik}|}{|D|}$$ 计算信息增益$$g(D,A)=H(D)−H(D|A)$$ 决策树的生成本次我们只介绍ID3算法，ID3算法由Ross Quinlan发明，建立在“奥卡姆剃刀”的基础上：越是小型的决策树越优于大的决策树（be simple简单理论）。ID3算法中根据信息增益评估和选择特征，每次选择信息增益最大的特征作为判断模块建立子结点。ID3算法可用于划分标称型数据集，没有剪枝的过程，为了去除过度数据匹配的问题，可通过裁剪合并相邻的无法产生大量信息增益的叶子节点（例如设置信息增益阀值）。使用信息增益的话其实是有一个缺点，那就是它偏向于具有大量值的属性。就是说在训练集中，某个属性所取的不同值的个数越多，那么越有可能拿它来作为分裂属性，而这样做有时候是没有意义的，另外ID3不能处理连续分布的数据特征，于是就有了C4.5算法。CART算法也支持连续分布的数据特征。123456789101112131415161718192021222324252627282930def select(df, features, cate , H_D): gain_dict = &#123;&#125; for feat in features: groups = df.groupby(feat) feat_value_entropy = 0 for feat_value, group in groups: feat_value_P = len(group) / len(df) # 特征取某值的概率 feat_value_cate_P = group[cate].value_counts() / group[cate].count() # 特征取某值对应不同的类别的概率 feat_value_entropy += feat_value_P * calc_entropy(feat_value_cate_P) imfor_gain = H_D - feat_value_entropy gain_dict[feat] = imfor_gain return max(gain_dict, key=lambda x:gain_dict[x])def create_tree(df, cate, H_D ): feat = df.columns[:-1].tolist() cate_values = df[cate].unique() if len(cate_values)==1: return cate_values[0] if len(feat) == 0: # 用完所有特征后 temp = df[cate].value_counts().to_dict() return max(temp, key=lambda x:temp[x]) # 取最多的类别作为返回值 best_feat = select(df, feat, cate, H_D) my_tree = &#123; best_feat:&#123;&#125; &#125; unique_feat_values = df[best_feat].unique() for feat_value in unique_feat_values: df_ = df[df[best_feat] == feat_value].copy() df_ = df_.drop(best_feat, axis=1) # 这里一定不能是df，必须是一个新的df_，才能使递归*中的feat*越来越小 my_tree[best_feat][feat_value] = create_tree(df_, cate, H_D ) return my_tree 我们这里用Python语言的字典套字典类型存储树的信息，简单方便。当然也可以定义一个新的数据结构存储树。来生成一个树：1234567df = pd.read_excel("TreeData.xlsx", index_col="id")cate = df.columns[-1]P_cate = df[cate].value_counts() / df[cate].count()H_D = calc_entropy(P_cate)tree = create_tree(df,cate, H_D)print(tree)# &#123;'有自己的房子': &#123;'否': &#123;'有工作': &#123;'否': '否', '是': '是'&#125;&#125;, '是': '是'&#125;&#125; 决策树的可视化我们主要用python的matplotlib来处理图像，它的annotate很方便用于注释。（以下代码来源：机器学习实战，我对其简单的更改了一下）先获得叶子节点个数和树的深度：1234567891011121314151617181920212223def get_leafs_num(tree_): num_leafs = 0 first_key = list(tree_.keys())[0] second_dict = tree_[first_key] for key in second_dict.keys(): if type(second_dict[key]).__name__ == "dict": num_leafs += get_leafs_num(second_dict[key]) else: num_leafs += 1 return num_leafsdef get_tree_depth(tree_): max_depth = 0 first_key = list(tree_.keys())[0] second_dict = tree_[first_key] for key in second_dict.keys(): if type(second_dict[key]).__name__ == "dict": # 如果还是字典，继续深入 this_depth = 1 + get_tree_depth(second_dict[key]) else: this_depth = 1 if this_depth &gt; max_depth: max_depth = this_depth return max_depth 然后再画图：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import matplotlib.pyplot as pltplt.rcParams['font.sans-serif']=['SimHei'] plt.rcParams['axes.unicode_minus']=False decision_node=&#123;"boxstyle": "sawtooth", "fc": "0.8", &#125;leaf_node=&#123;"boxstyle": "round4", "fc": "0.8"&#125;arrow_args=&#123;"arrowstyle": "&lt;-"&#125;def plot_node(node_txt, centerPt, parentPt, node_type): global ax1 ax1.annotate(node_txt, xy=parentPt, xycoords='axes fraction',xytext=centerPt, textcoords='axes fraction',va="center", ha="center", bbox=node_type, arrowprops=arrow_args)def plot_mid_text(cntrPt, parentPt, txt_string): # 在两个节点之间的线上写上字 global ax1 xMid = (parentPt[0] - cntrPt[0]) / 2.0 + cntrPt[0] yMid = (parentPt[1] - cntrPt[1]) / 2.0 + cntrPt[1] ax1.text(xMid, yMid, txt_string) # text() 的使用def plot_tree( tree_, parent_point, node_txt): global ax1,xOff,yOff,totalD,totalW num_leafs = get_leafs_num(tree_) depth = get_tree_depth(tree_) first_key = list(tree_.keys())[0] center_point = (xOff + (1.0 + float(num_leafs)) / 2.0 / totalW, yOff) plot_mid_text( center_point, parent_point, node_txt) # 在父子节点间填充文本信息 plot_node(first_key, center_point, parent_point, decision_node) # 绘制带箭头的注解 second_dict = tree_[first_key] yOff = yOff - 1.0 / totalD for key in second_dict.keys(): if type(second_dict[key]).__name__ == 'dict': # 判断是不是字典， plot_tree(second_dict[key], center_point, str(key)) # 递归绘制树形图 else: # 如果是叶节点 xOff = xOff + 1.0 / totalW plot_node(second_dict[key], (xOff, yOff), center_point, leaf_node) plot_mid_text((xOff, yOff), center_point, str(key)) yOff = yOff + 1.0 / totalDdef show_tree(tree_): global ax1,xOff,yOff,totalD,totalW fig = plt.figure(1, facecolor='white') fig.clf() # 清空绘图区 axprops = dict(xticks=[], yticks=[]) ax1 = plt.subplot(111, frameon=False, **axprops) totalW = float(get_leafs_num(tree_)) totalD = float(get_tree_depth(tree_)) xOff = -0.5 / totalW # 追踪已经绘制的节点位置 初始值为 将总宽度平分 在取第一个的一半 yOff = 1.0 plot_tree(tree_, (0.5, 1.0), '') # 调用函数，并指出根节点源坐标 plt.show() 下面用一个实例来可视化一下：1234tree = &#123;'有自己的房子': &#123;'否': &#123;'有工作': &#123;'否': '否', '是': '是'&#125;&#125;, '是': '是'&#125;&#125;print(tree)show_tree(tree) 可视化结果如下： 由于篇幅过长，完整代码（结构化封装）就不在这里给出，详情参见我的GitHub。 测试这里我们采用uci的lense隐形眼镜测试集,总样本为24个，四个特征，三个类别。我们通过网络爬虫，直接从该网址抓取数据，并转换为dataframe类型。 – 3 Classes: 1 : the patient should be fitted with hard contact lenses, 2 : the patient should be fitted with soft contact lenses, 3 : the patient should not be fitted with contact lenses. – 4 Features: 1. age of the patient: (1) young, (2) pre-presbyopic, (3) presbyopic 2. spectacle prescription: (1) myope, (2) hypermetrope 3. astigmatic: (1) no, (2) yes 4. tear production rate: (1) reduced, (2) normal 下面我们给出代码：1234567891011121314151617181920212223242526from ID3 import ID3_treeimport requestsimport pandas as pddef get_data(): url = "http://archive.ics.uci.edu/ml/machine-learning-databases/lenses/lenses.data" data = requests.get(url).text verctors = data.split('\n') verctors = [ver.split() for ver in verctors] features = ["id","age","prescript","astigmatic","tearRate","category"] df = pd.DataFrame(verctors,columns=features, dtype=int) df = df.set_index("id").dropna() key_list = [&#123;"1":"young", "2":"pre-presbyopic", "3":"presbyopic"&#125;, &#123;"1": "myope", "2": "hypermetrope"&#125; , &#123;"1": "no", "2": "yes"&#125;, &#123;"1": "reduced", "2":"normal"&#125;,&#123;"1":"hard","2":"soft","3":"no lenses"&#125;] for i in range(5): df.iloc[:,i] = df.iloc[:,i].apply(lambda x:key_list[i][x]) return dfif __name__ == "__main__": df = get_data() # print(df) id3_tree = ID3_tree(df) id3_tree = id3_tree.train(df) my_tree = id3_tree.my_tree print(my_tree) id3_tree.show_tree(my_tree) 得出的决策树如下： 后续由于本文只给出了ID3算法生成决策树和决策树的可视化，日后我将继续给出C4.5的生成决策树算法、决策树的减枝问题与及CART分类和回归树的构造。然后我们还可以把它拓宽，引入集成学习的随机森林。 作者时间精力有限，我就先写到这里啦。如有疑问，记得联系我哦。 参考文献 《统计学习方法》李航 著 清华大学出版社《机器学习实战》Peter Harrington 著 人民邮电出版社]]></content>
      <categories>
        <category>machine_leanring</category>
        <category>code</category>
      </categories>
      <tags>
        <tag>machine_leanring</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Radial basis function kernel]]></title>
    <url>%2F2018%2F03%2F18%2FRadial-basis-function-kernel%2F</url>
    <content type="text"><![CDATA[高斯径向基函数简介：在机器学习中，（高斯）径向基函数核（英语：Radial basis function kernel），或称为RBF核，是一种常用的核函数。它是支持向量机分类中最为常用的核函数。—— 维基百科 高斯径向基函数高斯径向基函数公式如下：$$K(x_1,x_2)=\exp{(-\frac{\parallel x_1-x_2 \parallel^2 }{2\sigma^2})}, \sigma&gt;0$$ 那么它有什么几何意义呢?先看看x经过映射以后，在高维空间里这个点到原点的距离公式： $$ \parallel x_i-0\parallel^2 = \parallel x_i \parallel^2=\left \langle \Phi (x_i), \Phi (x_i)\right \rangle=K(x_i,x_i)=1$$这表明样本x映射到高维空间后，在高维空间中的点$\Phi (x_i)$到高维空间中原点的距离为1，也即$\Phi (x_i)$存在于一个超球面上。 为什么核函数能映射到高维空间呢？先考虑普通的多项式核函数：$K(x,y)=(x^T\cdot y+m)^p$,其中$x,y\in \mathbb{R}^n$，多项式参数$p,m\in \mathbb{R}$考虑$x=(x_1,x_2),y=(y_1,y_2),m=0,p=2$，即$K(x,y)=(x^T\cdot y)^2=x_1^2y_1^2+2x_1x_2y_1y_2+x_2^2y_2^2$现在回到之前的映射$k(x,y)=\left \langle \Phi(x),\Phi(y)\right \rangle$，并取$\Phi(x)=(x_1^2,\sqrt{2}x_1x_2,x_2^2)$则有$k(x,y)=\left \langle \Phi(x),\Phi(y)\right \rangle=x_1^2y_1^2+2x_1x_2y_1y_2+x_2^2y_2^2=(x^T\cdot y)^2=K(x,y)$这就是前面的$K(x,y)$，因此，该核函数就将2维映射到了3维空间。 径向基核又为什么能够映射到无限维空间呢？看完了普通多项式核函数由2维向3维的映射，再来看看高斯径向基函数会把2维平面上一点映射到多少维。$$\begin{eqnarray}K(x,y) &amp; = &amp; \exp(| x_1-x_2 |^2 ) \&amp; = &amp; \exp(-(x_1-y_1)^2-(x_2-y_2)^2) \&amp; = &amp; \exp(-x_1^2+2x_1y_1-y_1^2-x_2^2+2x_2y_2-y_2^2) \&amp; = &amp; \exp(-|x|^2)\exp(-|y|^2)\exp(2x^Ty)\\end{eqnarray}$$将最后一项泰勒展开你就会恍然大悟：$$K(x,y)=\exp(-|x|^2)\exp(-|y|^2)\sum_{n=0}^{\infty}\frac{(2x^Ty)^n}{n!}$$ 再具体一点：高斯核是这样定义的：$K(x_1,x_2)=\exp{(-\frac{\parallel x_1-x_2 \parallel^2 }{2\sigma^2})}, \sigma&gt;0$尽管我不会解释它（但相信我）高斯核可以简单修正为这个样子：$K(x_1,x_2)=\exp(-\frac{x_1\cdot x_2}{\sigma^2}),\sigma&gt;0$，这里$x_1\cdot x_2$可解释为内积。再用泰勒展开就会得到$K(x_1,x_2)=\sum_{n=0}^{\infty}\frac{(x_1\cdot x_2)^n}{\sigma^nn!}$求和号里面的元素是不是看起来很熟悉呢？没错，这就是一个n次多项式核。因为每一个多项式核都将一个向量投影到更高维的空间中，因此高斯核是那些$degree\geq0$的多项式核的组合，所以我们说高斯核是投影到无穷维空间中。 参考Quora上的问题：https://www.quora.com/Machine-Learning/Why-does-the-RBF-radial-basis-function-kernel-map-into-infinite-dimensional-space-mentioned-many-times-in-machine-learning-lectures 最后初学者使用markdown与LaTeX的语法真不适应啊，就写到这里吧，写篇博客太累了orz饭都没吃，关于RBF神经网络的话，以后用到再来讲吧，累死我惹。如有疑问，欢迎咨询，联系方式见“关于”页面。]]></content>
      <categories>
        <category>math</category>
        <category>theory</category>
      </categories>
      <tags>
        <tag>machine_leanring</tag>
        <tag>math</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python 的闭包、装饰器]]></title>
    <url>%2F2018%2F03%2F17%2Fpython-%E7%9A%84%E9%97%AD%E5%8C%85%E3%80%81%E8%A3%85%E9%A5%B0%E5%99%A8%2F</url>
    <content type="text"><![CDATA[一、闭包（Closure）什么是闭包？ 在计算机科学中，闭包（英语：Closure），又称词法闭包（Lexical Closure）或函数闭包（function closures），是引用了自由变量的函数。这个被引用的自由变量将和这个函数一同存在，即使已经离开了创造它的环境也不例外。所以，有另一种说法认为闭包是由函数和与其相关的引用环境组合而成的实体。闭包在运行时可以有多个实例，不同的引用环境和相同的函数组合可以产生不同的实例。—— 维基百科) 这里我给个简单的解释：一个闭包就是你调用了一个函数A，这个函数A返回了一个函数B给你。这个返回的函数B就叫做闭包。你在调用函数A的时候传递的参数就是自由变量。 举个例子：1234567def func(name): def inner_func(age): print 'name:', name, 'age:', age return inner_funcbb = func('matrix')bb(26) # &gt;&gt;&gt; name: matrix age: 26 这里面调用func的时候就产生了一个闭包– inner_func,并且该闭包持有自由变量– name，因此这也意味着，当函数func的生命周期结束之后，name这个变量依然存在，因为它被闭包引用了，所以不会被回收。 也有人说这种内部函数inner_func可以使用外部函数的变量name的行为就叫闭包。 二、装饰器（Decorator）什么是装饰器？ “装饰器的功能是将被装饰的函数当作参数传递给与装饰器对应的函数（名称相同的函数），并返回包装后的被装饰的函数” 听起来有点绕，没关系，直接看示意图,其中a为 与装饰器@a对应的函数，b为装饰器修饰的函数，装饰器@a的作用是：简而言之：@a 就是将 b 传递给 a()，并返回新的 b = a(b) 举个例子 先导入包： 1234from functools import reduceimport mathimport logginglogging.basicConfig(level=logging.INFO) 定义一个检查参数的装饰器： 123456789def checkParams(fn): def wrapper(*numbers): temp = map(lambda x:isinstance(x,(int,)),numbers) # 检查参数是否都为整型 if reduce(lambda x,y: x and y, temp): # 若都为整型 return fn(*numbers) # 则调用fn(*numbers)返回计算结果 #否则通过logging记录错误信息，并友好退出 logging.warning("variable numbers cannot be added") return return wrapper #fn引用gcd，被封存在闭包的执行环境中返回 然后定义求最大公约数的函数（能求多个的最大公约数）： 123def gcd(*numbers): """return the greatest common divisor of the given integers.""" return reduce(math.gcd, numbers) 调用 123&gt;&gt;&gt;gcd = checkParams(gcd)&gt;&gt;&gt;gcd(3, 'hello')# 输出 WARNING:root: variable numbers cannot be added 注意checkParams函数： 首先看参数fn，当我们调用checkParams(gcd)的时候，它将成为函数对象gcd的一个本地(Local)引用； 在checkParams内部，我们定义了一个wrapper函数，添加了参数类型检查的功能，然后调用了fn(*numbers)，根据LEGB法则，解释器将搜索几个作用域，并最终在(Enclosing层) checkParams函数的本地作用域中找到fn； 注意最后的return wrapper，这将创建一个闭包，fn变量(gcd函数对象的一个引用)将会封存在闭包的执行环境中，不会随着checkParams的返回而被回收； 当调用gcd = checkParams(gcd)时，gcd指向了新的wrapper对象，它添加了参数检查和记录日志的功能，同时又能够通过封存的fn，继续调用原始的gcd进行最大公约数运算。 因此调用gcd(3, ‘hello’)将不会返回计算结果，而是打印出日志：root: variable numbers cannot be added。 有人觉得add = checkParams(add)这样的写法未免太过麻烦，于是python提供了一种更优雅的写法，被称为语法糖：12345@checkParamsdef lcm(*numbers): """return lowest common multiple.""" f = lambda a,b:int((a*b)/gcd(a,b)) return reduce(f, numbers) 其实这只是一种写法上的优化，解释器仍然会将它转化为gcd = checkParams(gcd)来执行。]]></content>
      <categories>
        <category>math</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>fun math</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo常用命令笔记]]></title>
    <url>%2F2018%2F03%2F13%2Fhexo%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[hexo123npm install hexo -g #安装npm update hexo -g #升级hexo init #初始化 简写12345hexo n "我的博客" == hexo new "我的博客" #新建文章hexo p == hexo publishhexo g == hexo generate#生成hexo s == hexo server #启动服务预览hexo d == hexo deploy#部署 服务器1234hexo server #Hexo 会监视文件变动并自动更新，您无须重启服务器。hexo server -s #静态模式hexo server -p 5000 #更改端口hexo server -i 192.168.1.1 #自定义 IP 123hexo clean #清除缓存 网页正常情况下可以忽略此条命令hexo g #生成静态网页hexo d #开始部署 监视文件变动12hexo generate #使用 Hexo 生成静态文件快速而且简单hexo generate --watch #监视文件变动 完成后部署123# 以下两个命令的作用是相同的hexo generate --deployhexo deploy --generate 简写12hexo deploy -ghexo server -g 草稿1hexo publish [layout] &lt;title&gt; 模版12345hexo new &quot;postName&quot; #新建文章hexo new page &quot;pageName&quot; #新建页面hexo generate #生成静态页面至public目录hexo server #开启预览访问端口（默认端口4000，&apos;ctrl + c&apos;关闭server）hexo deploy #将.deploy目录部署到GitHub 如：123hexo new [layout] &lt;title&gt;hexo new photo &quot;My Gallery&quot;hexo new &quot;Hello World&quot; --lang tw 变量 描述 layout 布局 title 标题 date 文件建立日期 12345678title: 使用Hexo搭建个人博客layout: postdate: 2014-03-03 19:07:43comments: truecategories: Blogtags: [Hexo]keywords: Hexo, Blogdescription: 生命在于折腾，又把博客折腾到Hexo了。给Hexo点赞。]]></content>
      <categories>
        <category>hexo</category>
        <category>Instructions</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[my_knn]]></title>
    <url>%2F2018%2F03%2F13%2Fmy-knn%2F</url>
    <content type="text"><![CDATA[关于在一个月前，需要用1nn做二分类的测试的时候，开始因为用sklearn训练数据时用错了数据集，百思不得其解，于是自己写了个knn来训练，当时写好后，才真正把原理给弄懂了orz，原来是数据集训练时用错了。。。改正后对比了一下自己的knn和sklearn的knn的准确率都差不多（也就是说测试通过啦），就上传到了我的GitHub。 当时我虽然有个用腾讯云搭建的博客，但基本上都没在上面写过了orz，本博客当时还没有问世，正好基于GitHub的服务器最近搭了这个博客，空空的也不好，最近老师第一讲就讲knn，那就把之前的代码贴上吧。 原理对于一个输入的测试数据，计算该样本点到训练数据各样本点的距离，然后对所有距离由小到大排列，取前k个数据；统计该k个数据中对应的标签出现次数最多的标签，则该测试样本就被标记为该标签。 算法 输入: 训练数据集：$T={(X_1,y_1),(X_2,y_2),…,(X_N,y_N)}$, 其中$X_i={x_i^1,x_i^2,…,x_i^n}$,有n个特征，N个样本点; 输入：最近邻个数k，及要预测的样本点$X_0={x_0^1,,x_0^2,…,x_0^n}$; 计算：样本点X_0到训练数据集T中各样本点的距离（一般为欧氏距离）; 排序：将以上算出的距离由小到大排序，并选出前k个距离数据; 统计：统计前k个距离数据中各个标签对应的个数，选出个数最多的那个标签，即为该样本点预测的结果。 代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556import numpy as npfrom sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_splitclass my_knn(object): """docstring for my_knn""" def __init__(self, k): super(my_knn, self).__init__() self.k = k def train(self, X_train, y_train): self.X_train, self.y_train = np.array(X_train), np.array(y_train) if len(self.X_train) != len(self.y_train): raise ValueError("X_test,y_test or y_train was not equail!" "The length of X_test,y_test is %s" "But the length of y_train is %s" % (len(self.X_train), len(self.y_train))) return self def predict_one(self, X): dist2xtrain = np.sum((X - self.X_train)**2, axis=1)**0.5 index = dist2xtrain.argsort() # 从小到大（近到远） label_count = &#123;&#125; for i in range(self.k): label = self.y_train[index[i]] label_count[label] = label_count.get(label, 0) + 1 # 将label_count的值从大到小排列label_count的键 y_predict = sorted(label_count, key=lambda x: label_count[x], reverse=True)[0] return y_predict def predict_all(self, X): return np.array(list(map(self.predict_one, X))) def calc_accuracy(self, X, y): predict = self.predict_all(X) total = X.shape[0] right = sum(predict == y) accuracy = right/total return accuracyif __name__ == "__main__": data_set = load_iris() datas = data_set["data"] labels = data_set['target'] X_train, X_test, y_train, y_test = train_test_split(datas, labels, test_size=0.4, random_state=0) knn = my_knn(1) knn = knn.train(X_train,y_train) accuracy = knn.calc_accuracy(X_test,y_test) print("%.3f%%" % (accuracy * 100)) from sklearn.neighbors import KNeighborsClassifier neigh = KNeighborsClassifier(n_neighbors=1) neigh.fit(X_train, y_train) print(neigh.score(X_train,y_train)) print(neigh.score(X_test, y_test)) 最后关于对knn的kd树加速这部分还需要日后的后续学习，这里就先不说啦（其实是我也不会23333）。由于我对markdown语法不太熟悉，写起文章来的有点别扭还望理解（逃。 写给自己 还是要多花点时间学习啊！一个多月没学习就忘得差不多了orz,还好看一下就能回想起来。多练习吧！]]></content>
      <categories>
        <category>machine_leanring</category>
        <category>code</category>
      </categories>
      <tags>
        <tag>machine_leanring</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[my_bayes]]></title>
    <url>%2F2018%2F03%2F13%2Fmy-bayes%2F</url>
    <content type="text"><![CDATA[前言本周一晚上老师讲到了naive bayes（朴素贝叶斯分类器），于是自己用python来实现了一下。现在这个脚本对于比较大的数据可能会计算的比较慢，还需要以后慢慢再研究一下里面的加速。 本程序主要利用了pandas里dataframe的groupby分组函数，大大的方便了对数据的统计。对于条件概率，有不同的标签，不同的特征和特征里的不同数据，我们采用了dict数据结构，第一层key为标签，value是一个新的dict；第二层（前面那个新的dict）的key为特征，value是一个Series或者字典；第三层的key/index为特征的取值，value为频数/概率。、、（虽然看起来比较拗口，但我感觉这样能够比较清晰的分清了各个条件概率了，如果你有更好的方法，欢迎留言给我，谢谢。） 算法 输入：训练数据集及其标签集，要预测的数据集 统计各标签出现的频数，并拉普拉斯平滑，计算先验概率 统计在各标签下各个特征的频数，并拉普拉斯平滑，计算条件概率 查找要预测数据集各特征在不同标签下的条件概率和先验概率相乘得到（半）后验概率 对半后验概率进行从大到小排序，选出最大值对应的标签，即为预测结果 实例化测试ps：这里半后验概率为我自己的定义：$P(Y_j) *\prod_{i=1}^N P(A_i|Y_j) ; i:1\to n_{feature}; j:1\to n_{label}$ 解释本程序主要分为一下部分： 定义一个bayes分类器（类） 计算先验概率 计算所有条件概率 进行调用训练 对测试数据进行预测 实例化测试 以上各对应之下的各个函数：（废话不多说，直接上代码） 代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import numpy as npimport pandas as pdclass my_naive_bayes(object): def __init__(self, df): super(my_naive_bayes, self).__init__() self.df = df self.X_train = df.iloc[:,:-1] self.y_train = df.iloc[:,-1] self.label_set = set(self.y_train) self.features = df.columns[:-1] self.label_name = df.columns[-1] self.feature_dict = &#123;&#125; self.n_sample = len(df) def get_prior_p(self, g): n = len(g) prior_p = &#123;&#125; for label in self.label_set: prior_p[label] = g.size()[label] / self.n_sample return prior_p def get_cond_p(self, g): cond_p = &#123;&#125; for label, group in g: cond_p[label] = &#123;&#125; for feature in self.features: counts = group[feature].value_counts() cond_p[label][feature] = counts / sum(counts) return cond_p def train(self, ): for feature in self.features: self.feature_dict[feature] = set(self.df[feature]) g = self.df.groupby(self.label_name) self.prior_p = self.get_prior_p(g) self.cond_p = self.get_cond_p(g) return self def predict_one(self, test_X): semi_post_p = &#123;&#125; for label in self.label_set: temp = 1 for feature in self.features: temp = temp * self.cond_p[label][feature][test_X[feature]] semi_post_p[label] = self.prior_p[label] * temp return max(semi_post_p, key=semi_post_p.get)if __name__ == '__main__': df = pd.read_excel("bayes_data.xlsx",index_col="index") # n = len(df) # train_n = int(n*0.6) # train_df = df[:train_n] # test_df = df[train_n:] bayes = my_naive_bayes(df) bayes = bayes.train() test_x = df.loc[6] label = bayes.predict_one(test_x) print(label) 最后，好好学习，天天向上！]]></content>
      <categories>
        <category>machine_leanring</category>
        <category>code</category>
      </categories>
      <tags>
        <tag>machine_leanring</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[18_03_07]]></title>
    <url>%2F2018%2F03%2F07%2F18-03-07%2F</url>
    <content type="text"><![CDATA[今天是开学第三天，各种事情莫名烦躁。]]></content>
      <categories>
        <category>diary</category>
      </categories>
      <tags>
        <tag>diary</tag>
        <tag>life-style</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVM explained-well]]></title>
    <url>%2F2018%2F03%2F02%2FSVM-explained-well%2F</url>
    <content type="text"><![CDATA[Support vector machines (SVM) User copperking stepped up to the plate: “We have 2 colors of balls on the table that we want to separate. We get a stick and put it on the table, this works pretty well right? Some villain comes and places more balls on the table, it kind of works but one of the balls is on the wrong side and there is probably a better place to put the stick now. SVMs try to put the stick in the best possible place by having as big a gap on either side of the stick as possible. Now when the villain returns the stick is still in a pretty good spot. There is another trick in the SVM toolbox that is even more important. Say the villain has seen how good you are with a stick so he gives you a new challenge. svm7-300x167.png There’s no stick in the world that will let you split those balls well, so what do you do? You flip the table of course! Throwing the balls into the air. Then, with your pro ninja skills, you grab a sheet of paper and slip it between the balls. Now, looking at the balls from where the villain is standing, they balls will look split by some curvy line. Boring adults the call balls data, the stick a classifier, the biggest gap trick optimization, call flipping the table kernelling and the piece of paper a hyperplane.” I think the last step is the most beautiful no mater in mathematic or machine learning! Hope it will help you. source: http://bytesizebio.net/2014/02/05/support-vector-machines-explained-well/]]></content>
      <categories>
        <category>theory</category>
      </categories>
      <tags>
        <tag>machine_leanring</tag>
        <tag>fun math</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[new-post]]></title>
    <url>%2F2018%2F02%2F27%2Fnew-post%2F</url>
    <content type="text"><![CDATA[this is a test blog.welcome to treamy’s world. Create a new post1$ hexo new "My New Post" add some code1&gt;&gt;&gt; print("My New Post") 标签页面1&gt;运行以下命令1$ hexo new page "tags" 同时，在/source目录下会生成一个tags文件夹，里面包含一个index.md文件 推送到服务器上1$ hexo g -d 先generate一下生成静态页面，再deploy部署到服务器。好像hexo d -g也可以。。一次记错了写成这个也行。。我也不知道为啥在一个网页上看到说他们左右是相同的，还是用前面那个较好解读的吧。]]></content>
      <tags>
        <tag>test</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F02%2F27%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
